[
  {
    "objectID": "plugins.pandas_profiling.html",
    "href": "plugins.pandas_profiling.html",
    "title": "Pandas Profiling",
    "section": "",
    "text": "PandasProfilingPlugin concatenates features and target(s) and builds an EDA report. If WandBPlugin is included alongside this plugin, the report will also be logged to the Weights and Biases project.\n\nsource\n\nPandasProfilingPlugin\n\n PandasProfilingPlugin (title:str='pandas_profiling_report',\n                        explorative:bool=False, minimal:bool=True,\n                        html_path:Union[str,pathlib.Path]=None, **kwargs)\n\nPandas Profiling plugin. Kwargs from the constructor are passed to ProfileReport().\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntitle\nstr\npandas_profiling_report\nReport title.\n\n\nexplorative\nbool\nFalse\nEnable explorative mode. Default False.\n\n\nminimal\nbool\nTrue\nEnable minimal mode. Default True.\n\n\nhtml_path\ntyping.Union[str, pathlib.Path]\nNone\nPath where the HTML report will be saved. Default is the title of the report.\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nPandasProfilingPlugin.on_setup_data\n\n PandasProfilingPlugin.on_setup_data ()\n\nCreate Pandas Profiling HTML report."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Poniard",
    "section": "Introduction",
    "text": "Introduction\n\nA poniard /ˈpɒnjərd/ or poignard (Fr.) is a long, lightweight thrusting knife (Wikipedia).\n\nPoniard is a scikit-learn companion library that streamlines the process of fitting different machine learning models and comparing them.\nIt can be used to provide quick answers to questions like these: * What is the reasonable range of scores for this task? * Is a simple and explainable linear model enough or should I work with forests and gradient boosters? * Are the features good enough as is or should I work on feature engineering? * How much can hyperparemeter tuning improve metrics? * Do I need to work on a custom preprocessing strategy?\nThis is not meant to be end to end solution, and you definitely should keep on working on your models after you are done with Poniard.\nThe core functionality has been tested to work on Python 3.7 through 3.10 on Linux systems, and from 3.8 to 3.10 on macOS."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Poniard",
    "section": "Installation",
    "text": "Installation\nStable version:\npip install poniard\nDev version with most up to date changes:\npip install git+https://github.com/rxavier/poniard.git@develop#egg=poniard"
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Poniard",
    "section": "Documentation",
    "text": "Documentation\nCheck the full Quarto docs, including guides and API reference."
  },
  {
    "objectID": "index.html#usagefeatures",
    "href": "index.html#usagefeatures",
    "title": "Poniard",
    "section": "Usage/features",
    "text": "Usage/features\n\nBasics\nThe API was designed with tabular tasks in mind, but it should also work with time series tasks provided an appropiate cross validation strategy is used (don’t shuffle!)\nThe usual Poniard flow is: 1. Define some estimators. 2. Define some metrics. 3. Define a cross validation strategy. 4. Fit everything. 5. Print the results.\nPoniard provides sane defaults for 1, 2 and 3, so in most cases you can just do…\n\nfrom poniard import PoniardRegressor\nfrom sklearn.datasets import load_diabetes\n\n\nX, y = load_diabetes(return_X_y=True, as_frame=True)\npnd = PoniardRegressor(random_state=0)\npnd.setup(X, y)\npnd.fit()\n\n\n                         Setup info\n                         Target\n                             Type: continuous\n                             Shape: (442,)\n                             Unique values: 214\n                             Metrics\n                             Main metric: neg_mean_squared_error\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 44\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      \n      sex\n      \n    \n    \n      1\n      bmi\n      \n      \n      \n    \n    \n      2\n      bp\n      \n      \n      \n    \n    \n      3\n      s1\n      \n      \n      \n    \n    \n      4\n      s2\n      \n      \n      \n    \n    \n      5\n      s3\n      \n      \n      \n    \n    \n      6\n      s4\n      \n      \n      \n    \n    \n      7\n      s5\n      \n      \n      \n    \n    \n      8\n      s6\n      \n      \n      \n    \n  \n\n\n\n\n\n\nPoniardRegressor(random_state=0)\n\n\n… and get a nice table showing the average of each metric in all folds for every model, including fit and score times (thanks, scikit-learn cross_validate function!)\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.005265\n      0.001960\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003509\n      0.001755\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.101435\n      0.004821\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.334695\n      0.009266\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003038\n      0.002083\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.055696\n      0.002855\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.003470\n      0.001721\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003010\n      0.001627\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004179\n      0.001667\n    \n  \n\n\n\nAlternatively, you can also get a nice plot of your different metrics by using the PoniardBaseEstimator.plot.metrics method.\n\n\nType inference\nPoniard uses some basic heuristics to infer the data types.\nFloat and integer columns are defined as numeric if the number of unique values is greater than indicated by the categorical_threshold parameter.\nString/object/categorical columns are assumed to be categorical.\nDatetime features are processed separately with a custom encoder.\nFor categorical features, high and low cardinality is defined by the cardinality_threshold parameter. Only low cardinality categorical features are one-hot encoded.\n\n\nEnsembles\nPoniard makes it easy to combine various estimators in stacking or voting ensembles. The base esimators can be selected according to their performance (top-n) or chosen by their names.\nPoniard also reports how similar the predictions of the estimators are, so ensembles with different base estimators can be built. A basic correlation table of the cross-validated predictions is built for regression tasks, while Cramér’s V is used for classification.\nBy default, it computes this similarity of prediction errors instead of the actual predictions; this helps in building ensembles with good scoring estimators and uncorrelated errors, which in principle and hopefully should lead to a “wisdom of crowds” kind of situation.\n\n\nHyperparameter optimization\nThe PoniardBaseEstimator.tune_estimator method can be used to optimize the hyperparameters of a given estimator, either by passing a grid of parameters or using the inbuilt ones available for default estimators. The tuned estimator will be added to the list of estimators and will be scored the next time PoniardBaseEstimator.fit is called.\n\n\nPlotting\nThe plot accessor provides several plotting methods based on the attached Poniard estimator instance. These Plotly plots are based on a default template, but can be modified by passing a different PoniardPlotFactory to the Poniard plot_options argument.\n\n\nPlugin system\nThe plugins argument in Poniard estimators takes a plugin or list of plugins that subclass BasePlugin. These plugins have access to the Poniard estimator instance and hook onto different sections of the process, for example, on setup start, on fit end, on remove estimator, etc.\nThis makes it easy for third parties to extend Poniard’s functionality.\nTwo plugins are baked into Poniard. 1. Weights and Biases: logs your data, plots, runs wandb scikit-learn analysis, saves model artifacts, etc. 2. Pandas Profiling: generates an HTML report of the features and target. If the Weights and Biases plugin is present, also logs this report to the wandb run.\nThe requirements for these plugins are not included in the base Poniard dependencies, so you can safely ignore them if you don’t intend to use them."
  },
  {
    "objectID": "index.html#design-philosophy",
    "href": "index.html#design-philosophy",
    "title": "Poniard",
    "section": "Design philosophy",
    "text": "Design philosophy\n\nNot another dependency\nWe try very hard to avoid cluttering the environment with stuff you won’t use outside of this library. Poniard’s dependencies are:\n\nscikit-learn (duh)\npandas\nXGBoost\nPlotly\ntqdm\nThat’s it!\n\nApart from tqdm and possibly Plotly, all dependencies most likely were going to be installed anyway, so Poniard’s added footprint should be small.\n\n\nWe don’t do that here (AutoML)\nPoniard tries not to take control away from the user. As such, it is not designed to perform 2 hours of feature engineering and selection, try every model under the sun together with endless ensembles and select the top performing model according to some metric.\nInstead, it strives to abstract away some of the boilerplate code needed to fit and compare a number of models and allows the user to decide what to do with the results.\nPoniard can be your first stab at a prediction problem, but it definitely shouldn’t be your last one.\n\n\nOpinionated with a few exceptions\nWhile some parameters can be modified to control how variable type inference and preprocessing are performed, the API is designed to prevent parameter proliferation.\n\n\nCross validate all the things\nEverything in Poniard is run with cross validation by default, and in fact no relevant functionality can be used without cross validation.\n\n\nUse baselines\nA dummy estimator is always included in model comparisons so you can gauge whether your model is better than a dumb strategy.\n\n\nFast TTFM (time to first model)\nPreprocessing tries to ensure that your models run successfully without significant data munging. By default, Poniard imputes missing data and one-hot encodes or target encodes (depending on cardinality) inferred categorical variables, which in most cases is enough for scikit-learn algorithms to fit without complaints. Additionally, it scales numeric data and drops features with a single unique value."
  },
  {
    "objectID": "index.html#similar-projects",
    "href": "index.html#similar-projects",
    "title": "Poniard",
    "section": "Similar projects",
    "text": "Similar projects\nPoniard is not a groundbreaking idea, and a number of libraries follow a similar approach.\nATOM is perhaps the most similar library to Poniard, albeit with a different approach to the API.\nLazyPredict is similar in that it runs multiple estimators and provides results for various metrics. Unlike Poniard, by default it tries most scikit-learn estimators, and is not based on cross validation.\nPyCaret is a whole other beast that includes model explainability, deployment, plotting, NLP, anomaly detection, etc., which leads to a list of dependencies several times larger than Poniard’s, and a more complicated API."
  },
  {
    "objectID": "error_analysis.error_analysis.html",
    "href": "error_analysis.error_analysis.html",
    "title": "Error analysis",
    "section": "",
    "text": "Inspecting which classes or target ranges a model struggles with the most is a vital step in the model building iterative process. ErrorAnalyzer aims at streamlining this process.\n\nsource\n\nErrorAnalyzer\n\n ErrorAnalyzer (task:str)\n\nAn error analyzer for predictive models.\nCompare ground truth and predicted target and rank the largest deviations (either by probabilities for classifiers and actual values for regressors).\nThis class is tightly integrated with PoniardBaseEstimator, but does not require it.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntask\nstr\nThe machine learning task. Either ‘regression’ or ‘classification’.\n\n\n\n\nsource\n\n\nErrorAnalyzer.from_poniard\n\n ErrorAnalyzer.from_poniard\n                             (poniard:poniard.estimators.core.PoniardBaseE\n                             stimator,\n                             estimator_names:Union[str,Sequence[str]])\n\nUse a Poniard instance to instantiate ErrorAnalyzer.\nAutomatically sets the task and gives access to the underlying data.\n\n\n\n\nType\nDetails\n\n\n\n\nponiard\nPoniardBaseEstimator\nA PoniardClassifier or PoniardRegressor instance.\n\n\nestimator_names\ntyping.Union[str, typing.Sequence[str]]\nArray of estimators for which to compute errors.\n\n\nReturns\n\nAn instance of the class.\n\n\n\n\nsource\n\n\nErrorAnalyzer.rank_errors\n\n ErrorAnalyzer.rank_errors (y:Union[numpy.ndarray,pandas.core.series.Serie\n                            s,pandas.core.frame.DataFrame,NoneType]=None, \n                            predictions:Union[numpy.ndarray,pandas.core.se\n                            ries.Series,pandas.core.frame.DataFrame,NoneTy\n                            pe]=None, probas:Union[numpy.ndarray,pandas.co\n                            re.series.Series,pandas.core.frame.DataFrame,N\n                            oneType]=None, exclude_correct:bool=True)\n\nCompare the y ground truth with predictions and probas and sort by the largest deviations.\nIf ErrorAnalyzer.from_poniard was used, no data needs to be passed to this method.\nIn this context “error” refers to:\n\nmisclassified samples in binary and multiclass problems.\nmisclassified samples in any of the labels for multilabel problems.\nsamples with predicted values outside the truth - 1SD <-> truth + 1SD range for regression problems.\nsamples with predicted values outside the truth - 1SD <-> truth + 1SD range in any of the targets for multioutput regression problems.\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\ntyping.Union[numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame, NoneType]\nNone\nGround truth target.\n\n\npredictions\ntyping.Union[numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame, NoneType]\nNone\nPredicted target.\n\n\nprobas\ntyping.Union[numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame, NoneType]\nNone\nPredicted probabilities for each class in classification tasks.\n\n\nexclude_correct\nbool\nTrue\nWhether to exclude correctly predicted samples in the output ranking. Default True.\n\n\nReturns\nDict\n\nRanked errors\n\n\n\nErrorAnalyzer.rank_errors works for simple classification…\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom poniard import PoniardClassifier\n\n\nx, y = load_breast_cancer(return_X_y=True, as_frame=True)\npnd = (\n    PoniardClassifier(estimators=[KNeighborsClassifier(), LogisticRegression()])\n    .setup(x, y, show_info=False)\n    .fit()\n)\nerror_analysis = ErrorAnalyzer.from_poniard(\n    pnd, [\"KNeighborsClassifier\", \"LogisticRegression\"]\n)\nranked_errors = error_analysis.rank_errors()\nranked_errors[\"LogisticRegression\"][\"values\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      y\n      prediction\n      proba_0\n      proba_1\n      error\n    \n  \n  \n    \n      297\n      0\n      1\n      0.002394\n      0.997606\n      0.997606\n    \n    \n      73\n      0\n      1\n      0.060207\n      0.939793\n      0.939793\n    \n    \n      40\n      0\n      1\n      0.062019\n      0.937981\n      0.937981\n    \n    \n      135\n      0\n      1\n      0.115223\n      0.884777\n      0.884777\n    \n    \n      190\n      0\n      1\n      0.215570\n      0.784430\n      0.784430\n    \n    \n      263\n      0\n      1\n      0.278617\n      0.721383\n      0.721383\n    \n    \n      68\n      1\n      0\n      0.694271\n      0.305729\n      0.694271\n    \n    \n      213\n      0\n      1\n      0.344298\n      0.655702\n      0.655702\n    \n    \n      146\n      0\n      1\n      0.397514\n      0.602486\n      0.602486\n    \n    \n      541\n      1\n      0\n      0.585451\n      0.414549\n      0.585451\n    \n    \n      363\n      1\n      0\n      0.524384\n      0.475616\n      0.524384\n    \n    \n      255\n      0\n      1\n      0.490481\n      0.509519\n      0.509519\n    \n  \n\n\n\n\nAs well as more complicated setups, such as multioutput regression.\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom poniard import PoniardRegressor\nfrom poniard.preprocessing import PoniardPreprocessor\n\n\nx, y = make_regression(\n    n_samples=1000,\n    n_features=10,\n    n_targets=2,\n    n_informative=3,\n    noise=50,\n    random_state=0,\n)\nx += np.random.normal()\nprep = PoniardPreprocessor(numeric_threshold=10)\npnd = (\n    PoniardRegressor(\n        estimators={\n            \"lr\": MultiOutputRegressor(LinearRegression()),\n            \"knn\": MultiOutputRegressor(KNeighborsRegressor()),\n        },\n        custom_preprocessor=prep,\n    )\n    .setup(x, y, show_info=False)\n    .fit()\n)\nerror_analysis = ErrorAnalyzer.from_poniard(pnd, [\"lr\", \"knn\"])\nranked_errors = error_analysis.rank_errors()\nranked_errors[\"knn\"][\"values\"]\n\n/Users/rafxavier/Documents/Repos/personal/poniard/poniard/preprocessing/core.py:145: UserWarning: TargetEncoder is not supported for multilabel or multioutput targets. Switching to OrdinalEncoder.\n  ) = self._setup_transformers()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      y_0\n      y_1\n      prediction_0\n      prediction_1\n      error_0\n      error_1\n      error\n    \n  \n  \n    \n      679\n      -285.183722\n      -361.210314\n      -83.567331\n      -174.502727\n      201.616391\n      186.707587\n      194.161989\n    \n    \n      580\n      -206.914531\n      -316.893779\n      -68.562490\n      -119.157106\n      138.352042\n      197.736673\n      168.044357\n    \n    \n      466\n      -193.559624\n      -270.201613\n      -47.648875\n      -83.285570\n      145.910749\n      186.916043\n      166.413396\n    \n    \n      543\n      166.559570\n      307.797957\n      44.254690\n      110.607538\n      122.304880\n      197.190419\n      159.747649\n    \n    \n      110\n      199.955678\n      175.857068\n      0.293321\n      62.923251\n      199.662358\n      112.933817\n      156.298087\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      563\n      138.150171\n      34.382971\n      60.332156\n      27.100214\n      77.818015\n      7.282756\n      42.550386\n    \n    \n      911\n      -94.526886\n      -32.665129\n      -15.892835\n      -27.263218\n      78.634051\n      5.401912\n      42.017982\n    \n    \n      441\n      -6.393304\n      65.144850\n      73.175490\n      61.991206\n      79.568794\n      3.153644\n      41.361219\n    \n    \n      582\n      1.808688\n      -64.975064\n      -76.728970\n      -61.481938\n      78.537658\n      3.493126\n      41.015392\n    \n    \n      794\n      60.170481\n      -36.460608\n      -17.018031\n      -35.620963\n      77.188512\n      0.839645\n      39.014078\n    \n  \n\n302 rows × 7 columns\n\n\n\nIt can also be used without Poniard.\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\nfrom poniard import PoniardRegressor\nfrom poniard.preprocessing import PoniardPreprocessor\n\n\nx, y = make_regression(\n    n_samples=2000,\n    n_features=10,\n    n_targets=1,\n    n_informative=3,\n    noise=50,\n    random_state=0,\n)\nx += np.random.normal()\ny_pred = y.copy()\ny_pred[np.random.randint(0, y.shape[0], 50)] = np.random.normal()\n\nerror_analysis = ErrorAnalyzer(task=\"regression\")\nranked_errors = error_analysis.rank_errors(y, y_pred)\nranked_errors[\"values\"]\n\n\n\n\n\n  \n    \n      \n      y\n      prediction\n      error\n    \n  \n  \n    \n      1050\n      -444.569706\n      -0.686456\n      443.883250\n    \n    \n      765\n      370.930877\n      -0.686456\n      371.617332\n    \n    \n      1228\n      263.743690\n      -0.686456\n      264.430145\n    \n    \n      1596\n      -256.521733\n      -0.686456\n      255.835277\n    \n    \n      1585\n      231.575997\n      -0.686456\n      232.262453\n    \n    \n      274\n      -224.219953\n      -0.686456\n      223.533497\n    \n    \n      1580\n      -214.900591\n      -0.686456\n      214.214136\n    \n    \n      379\n      -196.567862\n      -0.686456\n      195.881407\n    \n    \n      1376\n      188.622308\n      -0.686456\n      189.308763\n    \n    \n      87\n      -183.694218\n      -0.686456\n      183.007762\n    \n    \n      189\n      -165.369727\n      -0.686456\n      164.683272\n    \n    \n      780\n      162.247316\n      -0.686456\n      162.933772\n    \n    \n      655\n      -159.524622\n      -0.686456\n      158.838167\n    \n    \n      1421\n      155.527671\n      -0.686456\n      156.214126\n    \n    \n      277\n      -142.867757\n      -0.686456\n      142.181301\n    \n  \n\n\n\n\n\nsource\n\n\nErrorAnalyzer.merge_errors\n\n ErrorAnalyzer.merge_errors (errors:Dict[str,Dict[str,Union[pandas.core.fr\n                             ame.DataFrame,pandas.core.series.Series]]])\n\nMerge multiple error rankings. This is particularly useful when evaluating multiple estimators.\nCompute how many estimators had the specific error and the average error between them.\nThis method works best when using ErrorAnalyzer.from_poniard, since errors can be the output of ErrorAnalyzer.rank_errors. However, this is not required; as long as errors is properly defined ({str: {str: pd.DataFrame, str: pd.Series}})\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nerrors\ntyping.Dict[str, typing.Dict[str, typing.Union[pandas.core.frame.DataFrame, pandas.core.series.Series]]]\nDictionary of errors and error indexes.\n\n\nReturns\nDict\nMerged errors\n\n\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n\n\nfrom poniard import PoniardClassifier\n\n\nx, y = load_iris(return_X_y=True, as_frame=True)\npnd = (\n    PoniardClassifier(\n        estimators=[\n            LogisticRegression(),\n            RandomForestClassifier(),\n            HistGradientBoostingClassifier(),\n        ]\n    )\n    .setup(x, y, show_info=False)\n    .fit()\n)\nerror_analysis = ErrorAnalyzer.from_poniard(\n    pnd,\n    [\"RandomForestClassifier\", \"LogisticRegression\", \"HistGradientBoostingClassifier\"],\n)\nranked_errors = error_analysis.rank_errors()\nmerged_errors = error_analysis.merge_errors(ranked_errors)\nmerged_errors[\"values\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      mean_error\n      freq\n      estimators\n    \n    \n      index\n      \n      \n      \n    \n  \n  \n    \n      106\n      0.848222\n      3\n      [RandomForestClassifier, LogisticRegression, H...\n    \n    \n      70\n      0.842455\n      3\n      [RandomForestClassifier, LogisticRegression, H...\n    \n    \n      77\n      0.824483\n      3\n      [RandomForestClassifier, LogisticRegression, H...\n    \n    \n      119\n      0.817890\n      3\n      [RandomForestClassifier, LogisticRegression, H...\n    \n    \n      133\n      0.741060\n      3\n      [RandomForestClassifier, LogisticRegression, H...\n    \n    \n      83\n      0.909402\n      2\n      [RandomForestClassifier, HistGradientBoostingC...\n    \n    \n      72\n      0.793324\n      2\n      [RandomForestClassifier, HistGradientBoostingC...\n    \n    \n      129\n      0.763608\n      2\n      [RandomForestClassifier, HistGradientBoostingC...\n    \n    \n      138\n      0.610000\n      1\n      [RandomForestClassifier]\n    \n    \n      134\n      0.529012\n      1\n      [LogisticRegression]\n    \n  \n\n\n\n\n\nsource\n\n\nErrorAnalyzer.analyze_target\n\n ErrorAnalyzer.analyze_target (errors_idx:pandas.core.series.Series, y:Uni\n                               on[numpy.ndarray,pandas.core.series.Series,\n                               pandas.core.frame.DataFrame,NoneType]=None,\n                               reg_bins:int=5, as_ratio:bool=False,\n                               wrt_target:bool=False)\n\nAnalyze which target classes/ranges have the most errors and compare with observed target distribution.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nerrors_idx\nSeries\n\nIndex of ranked errors.\n\n\ny\ntyping.Union[numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame, NoneType]\nNone\nGround truth. Not needed if using ErrorAnalyzer.from_poniard.\n\n\nreg_bins\nint\n5\nNumber of bins in which to place ground truth targets for regression tasks.\n\n\nas_ratio\nbool\nFalse\nWhether to show error ratios instead of error counts per class/bin. Default False.\n\n\nwrt_target\nbool\nFalse\nWhether to compute counts of errors or error ratios with respectto the ground truth. Default False.\n\n\nReturns\npd.DataFrame\n\nCounts per error.\n\n\n\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\nfrom poniard import PoniardRegressor\n\n\nx, y = load_diabetes(return_X_y=True, as_frame=True)\npnd = PoniardRegressor(estimators=LinearRegression()).setup(x, y, show_info=False).fit()\nerror_analysis = ErrorAnalyzer.from_poniard(pnd, [\"LinearRegression\"])\nranked_errors = error_analysis.rank_errors()\n\nerror_analysis.analyze_target(ranked_errors[\"LinearRegression\"][\"idx\"])\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      0_errors\n      0_target\n    \n    \n      bins\n      \n      \n    \n  \n  \n    \n      (232.0, 346.0]\n      33\n      88\n    \n    \n      (24.999, 77.0]\n      18\n      91\n    \n    \n      (77.0, 115.0]\n      9\n      87\n    \n    \n      (168.0, 232.0]\n      9\n      87\n    \n    \n      (115.0, 168.0]\n      5\n      89\n    \n  \n\n\n\n\n\nerror_analysis.analyze_target(ranked_errors[\"LinearRegression\"][\"idx\"], wrt_target=True)\n\nbins\n(232.0, 346.0]    0.375000\n(24.999, 77.0]    0.197802\n(77.0, 115.0]     0.103448\n(168.0, 232.0]    0.103448\n(115.0, 168.0]    0.056180\ndtype: float64\n\n\n\nsource\n\n\nErrorAnalyzer.analyze_features\n\n ErrorAnalyzer.analyze_features (errors_idx:pandas.core.series.Series, X:U\n                                 nion[numpy.ndarray,pandas.core.series.Ser\n                                 ies,pandas.core.frame.DataFrame,NoneType]\n                                 =None, features:Optional[Sequence[Union[s\n                                 tr,int]]]=None, estimator_name:Union[str,\n                                 sklearn.base.BaseEstimator,NoneType]=None\n                                 , n_features:Union[int,float,NoneType]=No\n                                 ne)\n\nCross tabulate features with prediction errors.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nerrors_idx\nSeries\n\nIndex of ranked errors.\n\n\nX\ntyping.Union[numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame, NoneType]\nNone\nFeatures array. Not needed if using ErrorAnalyzer.from_poniard.\n\n\nfeatures\ntyping.Optional[typing.Sequence[typing.Union[str, int]]]\nNone\nArray of features to analyze. If None, all features will be analyzed.\n\n\nestimator_name\ntyping.Union[str, sklearn.base.BaseEstimator, NoneType]\nNone\nOnly valid if using ErrorAnalyzer.from_poniard. Allows using an estimator tocompute permutation importances and analyzing only the top n_features.\n\n\nn_features\ntyping.Union[int, float, NoneType]\nNone\nHow many features to analyze based on permutation importances.\n\n\nReturns\nDict[str, pd.DataFrame]\n\nPer feature summary.\n\n\n\n\nerror_analysis.analyze_features(ranked_errors[\"LinearRegression\"][\"idx\"])[\"age\"]\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      error\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      368.0\n      0.000467\n      0.047873\n      -0.107226\n      -0.035483\n      0.005383\n      0.038076\n      0.110727\n    \n    \n      1\n      74.0\n      -0.002324\n      0.046585\n      -0.096328\n      -0.037299\n      0.005383\n      0.026270\n      0.110727"
  },
  {
    "objectID": "guide/preprocessing.html",
    "href": "guide/preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Poniard tries to apply minimal preprocessing to data. In general, it just tries to make sure that models fit correctly without introducing signifcant transformation overhead. In particular, there is no anomaly detection, dimensionality reduction, clustering, resampling, feature creation from polynomial interactions, feature selection, etc.\nThis is so the user always knows what’s going on.\nHowever, the default options may not be suitable for your data or objectives, so these can be set during initialization or modified afterwards."
  },
  {
    "objectID": "guide/preprocessing.html#default-preprocessing-pipeline",
    "href": "guide/preprocessing.html#default-preprocessing-pipeline",
    "title": "Preprocessing",
    "section": "Default preprocessing pipeline",
    "text": "Default preprocessing pipeline\nThe list of default transformations is:\n\nMissing data imputation.\nZ-score scaling for numeric variables.\nOne-hot encoding for low cardinality categorical variables.\nTarget encoding for the remaining categorical variables. This is a custom transformer based on Micci-Barreca, 2001, with implementation heavily based on Dirty Cat. If the task is multilabel or multioutput, ordinal encoding will be used instead.\nDatetime encoding for datetime variables. This also uses a custom transformer that extracts multiple datetime levels.\nZero-variance feature elimination.\n\nThis includes some type inference logic that decides whether a given feature is either numeric, categorical high cardinality, categorical low cardinality or datetime (see Type inference).\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame(\n    {\n        \"type\": random.choices([\"house\", \"apartment\"], k=500),\n        \"age\": rng.uniform(1, 200, 500).astype(int),\n        \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n        \"rating\": random.choices(range(50), k=500),\n        \"target\": random.choices([0, 1], k=500),\n    }\n)\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier().setup(X, y)\npnd.preprocessor\n\n\n                         Setup info\n                         Target\n                             Type: binary\n                             Shape: (500,)\n                             Unique values: 2\n                             Metrics\n                             Main metric: roc_auc\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 50\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\n\n\n\n\n\n\nEmpty subpreprocessors\n\n\n\nIf no features are assigned to a subpreprocessor (like datetime_preprocessor or categorical_low_preprocessor), then it will be dropped. This does not affect results as scikit-learn effectively ignores transformers with no assigned features, but it makes the HTML representation cleaner."
  },
  {
    "objectID": "guide/preprocessing.html#type-inference",
    "href": "guide/preprocessing.html#type-inference",
    "title": "Preprocessing",
    "section": "Type inference",
    "text": "Type inference\nType inference is governed by the input data types and two thresholds.\nNumber features (as defined by numpy) with unique values greater than numeric_threshold will be treated as numeric, with the remainder being treated as non-numeric. If this parameter is a float, the actual threshold is numeric_threshold * samples.\nNon-numeric features (either because they are number features below numeric_threshold or they are non-number features like strings) with unique values greater than cardinality_threshold will be considered high cardinality. Likewise, in the case of a float value, the threshold is cardinality_threshold * samples.\nThese thresholds are part of PoniardPreprocessor, which is the default preprocessor used in PoniardBaseEstimator.\nDefaults are set at reasonable limits, but do pay attention to the output of PoniardBaseEstimator.setup as it might expose misclassified features. In that scenario there’s three options:\n\npass a PoniardPreprocessor with different thresholds that better acommodate the dataset to custom_preprocessor.\npass a sklearn transformer (including pipelines) to custom_preprocessor that applies appropiate transformations to different sets of features.\nuse the PoniardBaseEstimator.reassign_types method to explicitly assign features to the three categories.\n\nIn the following example, PoniardBaseEstimator.reassign_types is used to make every feature numeric as far as preprocessing goes.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom poniard import PoniardRegressor\n\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nreg = PoniardRegressor()\nreg.setup(X, y)\nreg.preprocessor\n\n\n                         Setup info\n                         Target\n                             Type: continuous\n                             Shape: (20640,)\n                             Unique values: 3842\n                             Metrics\n                             Main metric: neg_mean_squared_error\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 2064\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      MedInc\n      HouseAge\n      \n      \n    \n    \n      1\n      AveRooms\n      Latitude\n      \n      \n    \n    \n      2\n      AveBedrms\n      Longitude\n      \n      \n    \n    \n      3\n      Population\n      \n      \n      \n    \n    \n      4\n      AveOccup\n      \n      \n      \n    \n  \n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge', 'Latitude',\n                                                   'Longitude'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge', 'Latitude',\n                                                   'Longitude'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['MedInc', 'AveRooms', 'AveBedrms',\n                                  'Population', 'AveOccup']),\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='regression'))]),\n                                 ['HouseAge', 'Latitude', 'Longitude'])])numeric_preprocessor['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_high_preprocessor['HouseAge', 'Latitude', 'Longitude']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='regression')VarianceThresholdVarianceThreshold()\n\n\n\nreg.reassign_types(\n    numeric=[\n        \"HouseAge\",\n        \"Latitude\",\n        \"Longitude\",\n    ]\n)\nreg.preprocessor\n\n\nAssigned feature types:\n                            \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      MedInc\n      \n      \n      \n    \n    \n      1\n      AveRooms\n      \n      \n      \n    \n    \n      2\n      AveBedrms\n      \n      \n      \n    \n    \n      3\n      Population\n      \n      \n      \n    \n    \n      4\n      AveOccup\n      \n      \n      \n    \n    \n      5\n      HouseAge\n      \n      \n      \n    \n    \n      6\n      Latitude\n      \n      \n      \n    \n    \n      7\n      Longitude\n      \n      \n      \n    \n  \n\n\n\nPipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: PipelinePipeline(steps=[('numeric_imputer', SimpleImputer()),\n                ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()VarianceThresholdVarianceThreshold()"
  },
  {
    "objectID": "guide/preprocessing.html#modifying-the-default-preprocessor-during-construction",
    "href": "guide/preprocessing.html#modifying-the-default-preprocessor-during-construction",
    "title": "Preprocessing",
    "section": "Modifying the default preprocessor during construction",
    "text": "Modifying the default preprocessor during construction\nCombining properly setup feature types with the scaler, numeric_imputer and high_cardinality_encoder parameters in PoniardPreprocessor allows almost complete customization of the default preprocessing pipeline.\nThese three parameters take strings representing transformers (as in scaler=\"minmax\" will use scikit-learn’s MinMaxScaler, see the reference), and also accept scikit-learn transformers and pipelines.\nFor now, we are deliberately not providing options for the categorical imputer (a SimpleImputer(strategy=\"most_frequent\") is used) or the low cardinality categorical encoder (always OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse=False)). While this is not set in stone, we feel that these are less debatable.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.impute import KNNImputer\nfrom poniard import PoniardRegressor\nfrom poniard.preprocessing import PoniardPreprocessor\n\n\npreprocessor = PoniardPreprocessor(numeric_imputer=KNNImputer(), scaler=\"robust\")\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nreg = PoniardRegressor(custom_preprocessor=preprocessor)\nreg.setup(X, y)\nreg.reassign_types(\n    numeric=[\n        \"AveRooms\",\n        \"AveBedrms\",\n        \"Population\",\n        \"AveOccup\",\n        \"Latitude\",\n        \"Longitude\",\n    ],\n    categorical_high=[\"HouseAge\"],\n)\nreg.preprocessor\n\n\n                         Setup info\n                         Target\n                             Type: continuous\n                             Shape: (20640,)\n                             Unique values: 3842\n                             Metrics\n                             Main metric: neg_mean_squared_error\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 2064\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      MedInc\n      HouseAge\n      \n      \n    \n    \n      1\n      AveRooms\n      Latitude\n      \n      \n    \n    \n      2\n      AveBedrms\n      Longitude\n      \n      \n    \n    \n      3\n      Population\n      \n      \n      \n    \n    \n      4\n      AveOccup\n      \n      \n      \n    \n  \n\n\n\n\nAssigned feature types:\n                            \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      MedInc\n      HouseAge\n      \n      \n    \n    \n      1\n      AveRooms\n      \n      \n      \n    \n    \n      2\n      AveBedrms\n      \n      \n      \n    \n    \n      3\n      Population\n      \n      \n      \n    \n    \n      4\n      AveOccup\n      \n      \n      \n    \n    \n      5\n      Latitude\n      \n      \n      \n    \n    \n      6\n      Longitude\n      \n      \n      \n    \n  \n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   KNNImputer()),\n                                                                  ('scaler',\n                                                                   RobustScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup', 'Latitude',\n                                                   'Longitude']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   KNNImputer()),\n                                                                  ('scaler',\n                                                                   RobustScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup', 'Latitude',\n                                                   'Longitude']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  KNNImputer()),\n                                                 ('scaler', RobustScaler())]),\n                                 ['MedInc', 'AveRooms', 'AveBedrms',\n                                  'Population', 'AveOccup', 'Latitude',\n                                  'Longitude']),\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='regression'))]),\n                                 ['HouseAge'])])numeric_preprocessor['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']KNNImputerKNNImputer()RobustScalerRobustScaler()categorical_high_preprocessor['HouseAge']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='regression')VarianceThresholdVarianceThreshold()"
  },
  {
    "objectID": "guide/preprocessing.html#modifying-the-default-preprocessor-after-construction",
    "href": "guide/preprocessing.html#modifying-the-default-preprocessor-after-construction",
    "title": "Preprocessing",
    "section": "Modifying the default preprocessor after construction",
    "text": "Modifying the default preprocessor after construction\nTransformers and pipelines can be added to an existing preprocessor in any position with PoniardBaseEstimator.add_preprocessing_step\n\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\nreg.add_preprocessing_step(\n    (\"feature_selection\", SelectKBest(f_regression, k=5)), position=\"end\"\n)\nreg.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   KNNImputer()),\n                                                                  ('scaler',\n                                                                   RobustScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup', 'Latitude',\n                                                   'Longitude']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge'])])),\n                ('remove_invariant', VarianceThreshold()),\n                ('feature_selection',\n                 SelectKBest(k=5,\n                             score_func=<function f_regression at 0x17c88bf70>))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   KNNImputer()),\n                                                                  ('scaler',\n                                                                   RobustScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup', 'Latitude',\n                                                   'Longitude']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge'])])),\n                ('remove_invariant', VarianceThreshold()),\n                ('feature_selection',\n                 SelectKBest(k=5,\n                             score_func=<function f_regression at 0x17c88bf70>))])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  KNNImputer()),\n                                                 ('scaler', RobustScaler())]),\n                                 ['MedInc', 'AveRooms', 'AveBedrms',\n                                  'Population', 'AveOccup', 'Latitude',\n                                  'Longitude']),\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='regression'))]),\n                                 ['HouseAge'])])numeric_preprocessor['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']KNNImputerKNNImputer()RobustScalerRobustScaler()categorical_high_preprocessor['HouseAge']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='regression')VarianceThresholdVarianceThreshold()SelectKBestSelectKBest(k=5, score_func=<function f_regression at 0x17c88bf70>)"
  },
  {
    "objectID": "guide/preprocessing.html#use-a-custom-sklearn-preprocessor",
    "href": "guide/preprocessing.html#use-a-custom-sklearn-preprocessor",
    "title": "Preprocessing",
    "section": "Use a custom sklearn preprocessor",
    "text": "Use a custom sklearn preprocessor\nDuring init of either PoniardRegressor or PoniardClassifier (see docs for PoniardBaseEstimator which sets up most of the functionality), preprocess=False disables preprocessing altogether, while custom_preprocessor accepts a scikit-learn transformer (or pipeline/column transformer) that replaces the default Poniard transformation pipeline.\nLogically, there is no type inference involved when these options are used and full control is given to the user.\nIn the following example, we use TfidfVectorizer and Normalizer to process the 20 News Groups dataset.\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\n\nfrom poniard import PoniardClassifier\n\n\nX, y = fetch_20newsgroups(\n    return_X_y=True,\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    categories=(\"sci.crypt\", \"sci.electronics\", \"sci.med\"),\n)\npreprocessor = make_pipeline(TfidfVectorizer(), Normalizer())\npnd = PoniardClassifier(\n    estimators=[LogisticRegression()], custom_preprocessor=preprocessor\n)\npnd.setup(X, y)\npnd.preprocessor\n\n\n                         Setup info\n                         Target\n                             Type: multiclass\n                             Shape: (1780,)\n                             Unique values: 3\n                             Metrics\n                             Main metric: roc_auc_ovr\n                             \n\n\nPipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('normalizer', Normalizer())],\n         verbose=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('normalizer', Normalizer())],\n         verbose=0)TfidfVectorizerTfidfVectorizer()NormalizerNormalizer()\n\n\n\npnd.fit()\npnd.get_results()"
  },
  {
    "objectID": "guide/getting_started.html",
    "href": "guide/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "Essentially, a Poniard estimator is a set of scikit-learn estimators, a preprocessing strategy, a cross validation strategy and one or more metrics with which to score models.\nThe idea behind Poniard is to abstract away some of the boilerplate involved in fitting multiple models and comparing their cross validated results. However, a significant effort is made to keep everything flexible and as close to scikit-learn as possible.\nPoniard includes a PoniardClassifier and a PoniardRegressor, aligned with scikit-learn classifiers and regressors."
  },
  {
    "objectID": "guide/getting_started.html#basic-usage",
    "href": "guide/getting_started.html#basic-usage",
    "title": "Getting started",
    "section": "Basic usage",
    "text": "Basic usage\nIn the following example we will load a toy dataset (sklearn’s diabetes dataset, a simple regression task) and have at it with default parameters.\n\nfrom poniard import PoniardRegressor\nfrom sklearn.datasets import load_diabetes\n\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\npnd = PoniardRegressor()\npnd.setup(X, y)\n\n\n                         Setup info\n                         Target\n                             Type: continuous\n                             Shape: (442,)\n                             Unique values: 214\n                             Metrics\n                             Main metric: neg_mean_squared_error\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 44\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      \n      sex\n      \n    \n    \n      1\n      bmi\n      \n      \n      \n    \n    \n      2\n      bp\n      \n      \n      \n    \n    \n      3\n      s1\n      \n      \n      \n    \n    \n      4\n      s2\n      \n      \n      \n    \n    \n      5\n      s3\n      \n      \n      \n    \n    \n      6\n      s4\n      \n      \n      \n    \n    \n      7\n      s5\n      \n      \n      \n    \n    \n      8\n      s6\n      \n      \n      \n    \n  \n\n\n\nPoniardRegressor()\n\n\nOut of the box, you will get some useful information regarding the target variable and the features, as well information regarding current Poniard settings (first metric and thresholds). These are covered in detail later.\nOnce Poniard has parsed the data and built the preprocessing pipeline, we are free to run PoniardBaseEstimator.fit and PoniardBaseEstimator.get_results.\n\npnd.fit()\npnd.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.005163\n      0.002297\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003851\n      0.002356\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.101286\n      0.004822\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.279463\n      0.007446\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003590\n      0.002260\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.056158\n      0.002874\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.004315\n      0.002116\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003002\n      0.001690\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004800\n      0.001748\n    \n  \n\n\n\n\nIn those two lines, 9 different regression models were trained with cross validation and the average score for multiple metrics was printed.\n\n\n\n\n\n\nDummy estimators\n\n\n\nPoniard always include a DummyClassifier with strategy=\"prior\" or DummyRegressor with strategy=\"mean\" in order to have the absolute minimum baseline scores. Models should easily beat these, but you could be surprised.\n\n\nPoniard tries to provide good defaults everywhere.\n\nestimators: sklearn provides more than 40 classifiers and 50 regressors, but for most problems you can make do with a limited list of the most battle tested models. Poniard reflects that.\nmetrics: different metrics capture different aspects of the relationship between predictions and ground truth, so Poniard includes multiple suitable ones.\ncv: cross validation is a key aspect of the Poniard flow, and by default 5-fold validation is used.\n\n\n\n\n\n\n\nrandom_seed behavior\n\n\n\nPoniard estimators’ random_seed parameter is always set (if random_seed=None at initialization, it will be forced to 0) and injected into models and cross validators. The idea is to get a reproducible environment, including using the same cross validation folds for each model.\n\n\nDefault preprocessing deserves its own mention. By default, type inference will be run on the datasets’ features and transformations will be applied accordingly, which is handled by a PoniardPreprocessor built inside PoniardBaseEstimator. The end goal of the default preprocessor is to make models run without raising any errors.\nAs with most things in Poniard, the preprocessing pipeline can be modified (by passing a custom PoniardPreprocessor) or replaced entirely with the scikit-learn transformers you are used to.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4',\n                                  's5', 's6']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['sex'])])numeric_preprocessor['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['sex']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)VarianceThresholdVarianceThreshold()\n\n\nPoniard keeps track of which models it has cross validated, which means that\n\nIf a new one is added, it will not fit the existing ones.\nIf the preprocessor is changed after training, it will fit everything again.\n\n\nfrom sklearn.linear_model import SGDRegressor\n\n\npnd.add_estimators(SGDRegressor(max_iter=10000))\npnd.fit()\npnd.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.005163\n      0.002297\n    \n    \n      SGDRegressor\n      -2984.789764\n      -0.396191\n      -40.013179\n      0.487430\n      0.004640\n      0.001782\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003851\n      0.002356\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.101286\n      0.004822\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.279463\n      0.007446\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003590\n      0.002260\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.056158\n      0.002874\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.004315\n      0.002116\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003002\n      0.001690\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004800\n      0.001748\n    \n  \n\n\n\n\n\n\n\n\n\n\nSingle or array-like inputs\n\n\n\nAnywhere Poniard takes estimators or metrics, or strings representing them, a single element or a sequence of elements can be safely passed and will be handed gracefully.\n\n\nA quick view of an estimator is available through PoniardBaseEstimator.analyze_estimator.\n\npnd.analyze_estimator(\"SGDRegressor\", height=1000, width=800)"
  },
  {
    "objectID": "guide/getting_started.html#plots-when-get_results-is-not-enough",
    "href": "guide/getting_started.html#plots-when-get_results-is-not-enough",
    "title": "Getting started",
    "section": "Plots: when get_results is not enough",
    "text": "Plots: when get_results is not enough\nWhile a nicely formatted table is useful, graphical aid can make things go a lot smoother. Poniard estimators include a plot accessor that gives access to multiple prebuilt plots.\nCheck the plotting reference for a deeper dive.\n\nfig = pnd.plot.metrics(\n    kind=\"bar\", metrics=[\"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\"]\n)\nfig.show(\"notebook\")\n\n\n                                                \n\n\n\nfig = pnd.plot.residuals_histogram(estimator_names=[\"LinearRegression\", \"SGDRegressor\"])\nfig.show(\"notebook\")\n\n\n                                                \n\n\nBy using Plotly, users can have an easier time exploring charts, zooming in, selecting specific models, etc."
  },
  {
    "objectID": "guide/getting_started.html#a-reasonably-unified-api",
    "href": "guide/getting_started.html#a-reasonably-unified-api",
    "title": "Getting started",
    "section": "A reasonably unified API",
    "text": "A reasonably unified API\nSo far we have analyzed a regression task. Luckily, PoniardClassifier and PoniardRegressor differ only in default models, default cross validation strategy and default metrics.\n\nfrom poniard import PoniardClassifier\nfrom sklearn.datasets import load_wine\n\n\nX, y = load_wine(return_X_y=True, as_frame=True)\nclf = PoniardClassifier().setup(X, y, show_info=False)\n\n\nclf.fit()\nclf.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc_ovr\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      1.000000\n      0.983175\n      0.982828\n      0.983810\n      0.982571\n      0.004461\n      0.003472\n    \n    \n      RandomForestClassifier\n      0.999336\n      0.971905\n      0.973216\n      0.974098\n      0.972726\n      0.044476\n      0.007773\n    \n    \n      HistGradientBoostingClassifier\n      0.999311\n      0.971746\n      0.970350\n      0.976508\n      0.972109\n      0.268834\n      0.025884\n    \n    \n      SVC\n      0.999128\n      0.960635\n      0.960133\n      0.965079\n      0.960681\n      0.002355\n      0.002526\n    \n    \n      GaussianNB\n      0.998855\n      0.971905\n      0.973533\n      0.974098\n      0.972720\n      0.002148\n      0.002939\n    \n    \n      XGBClassifier\n      0.998213\n      0.949206\n      0.956410\n      0.950548\n      0.950512\n      0.020969\n      0.003620\n    \n    \n      KNeighborsClassifier\n      0.995903\n      0.960794\n      0.959845\n      0.965079\n      0.960468\n      0.001462\n      0.003054\n    \n    \n      DecisionTreeClassifier\n      0.945058\n      0.927302\n      0.933483\n      0.927961\n      0.928931\n      0.001737\n      0.002261\n    \n    \n      DummyClassifier\n      0.500000\n      0.399048\n      0.133016\n      0.333333\n      0.190095\n      0.001500\n      0.002517\n    \n  \n\n\n\n\n\nfig = clf.plot.confusion_matrix(estimator_name=\"LogisticRegression\")\nfig.show(\"notebook\")"
  },
  {
    "objectID": "guide/getting_started.html#intended-use",
    "href": "guide/getting_started.html#intended-use",
    "title": "Getting started",
    "section": "Intended use",
    "text": "Intended use\nIn the real world where real data lives, building machine learning models is not as simple as running an abstraction in 2 lines and calling it a day.\nOur preferred way of working is splitting the data in train-test sets before touching Poniard. That way you can pass the training data to PoniardBaseEstimator.setup and let the inbuilt cross validation handle model evaluation.\nWhen you are done, PoniardBaseEstimator.get_estimator simply returns a pipeline by name and optionally trains it with full data (which should be just training data), making it easy to continue working on models while preserving an unseen test set which can now be used to assess generalization power.\n\nclf.get_estimator(\"RandomForestClassifier\", retrain=False)\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())],\n                          verbose=0)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())],\n                          verbose=0)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(random_state=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: PipelinePipeline(steps=[('numeric_imputer', SimpleImputer()),\n                ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()VarianceThresholdVarianceThreshold()RandomForestClassifierRandomForestClassifier(random_state=0)"
  },
  {
    "objectID": "guide/end_to_end_example.html",
    "href": "guide/end_to_end_example.html",
    "title": "A slightly more complex case study",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom poniard import PoniardClassifier\nfrom poniard.plugins import WandBPlugin, PandasProfilingPlugin\nfrom poniard.error_analysis import ErrorAnalyzer\n\nWe’ll get the data and sample 10.000 observations to speed up training. Also, we’re going to cast pd.Categorical columns to object since scikit-learn doesn’t play will with them, drop the “fnlwgt” column as it refers to survey weights that should not provide any relevant information, and cast the target to 1-0.\n\n# Adult Census dataset\nX, y = fetch_openml(data_id=1590, return_X_y=True, as_frame=True)\n\nX = X.sample(n=10000, random_state=0).drop(\"fnlwgt\", axis=1)\ny = y.reindex(X.index)\n\ncategory_cols = X.select_dtypes(include=\"category\").columns\nX = X.astype({col: object for col in category_cols})\n\ny = y.replace({\">50K\": 1, \"<=50K\": 0})\n\nNext we split and pass only the training data to Poniard.\nWe’ll be using the 2 available plugins.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n\npnd = PoniardClassifier(\n    n_jobs=-1,\n    plugins=[WandBPlugin(project=\"adult-demo\"), PandasProfilingPlugin()],\n)\npnd.setup(X_train, y_train)\npnd.remove_estimators(\"SVC\")  # Doesn't scale nicely\n\nwandb: Currently logged in as: rxavier. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.3\n\n\nRun data is saved locally in /Users/rafxavier/Documents/Repos/personal/poniard/nbs/guide/wandb/run-20221203_231425-1n5yya80\n\n\nSyncing run wandering-butterfly-15 to Weights & Biases (docs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                         Setup info\n                         Target\n                             Type: binary\n                             Shape: (8000,)\n                             Unique values: 2\n                             Metrics\n                             Main metric: roc_auc\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 800\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      \n      age\n      education-num\n      \n    \n    \n      1\n      \n      capital-gain\n      workclass\n      \n    \n    \n      2\n      \n      capital-loss\n      education\n      \n    \n    \n      3\n      \n      hours-per-week\n      marital-status\n      \n    \n    \n      4\n      \n      native-country\n      occupation\n      \n    \n    \n      5\n      \n      \n      relationship\n      \n    \n    \n      6\n      \n      \n      race\n      \n    \n    \n      7\n      \n      \n      sex\n      \n    \n  \n\n\n\nPoniardClassifier(n_jobs=-1, plugins=[WandBPlugin(project='adult-demo'), PandasProfilingPlugin()])\n\n\nAs can be seen, Pandas Profiling already created a report and saved it to the default location. If ipywidgets is installed, the report will be included in the output.\nMeanwhile, Weights and Biases either logged in or prompted for a login, and started logging information about the run (preprocessor HTML representation, dataset, inferred types). Also, because plugins can check whether other plugins are included in a Poniard estimator (by using BasePlugin._check_plugin_used), wandb also uploaded the profile report.\nRight away there’s some misclassified features, so we’ll reassign them.\n\npnd.reassign_types(numeric=[\"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"])\n\n\nAssigned feature types:\n                            \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      native-country\n      education-num\n      \n    \n    \n      1\n      capital-gain\n      \n      workclass\n      \n    \n    \n      2\n      capital-loss\n      \n      education\n      \n    \n    \n      3\n      hours-per-week\n      \n      marital-status\n      \n    \n    \n      4\n      \n      \n      occupation\n      \n    \n    \n      5\n      \n      \n      relationship\n      \n    \n    \n      6\n      \n      \n      race\n      \n    \n    \n      7\n      \n      \n      sex\n      \n    \n  \n\n\n\nPoniardClassifier(n_jobs=-1, plugins=[WandBPlugin(project='adult-demo'), PandasProfilingPlugin()])\n\n\n\npnd.fit()\npnd.get_results()\n\n\n\n\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: Network error (ReadTimeout), entering retry loop.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\nwandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      HistGradientBoostingClassifier\n      0.915932\n      0.860000\n      0.750181\n      0.627740\n      0.683455\n      0.943964\n      0.059188\n    \n    \n      XGBClassifier\n      0.914350\n      0.859750\n      0.742949\n      0.638133\n      0.686398\n      2.057690\n      0.036659\n    \n    \n      LogisticRegression\n      0.898562\n      0.843250\n      0.714732\n      0.579968\n      0.640167\n      0.193686\n      0.027372\n    \n    \n      RandomForestClassifier\n      0.884420\n      0.835750\n      0.680117\n      0.600215\n      0.637618\n      6.557138\n      0.059342\n    \n    \n      KNeighborsClassifier\n      0.842528\n      0.815125\n      0.629923\n      0.562315\n      0.594060\n      0.058123\n      0.229496\n    \n    \n      GaussianNB\n      0.808289\n      0.583500\n      0.361538\n      0.931476\n      0.520086\n      0.055644\n      0.024651\n    \n    \n      DecisionTreeClassifier\n      0.743712\n      0.802375\n      0.587160\n      0.604880\n      0.595834\n      2.182848\n      0.024007\n    \n    \n      DummyClassifier\n      0.500000\n      0.759250\n      0.000000\n      0.000000\n      0.000000\n      0.058244\n      0.028908\n    \n  \n\n\n\n\nThe results table is nice, but we can also plot the metrics we’re interested in.\nSince there’s imbalance in y, we look at the ROC AUC and F1 score as well as accuracy.\n\npnd.plot.metrics(metrics=[\"f1\", \"roc_auc\", \"accuracy\"], kind=\"bar\")\n\n\n                                                \n\n\n\npnd.plot.roc_curve(estimator_names=[\"HistGradientBoostingClassifier\", \"XGBClassifier\"])\n\n\n\n\n\n\n\n\n                                                \n\n\nPerformance between HistGradientBoostingClassifier and XGBoost is very similar. We can go ahead and try to squeeze a bit more using hyperparameter tuning with PoniardBaseEstimator.tune_estimator.\nNote that we’re building custom parameter grids, since the grids included in Poniard are smaller and better suited to full grid search, instead of the random search we will use.\n\nname = \"HistGradientBoostingClassifier\"\ngrid = {\n    f\"{name}__learning_rate\": np.arange(0.1, 1.1, 0.1),\n    f\"{name}__max_iter\": np.arange(100, 520, 20),\n    f\"{name}__max_leaf_nodes\": np.arange(10, 110, 10),\n    f\"{name}__l2_regularization\": np.arange(0, 1.1, 0.1),\n}\npnd.tune_estimator(name, grid=grid, mode=\"random\", n_iter=100)\n\nPoniardClassifier(n_jobs=-1, plugins=[WandBPlugin(project='adult-demo'), PandasProfilingPlugin()])\n\n\nTo keep it reasonable, we tune XGBoost for 50 rounds as it takes a lot longer than HGB.\n\nname = \"XGBClassifier\"\ngrid = {\n    f\"{name}__n_estimators\": np.arange(100, 520, 20),\n    f\"{name}__max_depth\": np.arange(2, 21, 2),\n    f\"{name}__learning_rate\": np.arange(0.1, 1.1, 0.1),\n    f\"{name}__min_child_weight\": np.arange(1, 11, 1),\n}\npnd.tune_estimator(name, grid=grid, mode=\"random\", n_iter=50)\n\nPoniardClassifier(n_jobs=-1, plugins=[WandBPlugin(project='adult-demo'), PandasProfilingPlugin()])\n\n\nAfter tuning, the tuned estimators are added to PoniardBaseEstimator.pipelines, but they still need to be fit with CV. Calling PoniardBaseEstimator.fit will only run cross validation for new estimators only.\n\npnd.fit()\npnd.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      HistGradientBoostingClassifier_tuned\n      0.917760\n      0.862625\n      0.757019\n      0.631896\n      0.688738\n      0.503392\n      0.034052\n    \n    \n      XGBClassifier_tuned\n      0.916398\n      0.858750\n      0.744218\n      0.629814\n      0.682057\n      1.103070\n      0.022097\n    \n    \n      HistGradientBoostingClassifier\n      0.915932\n      0.860000\n      0.750181\n      0.627740\n      0.683455\n      0.943964\n      0.059188\n    \n    \n      XGBClassifier\n      0.914350\n      0.859750\n      0.742949\n      0.638133\n      0.686398\n      2.057690\n      0.036659\n    \n    \n      LogisticRegression\n      0.898562\n      0.843250\n      0.714732\n      0.579968\n      0.640167\n      0.193686\n      0.027372\n    \n    \n      RandomForestClassifier\n      0.884420\n      0.835750\n      0.680117\n      0.600215\n      0.637618\n      6.557138\n      0.059342\n    \n    \n      KNeighborsClassifier\n      0.842528\n      0.815125\n      0.629923\n      0.562315\n      0.594060\n      0.058123\n      0.229496\n    \n    \n      GaussianNB\n      0.808289\n      0.583500\n      0.361538\n      0.931476\n      0.520086\n      0.055644\n      0.024651\n    \n    \n      DecisionTreeClassifier\n      0.743712\n      0.802375\n      0.587160\n      0.604880\n      0.595834\n      2.182848\n      0.024007\n    \n    \n      DummyClassifier\n      0.500000\n      0.759250\n      0.000000\n      0.000000\n      0.000000\n      0.058244\n      0.028908\n    \n  \n\n\n\n\nFrom here we can get a summary of a given estimator’s performance with PoniardBaseEstimator.analyze_estimator.\n\npnd.analyze_estimator(\"HistGradientBoostingClassifier_tuned\")\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\nWe can also understand where errors are coming from by leveraging ErrorAnalyzer.\n\nanalyzer = ErrorAnalyzer.from_poniard(\n    pnd, estimator_names=\"HistGradientBoostingClassifier_tuned\"\n)\nerrors = analyzer.rank_errors()[\"HistGradientBoostingClassifier_tuned\"]\nerrors[\"values\"]\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      y\n      prediction\n      proba_0\n      proba_1\n      error\n    \n  \n  \n    \n      5953\n      1\n      0\n      0.999105\n      0.000895\n      0.999105\n    \n    \n      29128\n      1\n      0\n      0.997760\n      0.002240\n      0.997760\n    \n    \n      786\n      0\n      1\n      0.003175\n      0.996825\n      0.996825\n    \n    \n      11144\n      1\n      0\n      0.996473\n      0.003527\n      0.996473\n    \n    \n      2004\n      1\n      0\n      0.995659\n      0.004341\n      0.995659\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      37335\n      0\n      1\n      0.498782\n      0.501218\n      0.501218\n    \n    \n      17237\n      1\n      0\n      0.501033\n      0.498967\n      0.501033\n    \n    \n      29824\n      1\n      0\n      0.500940\n      0.499060\n      0.500940\n    \n    \n      18163\n      1\n      0\n      0.500752\n      0.499248\n      0.500752\n    \n    \n      35847\n      0\n      1\n      0.499532\n      0.500468\n      0.500468\n    \n  \n\n1099 rows × 5 columns\n\n\n\n\nanalyzer.analyze_target(errors[\"idx\"], as_ratio=True)\n\n\n\n\n\n  \n    \n      \n      0_errors\n      0_target\n    \n    \n      class\n      \n      \n    \n  \n  \n    \n      1\n      0.645132\n      0.24075\n    \n    \n      0\n      0.354868\n      0.75925\n    \n  \n\n\n\n\n\nanalyzer.analyze_target(errors[\"idx\"], wrt_target=True)\n\nclass\n1    0.368120\n0    0.064208\ndtype: float64\n\n\nAs expected, the model’s errors are concentrated on the rare class.\nWe can also get a glimpse on how the features differ between errors and non-errors.\n\nfeature_analysis = analyzer.analyze_features(errors[\"idx\"])\nfeature_analysis[\"marital-status\"]\n\n\n\n\n\n  \n    \n      marital-status\n      Divorced\n      Married-AF-spouse\n      Married-civ-spouse\n      Married-spouse-absent\n      Never-married\n      Separated\n      Widowed\n    \n    \n      error\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.145921\n      0.000435\n      0.397044\n      0.011013\n      0.373134\n      0.037096\n      0.035357\n    \n    \n      1\n      0.068244\n      0.000910\n      0.806187\n      0.003640\n      0.094631\n      0.010009\n      0.016379\n    \n  \n\n\n\n\n\nfeature_analysis[\"age\"]\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      error\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      6901.0\n      37.824518\n      13.926955\n      17.0\n      26.0\n      36.0\n      47.0\n      90.0\n    \n    \n      1\n      1099.0\n      43.348499\n      11.097495\n      18.0\n      35.0\n      42.0\n      50.0\n      90.0\n    \n  \n\n\n\n\n\nfeature_analysis[\"capital-gain\"]\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      error\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      6901.0\n      1265.274743\n      8104.97948\n      0.0\n      0.0\n      0.0\n      0.0\n      99999.0\n    \n    \n      1\n      1099.0\n      184.174704\n      2052.24528\n      0.0\n      0.0\n      0.0\n      0.0\n      41310.0\n    \n  \n\n\n\n\nWe might benefit from building an ensemble from multiple estimators. However, we’d like the estimators to complement each other, i.e, make different mistakes. This is when PoniardBaseEstimator.get_predictions_similarity can help.\n\npnd.get_predictions_similarity(on_errors=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      LogisticRegression\n      GaussianNB\n      KNeighborsClassifier\n      DecisionTreeClassifier\n      RandomForestClassifier\n      HistGradientBoostingClassifier\n      XGBClassifier\n      HistGradientBoostingClassifier_tuned\n      XGBClassifier_tuned\n    \n  \n  \n    \n      LogisticRegression\n      1.000000\n      0.016189\n      0.578868\n      0.464133\n      0.600827\n      0.688545\n      0.655039\n      0.711309\n      0.690366\n    \n    \n      GaussianNB\n      0.016189\n      1.000000\n      0.050041\n      0.065285\n      0.047443\n      0.014467\n      0.017282\n      0.018000\n      0.024393\n    \n    \n      KNeighborsClassifier\n      0.578868\n      0.050041\n      1.000000\n      0.480397\n      0.643573\n      0.582190\n      0.562862\n      0.568010\n      0.569996\n    \n    \n      DecisionTreeClassifier\n      0.464133\n      0.065285\n      0.480397\n      1.000000\n      0.618275\n      0.529288\n      0.533057\n      0.505370\n      0.496691\n    \n    \n      RandomForestClassifier\n      0.600827\n      0.047443\n      0.643573\n      0.618275\n      1.000000\n      0.659699\n      0.642367\n      0.627170\n      0.635312\n    \n    \n      HistGradientBoostingClassifier\n      0.688545\n      0.014467\n      0.582190\n      0.529288\n      0.659699\n      1.000000\n      0.846959\n      0.877606\n      0.833977\n    \n    \n      XGBClassifier\n      0.655039\n      0.017282\n      0.562862\n      0.533057\n      0.642367\n      0.846959\n      1.000000\n      0.844242\n      0.813421\n    \n    \n      HistGradientBoostingClassifier_tuned\n      0.711309\n      0.018000\n      0.568010\n      0.505370\n      0.627170\n      0.877606\n      0.844242\n      1.000000\n      0.839548\n    \n    \n      XGBClassifier_tuned\n      0.690366\n      0.024393\n      0.569996\n      0.496691\n      0.635312\n      0.833977\n      0.813421\n      0.839548\n      1.000000\n    \n  \n\n\n\n\nAnalyzing the results so far and the similarity table, it looks like including LogisticRegression along with the gradient boosters could be a good idea, so we’ll go ahead and tune LR.\n\nname = \"LogisticRegression\"\ngrid = {\n    f\"{name}__C\": np.geomspace(0.1, 100, 200),\n    f\"{name}__class_weight\": [None, \"balanced\"],\n}\npnd.tune_estimator(name, grid=grid, mode=\"random\", n_iter=100)\n\nPoniardClassifier(n_jobs=-1, plugins=[WandBPlugin(project='adult-demo'), PandasProfilingPlugin()])\n\n\nNext, we build a voting classifier with equal weights.\n\npnd.build_ensemble(\n    estimator_names=[\n        \"HistGradientBoostingClassifier_tuned\",\n        \"XGBClassifier_tuned\",\n        \"LogisticRegression_tuned\",\n    ],\n    method=\"voting\",\n    voting=\"soft\",\n)\npnd.fit()\npnd.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      HistGradientBoostingClassifier_tuned\n      0.917760\n      0.862625\n      0.757019\n      0.631896\n      0.688738\n      0.503392\n      0.034052\n    \n    \n      VotingClassifier\n      0.917293\n      0.863500\n      0.765881\n      0.623587\n      0.687299\n      2.131655\n      0.044481\n    \n    \n      XGBClassifier_tuned\n      0.916398\n      0.858750\n      0.744218\n      0.629814\n      0.682057\n      1.103070\n      0.022097\n    \n    \n      HistGradientBoostingClassifier\n      0.915932\n      0.860000\n      0.750181\n      0.627740\n      0.683455\n      0.943964\n      0.059188\n    \n    \n      XGBClassifier\n      0.914350\n      0.859750\n      0.742949\n      0.638133\n      0.686398\n      2.057690\n      0.036659\n    \n    \n      LogisticRegression_tuned\n      0.898678\n      0.842875\n      0.717942\n      0.571660\n      0.636357\n      0.131910\n      0.026045\n    \n    \n      LogisticRegression\n      0.898562\n      0.843250\n      0.714732\n      0.579968\n      0.640167\n      0.193686\n      0.027372\n    \n    \n      RandomForestClassifier\n      0.884420\n      0.835750\n      0.680117\n      0.600215\n      0.637618\n      6.557138\n      0.059342\n    \n    \n      KNeighborsClassifier\n      0.842528\n      0.815125\n      0.629923\n      0.562315\n      0.594060\n      0.058123\n      0.229496\n    \n    \n      GaussianNB\n      0.808289\n      0.583500\n      0.361538\n      0.931476\n      0.520086\n      0.055644\n      0.024651\n    \n    \n      DecisionTreeClassifier\n      0.743712\n      0.802375\n      0.587160\n      0.604880\n      0.595834\n      2.182848\n      0.024007\n    \n    \n      DummyClassifier\n      0.500000\n      0.759250\n      0.000000\n      0.000000\n      0.000000\n      0.058244\n      0.028908\n    \n  \n\n\n\n\nWe could even tune the ensemble weights.\n\nweights = []\n\nfor _ in range(200):\n    unscaled = np.random.uniform(0, 1, size=3)\n    scaled = unscaled / np.sum(unscaled)\n    weights.append(scaled.tolist())\n\npnd.tune_estimator(\n    \"VotingClassifier\",\n    grid={\"VotingClassifier__weights\": weights},\n    mode=\"random\",\n    n_iter=10,\n)\n\nPoniardClassifier(n_jobs=-1, plugins=[WandBPlugin(project='adult-demo'), PandasProfilingPlugin()])\n\n\n\npnd.fit()\npnd.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      VotingClassifier_tuned\n      0.918799\n      0.862625\n      0.760029\n      0.627224\n      0.687134\n      2.107462\n      0.050291\n    \n    \n      HistGradientBoostingClassifier_tuned\n      0.917760\n      0.862625\n      0.757019\n      0.631896\n      0.688738\n      0.503392\n      0.034052\n    \n    \n      VotingClassifier\n      0.917293\n      0.863500\n      0.765881\n      0.623587\n      0.687299\n      2.131655\n      0.044481\n    \n    \n      XGBClassifier_tuned\n      0.916398\n      0.858750\n      0.744218\n      0.629814\n      0.682057\n      1.103070\n      0.022097\n    \n    \n      HistGradientBoostingClassifier\n      0.915932\n      0.860000\n      0.750181\n      0.627740\n      0.683455\n      0.943964\n      0.059188\n    \n    \n      XGBClassifier\n      0.914350\n      0.859750\n      0.742949\n      0.638133\n      0.686398\n      2.057690\n      0.036659\n    \n    \n      LogisticRegression_tuned\n      0.898678\n      0.842875\n      0.717942\n      0.571660\n      0.636357\n      0.131910\n      0.026045\n    \n    \n      LogisticRegression\n      0.898562\n      0.843250\n      0.714732\n      0.579968\n      0.640167\n      0.193686\n      0.027372\n    \n    \n      RandomForestClassifier\n      0.884420\n      0.835750\n      0.680117\n      0.600215\n      0.637618\n      6.557138\n      0.059342\n    \n    \n      KNeighborsClassifier\n      0.842528\n      0.815125\n      0.629923\n      0.562315\n      0.594060\n      0.058123\n      0.229496\n    \n    \n      GaussianNB\n      0.808289\n      0.583500\n      0.361538\n      0.931476\n      0.520086\n      0.055644\n      0.024651\n    \n    \n      DecisionTreeClassifier\n      0.743712\n      0.802375\n      0.587160\n      0.604880\n      0.595834\n      2.182848\n      0.024007\n    \n    \n      DummyClassifier\n      0.500000\n      0.759250\n      0.000000\n      0.000000\n      0.000000\n      0.058244\n      0.028908\n    \n  \n\n\n\n\n\npnd.plot.confusion_matrix(\"VotingClassifier_tuned\", normalize=\"true\")\n\n\n\n\n\n                                                \n\n\n\npnd.plot.permutation_importance(\"VotingClassifier_tuned\")\n\n\n                                                \n\n\n\npnd.plot.partial_dependence(\"VotingClassifier_tuned\", \"marital-status\")\n\n\n                                                \n\n\n\nfinal = pnd.get_estimator(\"VotingClassifier_tuned\", retrain=True)\nfinal\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age',\n                                                                    'capital-gain',\n                                                                    'capital-loss',\n                                                                    'hours-per-week']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(st...\n                                                             predictor='auto',\n                                                             random_state=0,\n                                                             reg_alpha=0,\n                                                             reg_lambda=1,\n                                                             scale_pos_weight=1,\n                                                             subsample=1,\n                                                             tree_method='exact',\n                                                             use_label_encoder=False,\n                                                             validate_parameters=1,\n                                                             verbosity=0)),\n                                              ('LogisticRegression_tuned',\n                                               LogisticRegression(C=0.3255088599835058,\n                                                                  max_iter=5000,\n                                                                  random_state=0))],\n                                  verbose=0, voting='soft',\n                                  weights=[0.5963099656272408,\n                                           0.2756042875483777,\n                                           0.1280857468243813]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age',\n                                                                    'capital-gain',\n                                                                    'capital-loss',\n                                                                    'hours-per-week']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(st...\n                                                             predictor='auto',\n                                                             random_state=0,\n                                                             reg_alpha=0,\n                                                             reg_lambda=1,\n                                                             scale_pos_weight=1,\n                                                             subsample=1,\n                                                             tree_method='exact',\n                                                             use_label_encoder=False,\n                                                             validate_parameters=1,\n                                                             verbosity=0)),\n                                              ('LogisticRegression_tuned',\n                                               LogisticRegression(C=0.3255088599835058,\n                                                                  max_iter=5000,\n                                                                  random_state=0))],\n                                  verbose=0, voting='soft',\n                                  weights=[0.5963099656272408,\n                                           0.2756042875483777,\n                                           0.1280857468243813]))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'capital-gain',\n                                                   'capital-loss',\n                                                   'hours-per-week']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-h...\n                                                  ['education-num', 'workclass',\n                                                   'education',\n                                                   'marital-status',\n                                                   'occupation', 'relationship',\n                                                   'race', 'sex']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['native-country'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'capital-gain', 'capital-loss',\n                                  'hours-per-week']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_bi...\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['education-num', 'workclass', 'education',\n                                  'marital-status', 'occupation',\n                                  'relationship', 'race', 'sex']),\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['native-country'])])numeric_preprocessor['age', 'capital-gain', 'capital-loss', 'hours-per-week']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['education-num', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['native-country']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')VarianceThresholdVarianceThreshold()VotingClassifier_tuned: VotingClassifierVotingClassifier(estimators=[('HistGradientBoostingClassifier_tuned',\n                              HistGradientBoostingClassifier(l2_regularization=0.2,\n                                                             learning_rate=0.2,\n                                                             max_iter=120,\n                                                             max_leaf_nodes=10,\n                                                             random_state=0)),\n                             ('XGBClassifier_tuned',\n                              XGBClassifier(base_score=0.5, booster='gbtree',\n                                            colsample_bylevel=1,\n                                            colsample_bynode=1,\n                                            colsample_bytree=1,\n                                            enable_categorical=False, gamma=0,\n                                            gp...\n                                            predictor='auto', random_state=0,\n                                            reg_alpha=0, reg_lambda=1,\n                                            scale_pos_weight=1, subsample=1,\n                                            tree_method='exact',\n                                            use_label_encoder=False,\n                                            validate_parameters=1,\n                                            verbosity=0)),\n                             ('LogisticRegression_tuned',\n                              LogisticRegression(C=0.3255088599835058,\n                                                 max_iter=5000,\n                                                 random_state=0))],\n                 verbose=0, voting='soft',\n                 weights=[0.5963099656272408, 0.2756042875483777,\n                          0.1280857468243813])HistGradientBoostingClassifier_tunedHistGradientBoostingClassifierHistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.2,\n                               max_iter=120, max_leaf_nodes=10, random_state=0)XGBClassifier_tunedXGBClassifierXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.5, max_delta_step=0,\n              max_depth=4, min_child_weight=7, missing=nan,\n              monotone_constraints='()', n_estimators=100, n_jobs=1,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', use_label_encoder=False,\n              validate_parameters=1, verbosity=0)LogisticRegression_tunedLogisticRegressionLogisticRegression(C=0.3255088599835058, max_iter=5000, random_state=0)\n\n\n\nprint(classification_report(y_test, final.predict(X_test), digits=3))\n\n              precision    recall  f1-score   support\n\n           0      0.896     0.946     0.920      1518\n           1      0.793     0.654     0.717       482\n\n    accuracy                          0.875      2000\n   macro avg      0.845     0.800     0.818      2000\nweighted avg      0.871     0.875     0.871      2000"
  },
  {
    "objectID": "guide/main_parameters.html",
    "href": "guide/main_parameters.html",
    "title": "Main parameters",
    "section": "",
    "text": "At the core of Poniard lie the choice of estimators, metrics and CV strategy. While defaults might work for most cases, we try to keep it flexible."
  },
  {
    "objectID": "guide/main_parameters.html#estimators",
    "href": "guide/main_parameters.html#estimators",
    "title": "Main parameters",
    "section": "estimators",
    "text": "estimators\nEstimators can be passed as a dict of estimator_name: estimator_instance, as a sequence of estimator_instance or as a single estimator. When not specifying names, they will be obtained directly from the class.\nUsing a dictionary allows passing multiple instances of the same estimator with different parameters.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom poniard import PoniardClassifier\n\n\nX, y = make_classification(n_classes=3, n_features=5, n_informative=3)\npnd = PoniardClassifier(\n    estimators={\n        \"lr\": LogisticRegression(max_iter=5000),\n        \"lr_no_penalty\": LogisticRegression(max_iter=5000, penalty=\"none\"),\n        \"lda\": LinearDiscriminantAnalysis(),\n    }\n)\npnd.setup(X, y)\npnd.fit()\n\n\n                         Setup info\n                         Target\n                             Type: multiclass\n                             Shape: (100,)\n                             Unique values: 3\n                             Metrics\n                             Main metric: roc_auc_ovr\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 10\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      \n      \n      \n    \n    \n      1\n      1.0\n      \n      \n      \n    \n    \n      2\n      2.0\n      \n      \n      \n    \n    \n      3\n      3.0\n      \n      \n      \n    \n    \n      4\n      4.0\n      \n      \n      \n    \n  \n\n\n\n\n\n\nPoniardClassifier(estimators={'lr': LogisticRegression(max_iter=5000, random_state=0), 'lr_no_penalty': LogisticRegression(max_iter=5000, penalty='none', random_state=0), 'lda': LinearDiscriminantAnalysis()})\n\n\nSince we are in scikit-learn-land, most of the stuff you expect to work still works. For example, multilabel classification.\nHere we had to use a dictionary because estimator.__class__.__name__, which is used for assigning a name to each estimator when a list is passed, would be the same for both OneVsRestClassifier and they would be overwritten.\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nX, y = make_multilabel_classification(n_samples=1000, n_features=6)\npnd = PoniardClassifier(\n    estimators={\n        \"rf\": OneVsRestClassifier(RandomForestClassifier()),\n        \"lr\": OneVsRestClassifier(LogisticRegression(max_iter=5000)),\n    }\n)\npnd.setup(X, y, show_info=False)\npnd.fit()\n\n/Users/rafxavier/Documents/Repos/personal/poniard/poniard/preprocessing/core.py:145: UserWarning: TargetEncoder is not supported for multilabel or multioutput targets. Switching to OrdinalEncoder.\n  ) = self._setup_transformers()\n\n\n\n\n\nPoniardClassifier(estimators={'rf': OneVsRestClassifier(estimator=RandomForestClassifier()), 'lr': OneVsRestClassifier(estimator=LogisticRegression(max_iter=5000))})\n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      rf\n      0.841913\n      0.404\n      0.718702\n      0.592022\n      0.635481\n      0.568812\n      0.048382\n    \n    \n      lr\n      0.801743\n      0.320\n      0.651472\n      0.532183\n      0.570917\n      0.142863\n      0.009468\n    \n    \n      DummyClassifier\n      0.500000\n      0.093\n      0.194000\n      0.360000\n      0.251713\n      0.004363\n      0.006814"
  },
  {
    "objectID": "guide/main_parameters.html#metrics",
    "href": "guide/main_parameters.html#metrics",
    "title": "Main parameters",
    "section": "metrics",
    "text": "metrics\nMetrics can be passed as a list of strings such as \"accuracy\" or \"neg_mean_squared_error\", following the familiar scikit-learn nomenclature, or as a dict of str: Callable. For convenience, it can also be a single string.\nHowever, in a departure from scikit-learn, metrics will fail if a Callable is passed directly. This restriction is in place to facilitate naming columns in the PoniardBaseEstimator.get_results method.\n\n\n\n\n\n\nscoring vs. metrics parameters\n\n\n\nIn scikit-learn parlance, a metric is a measure of the prediction error of a model. Scoring, which is used in sklearn model evaluation objects (like GridSearchCV or cross_validate), reflects the same, but with the restriction that higher values are better than lower values. Poniard uses the parameter name metrics for now, but will eventually migrate to scoring as that reflects more accurately its meaning.\n\n\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom poniard import PoniardRegressor\n\n\nX, y = make_regression(n_samples=500, n_features=10, n_informative=5)\npnd = PoniardRegressor(\n    metrics=[\"explained_variance\", \"neg_median_absolute_error\"],\n    estimators=[LinearRegression()],\n)\npnd.setup(X, y, show_info=False)\npnd.fit()\npnd.get_results(return_train_scores=True)\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_explained_variance\n      train_explained_variance\n      test_neg_median_absolute_error\n      train_neg_median_absolute_error\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      1.0\n      1.000000e+00\n      -8.366641e-14\n      -8.038015e-14\n      0.001400\n      0.000375\n    \n    \n      DummyRegressor\n      0.0\n      4.440892e-17\n      -6.716617e+01\n      -6.567442e+01\n      0.000645\n      0.000255\n    \n  \n\n\n\n\n\nfrom sklearn.metrics import r2_score, make_scorer\nfrom sklearn.linear_model import Ridge\n\n\ndef scaled_r2(y_true, y_pred):\n    return round(r2_score(y_true, y_pred) * 100, 1)\n\n\npnd = PoniardRegressor(\n    metrics={\n        \"scaled_r2\": make_scorer(scaled_r2, greater_is_better=True),\n        \"usual_r2\": make_scorer(r2_score, greater_is_better=True),\n    },\n    estimators=[LinearRegression(), Ridge()],\n)\npnd.setup(X, y, show_info=False).fit().get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_scaled_r2\n      test_usual_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      100.00\n      1.000000\n      0.001491\n      0.000342\n    \n    \n      Ridge\n      100.00\n      0.999994\n      0.001253\n      0.000301\n    \n    \n      DummyRegressor\n      -0.88\n      -0.008754\n      0.000607\n      0.000254\n    \n  \n\n\n\n\nThe order in which scorers are passed matters; to be precise, the first scorer will be used in some methods if no other metric is defined.\n\nprint(pnd.metrics)\nfig = pnd.plot.permutation_importance(\"Ridge\")\nfig.show(\"notebook\")\n\n{'scaled_r2': make_scorer(scaled_r2), 'usual_r2': make_scorer(r2_score)}"
  },
  {
    "objectID": "guide/main_parameters.html#cv",
    "href": "guide/main_parameters.html#cv",
    "title": "Main parameters",
    "section": "cv",
    "text": "cv\nCross validation can be anything that scikit-learn accepts. By default, classification tasks will be paired with a StratifiedKFold if the target is binary, and KFold otherwise. Regression tasks use KFold by default.\ncv=int or cv=None are internally converted to one of the above classes so that Poniard’s random_state parameter can be passed on.\n\nfrom sklearn.model_selection import RepeatedKFold\n\n\npnd_5 = PoniardRegressor(cv=4).setup(X, y, show_info=False)\npnd_none = PoniardRegressor(cv=None).setup(X, y, show_info=False)\npnd_k = PoniardRegressor(cv=RepeatedKFold(n_splits=3)).setup(X, y, show_info=False)\n\nprint(pnd_5.cv, pnd_none.cv, pnd_k.cv, sep=\"\\n\")\n\nKFold(n_splits=4, random_state=0, shuffle=True)\nKFold(n_splits=5, random_state=0, shuffle=True)\nRepeatedKFold(n_repeats=10, n_splits=3, random_state=0)\n\n\nNote that even though we didn’t specify random_state for the third estimator, it gets injected during setup."
  },
  {
    "objectID": "plugins.core.html",
    "href": "plugins.core.html",
    "title": "Base plugin",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "plugins.core.html#developing-plugins",
    "href": "plugins.core.html#developing-plugins",
    "title": "Base plugin",
    "section": "Developing plugins",
    "text": "Developing plugins\nPlugins allow devs to extend Poniard funcionality beyond what the main module offers. Doing so is straightforward: subclass BasePlugin and implement the desired methods.\nCrucially, Poniard estimators inject themselves to all plugins during initialization, meaning that plugin instances have access to the estimator on the attribute _poniard.\nThe following minimal example builds a plugin that adds a new (useless) feature and modifies the preprocessor.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom poniard import PoniardClassifier\n\n\nclass StringFeaturePlugin(BasePlugin):\n    \"\"\"A plugin that adds a feature comprised of a single string.\n\n    Parameters\n    ----------\n    string :\n        The string to add as a feature.\n    \"\"\"\n\n    def __init__(self, string: str):\n        super().__init__()\n        self.string = string\n\n    def on_setup_data(self):\n        data = self._poniard.X\n        if hasattr(data, \"iloc\"):\n            self._poniard.X = data.assign(**{self.string: self.string})\n        else:\n            self._poniard.X = np.append(data, self.string, axis=1)\n        return\n\n    def on_setup_preprocessor(self):\n        old_preprocessor = self._poniard.preprocessor\n        if isinstance(old_preprocessor[-1], VarianceThreshold):\n            self._poniard.preprocessor = old_preprocessor[:-1]\n            self._poniard.pipelines = self._poniard._build_pipelines()\n        return\n\n\nfeatures = pd.DataFrame(\n    np.random.normal(size=(20, 2)), columns=[f\"X_{i}\" for i in range(2)]\n)\ntarget = np.random.choice([0, 1], size=20)\npnd = PoniardClassifier(plugins=StringFeaturePlugin(\"foobar\")).setup(features, target)\npnd.preprocessor\n\nTarget info\n-----------\nType: binary\nShape: (20,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 2\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      X_0\n      \n      foobar\n      \n    \n    \n      1\n      X_1\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['X_0', 'X_1']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['foobar'])]))],\n         verbose=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['X_0', 'X_1']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['foobar'])]))],\n         verbose=0)type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['X_0', 'X_1']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['foobar'])])numeric_preprocessor['X_0', 'X_1']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['foobar']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)"
  },
  {
    "objectID": "plot.plot_factory.html",
    "href": "plot.plot_factory.html",
    "title": "Plot factory",
    "section": "",
    "text": "PoniardPlotFactory is a series of plots meant to enhance the PoniardBaseEstimator.get_results experience. Like the rest of Poniard, wherever possible it will try to use cross validation (for example, PoniardPlotFactory.confusion_matrix uses predictions from scikit-learn’s cross_val_predict).\nsource"
  },
  {
    "objectID": "plot.plot_factory.html#general-plots",
    "href": "plot.plot_factory.html#general-plots",
    "title": "Plot factory",
    "section": "General plots",
    "text": "General plots\n\nsource\n\nPoniardPlotFactory.metrics\n\n PoniardPlotFactory.metrics (kind:str='strip', facet:str='col',\n                             metrics:Union[str,Sequence[str]]=None,\n                             only_test:bool=True, exclude_dummy:bool=True,\n                             show_means:bool=True, **kwargs)\n\nPlot metrics obtained by running PoniardBaseEstimator.fit.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nkind\nstr\nstrip\nEither “strip” or “bar”. Default “strip”.\n\n\nfacet\nstr\ncol\nEither “col” or “row”. Default “col”.\n\n\nmetrics\ntyping.Union[str, typing.Sequence[str]]\nNone\nString or list of strings. This must follow the names passed to thePoniard constructor. For example, if during init a dict of metrics was passed, itskeys can be passed here. Default None, which plots every estimator metric available.\n\n\nonly_test\nbool\nTrue\nWhether to plot only test scores. Default True.\n\n\nexclude_dummy\nbool\nTrue\nWhether to exclude dummy estimators. Default True.\n\n\nshow_means\nbool\nTrue\nWhether to plot means along with fold scores. Default True.\n\n\nkwargs\n\n\n\n\n\nReturns\nFigure\n\nPlotly strip or bar plot.\n\n\n\n\nfrom sklearn.datasets import load_iris\nfrom poniard import PoniardClassifier\n\n\nX, y = load_iris(return_X_y=True, as_frame=True)\npnd = PoniardClassifier().setup(X, y, show_info=False).fit()\n\n\n\n\n\npnd.plot.metrics(metrics=\"f1_macro\")\n\n\n                                                \n\n\n\npnd.plot.metrics(metrics=[\"roc_auc\", \"accuracy\"], facet=\"col\", kind=\"bar\")\n\n\n                                                \n\n\n\nsource\n\n\nPoniardPlotFactory.overfitness\n\n PoniardPlotFactory.overfitness (metric:Optional[str]=None,\n                                 exclude_dummy:bool=True)\n\nPlot the ratio of test scores to train scores for every estimator.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmetric\ntyping.Optional[str]\nNone\nString representing a metric. This must follow the names passed to thePoniard constructor. For example, if during init a dict of metrics was passed, one ofits keys can be passed here. Default None, which plots the first metric.\n\n\nexclude_dummy\nbool\nTrue\nWhether to exclude dummy estimators. Default True.\n\n\nReturns\nFigure\n\nPlotly strip plot."
  },
  {
    "objectID": "plot.plot_factory.html#classification-plots",
    "href": "plot.plot_factory.html#classification-plots",
    "title": "Plot factory",
    "section": "Classification plots",
    "text": "Classification plots\nPoniard offers additional plots based on cross validated predictions:\n\nsource\n\nPoniardPlotFactory.roc_curve\n\n PoniardPlotFactory.roc_curve\n                               (estimator_names:Optional[Sequence[str]]=No\n                               ne, response_method:str='auto', **kwargs)\n\nPlot ROC curve with cross validated predictions for multiple estimators.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_names\ntyping.Optional[typing.Sequence[str]]\nNone\nEstimators to include. If None, all estimators are used.\n\n\nresponse_method\nstr\nauto\nEither “auto”, “predict_proba” or “decision_function”. “auto” will try to usepredict_proba if all estimators have it, otherwise it will try decision_functionIf there is no common response_method, it will raise an error.\n\n\nkwargs\n\n\nPassed to sklearn.metrics.roc_curve().\n\n\nReturns\nFigure\n\nPlotly line plot.\n\n\n\nFor now, PoniardPlotFactory.roc_curve only works with binary classification tasks (you can track this issue here, so we’ll need a different dataset, like the well known German credit dataset.\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import LabelEncoder\nfrom poniard import PoniardClassifier\n\n\nX, y = fetch_openml(data_id=31, return_X_y=True, as_frame=True)\ncategorical_cols = X.select_dtypes(include=\"category\").columns\nX[categorical_cols] = X[categorical_cols].astype(str)\ny = LabelEncoder().fit_transform(y)\npnd = PoniardClassifier().setup(X, y, show_info=False).fit()\n\nestimators = [\"LogisticRegression\", \"XGBClassifier\", \"SVC\"]\npnd.plot.roc_curve(estimator_names=estimators)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\nsource\n\n\nPoniardPlotFactory.confusion_matrix\n\n PoniardPlotFactory.confusion_matrix (estimator_name:str, **kwargs)\n\nPlot confusion matrix with cross validated predictions for a single estimator.\n\n\n\n\nType\nDetails\n\n\n\n\nestimator_name\nstr\nEstimator to include.\n\n\nkwargs\n\nPassed to sklearn.metrics.confusion_matrix().\n\n\nReturns\nFigure\nPlotly image plot.\n\n\n\nLike roc_curve, confusion_matrix accepts kwargs that are passed on to the appropiate sklearn functions.\n\npnd.plot.confusion_matrix(estimator_name=\"LogisticRegression\", normalize=\"all\")"
  },
  {
    "objectID": "plot.plot_factory.html#regression-plots",
    "href": "plot.plot_factory.html#regression-plots",
    "title": "Plot factory",
    "section": "Regression plots",
    "text": "Regression plots\n\nsource\n\nPoniardPlotFactory.residuals\n\n PoniardPlotFactory.residuals (estimator_names:List[str])\n\nPlot regression residuals vs predictions for a list of estimators.\n\n\n\n\nType\nDetails\n\n\n\n\nestimator_names\ntyping.List[str]\nEstimators to include.\n\n\nReturns\nFigure\nResiduals plot.\n\n\n\n\nsource\n\n\nPoniardPlotFactory.residuals_histogram\n\n PoniardPlotFactory.residuals_histogram (estimator_names:List[str])\n\nPlot a histogram of regression residuals for a list of estimators.\n\n\n\n\nType\nDetails\n\n\n\n\nestimator_names\ntyping.List[str]\nEstimators to include.\n\n\nReturns\nFigure\nResiduals histogram plot.\n\n\n\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\n\nfrom poniard import PoniardRegressor\n\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\npnd = PoniardRegressor(estimators=[LinearRegression(), XGBRegressor()]).setup(\n    X, y, show_info=False\n)\npnd.fit()\n\n\n\n\nPoniardRegressor(estimators=[LinearRegression(), XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n             colsample_bynode=None, colsample_bytree=None,\n             enable_categorical=False, gamma=None, gpu_id=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_delta_step=None, max_depth=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=0, reg_alpha=None, reg_lambda=None,\n             scale_pos_weight=None, subsample=None, tree_method=None,\n             validate_parameters=None, verbosity=0)])\n\n\n\npnd.plot.residuals(estimator_names=[\"LinearRegression\", \"XGBRegressor\"])\n\n\n\n\n\n\n\n\n                                                \n\n\n\npnd.plot.residuals_histogram(estimator_names=[\"LinearRegression\", \"XGBRegressor\"])"
  },
  {
    "objectID": "plot.plot_factory.html#feature-plots",
    "href": "plot.plot_factory.html#feature-plots",
    "title": "Plot factory",
    "section": "Feature plots",
    "text": "Feature plots\nThese plots help in understanding how features interact with models.\n\nsource\n\nPoniardPlotFactory.permutation_importance\n\n PoniardPlotFactory.permutation_importance (estimator_name:str,\n                                            n_repeats:int=10,\n                                            kind:str='bar', **kwargs)\n\nPlot permutation importances for an estimator.\nThis shuffles features randomly one at a time and measures the change in the estimator’s performance. If the feature is important for the model, the estimator’s performance should decrease (represented by positive values in the plot). See the scikit-learn guide.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_name\nstr\n\nEstimator to include.\n\n\nn_repeats\nint\n10\nHow many times to repeat random permutations of a single feature. Default 10.\n\n\nkind\nstr\nbar\nEither “bar” or “strip”. Default “bar”. “strip” plots each permutation repetitionas well as the mean. Bar plots only the mean.\n\n\nkwargs\n\n\nPassed to sklearn.inspection.permutation_importance().\n\n\nReturns\nFigure\n\nPlotly bar or strip plot.\n\n\n\n\npnd.plot.permutation_importance(estimator_name=\"XGBRegressor\")\n\n\n                                                \n\n\n\nsource\n\n\nPoniardPlotFactory.partial_dependence\n\n PoniardPlotFactory.partial_dependence (estimator_name:str,\n                                        feature:Union[str,int], **kwargs)\n\nPlot partial dependence for a single feature of a single estimator.\nIn essence, visualize how the target changes within the feature’s range.\nOnly plots average partial dependence for all samples and not individual samples (ICE).\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nestimator_name\nstr\nEstimator to include.\n\n\nfeature\ntyping.Union[str, int]\nFeature for which to plot partial dependence. Can be a pandas column name or index.\n\n\nkwargs\n\nPassed to sklearn.inspection.partial_dependence().\n\n\nReturns\nFigure\nPlotly line plot.\n\n\n\n\npnd.plot.partial_dependence(\"XGBRegressor\", \"AveOccup\")"
  },
  {
    "objectID": "preprocessing.datetime.html",
    "href": "preprocessing.datetime.html",
    "title": "Datetime preprocessors",
    "section": "",
    "text": "class DateLevel(Enum):\n    \"\"\"An enum representing different date levels.\"\"\"\n\n    YEAR = \"year\"\n    QUARTER = \"quarter\"\n    MONTH = \"month\"\n    DAY = \"day\"\n    HOUR = \"hour\"\n    MINUTE = \"minute\"\n    SECOND = \"second\"\n    MICROSECOND = \"microsecond\"\n    NANOSECOND = \"nanosecond\"\n    WEEKDAY = \"weekday\"\n    DAYOFYEAR = \"dayofyear\"\n    DAYSINMONTH = \"daysinmonth\"\n\n\nsource\n\nDateLevel\n\n DateLevel (value, names=None, module=None, qualname=None, type=None,\n            start=1)\n\nAn enum representing different date levels.\n\nsource\n\n\nDatetimeEncoder\n\n DatetimeEncoder (levels:Optional[Sequence[DateLevel]]=None,\n                  fmt:Optional[str]=None)\n\nAn encoder for datetime columns that outputs integer features\nlevels is a list of DateLevel that define which date features to extract, i.e, [DateLevel.HOUR, DateLevel.MINUTE] will extract hours and minutes. If left to the default None, all available features will be extracted initially, but zero variance features will be dropped (for example, because the dates don’t have seconds).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevels\nOptional[SequenceDateLevel]\nNone\nDate features to extract.\n\n\nfmt\nOptional[str]\nNone\nDate format for string conversion if inputs are note datetime-like objects.Follows standard Pandas/stdlib formatting, or example, ‘%Y-%m-%d %H:%M:%S’.\n\n\n\n\nsource\n\n\nDatetimeEncoder.fit\n\n DatetimeEncoder.fit\n                      (X:Union[pandas.core.frame.DataFrame,numpy.ndarray,L\n                      ist], y=None)\n\nFit the DatetimeEncoder.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\n\nDatetime-like features..\n\n\ny\nNoneType\nNone\nUnused.\n\n\nReturns\nDatetimeEncoder\n\nFitted DatetimeEncoder.\n\n\n\nAfter fitting, the categories of each feature are held in the categories_ attribute.\n\nsource\n\n\nDatetimeEncoder.transform\n\n DatetimeEncoder.transform\n                            (X:Union[pandas.core.frame.DataFrame,numpy.nda\n                            rray,List])\n\nApply transformation. Will ignore zero variance features seen during DatetimeEncoder.fit.\nWhile this transformer is generally stateless, during DatetimeEncoder.fit it checks whether any of the extracted features have zero variance (only one unique value) and sets those levels to be ignored during DatetimeEncoder.transform.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nThe data to encode.\n\n\nReturns\nnp.ndarray\nTransformed input.\n\n\n\n\n\n\nTransformerMixin.fit_transform\n\n TransformerMixin.fit_transform (X, y=None, **fit_params)\n\nFit to data, then transform it.\nFits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\narray-like of shape (n_samples, n_features)\n\nInput samples.\n\n\ny\nNoneType\nNone\nTarget values (None for unsupervised transformations).\n\n\nfit_params\n\n\n\n\n\nReturns\nndarray array of shape (n_samples, n_features_new)\n\nTransformed array.\n\n\n\n\nimport pandas as pd\n\n\nX = pd.DataFrame(\n    {\n        \"hours\": pd.date_range(start=\"2022-01-01\", freq=\"H\", periods=25),\n        \"days\": pd.date_range(start=\"2022-01-01\", freq=\"D\", periods=25),\n    }\n)\n\nencoder = DatetimeEncoder()\npd.DataFrame(encoder.fit_transform(X), columns=encoder.get_feature_names_out()).head()\n\n\n\n\n\n  \n    \n      \n      hours_day\n      hours_hour\n      hours_weekday\n      hours_dayofyear\n      days_day\n      days_weekday\n      days_dayofyear\n    \n  \n  \n    \n      0\n      1\n      0\n      5\n      1\n      1\n      5\n      1\n    \n    \n      1\n      1\n      1\n      5\n      1\n      2\n      6\n      2\n    \n    \n      2\n      1\n      2\n      5\n      1\n      3\n      0\n      3\n    \n    \n      3\n      1\n      3\n      5\n      1\n      4\n      1\n      4\n    \n    \n      4\n      1\n      4\n      5\n      1\n      5\n      2\n      5\n    \n  \n\n\n\n\nDates can be strings as well, but datetimes and strings cannot be combined.\n\ndate_format = \"%Y-%m-%d\"\nX = pd.DataFrame(\n    {\n        \"days\": pd.date_range(start=\"2022-01-01\", freq=\"D\", periods=25).strftime(\n            date_format\n        ),\n        \"quarters\": pd.date_range(start=\"2023-01-01\", freq=\"Q\", periods=25).strftime(\n            date_format\n        ),\n    }\n)\n\nencoder = DatetimeEncoder(fmt=date_format)\npd.DataFrame(encoder.fit_transform(X), columns=encoder.get_feature_names_out()).head()\n\n\n\n\n\n  \n    \n      \n      days_day\n      days_weekday\n      days_dayofyear\n      quarters_year\n      quarters_quarter\n      quarters_month\n      quarters_day\n      quarters_weekday\n      quarters_dayofyear\n      quarters_daysinmonth\n    \n  \n  \n    \n      0\n      1\n      5\n      1\n      2023\n      1\n      3\n      31\n      4\n      90\n      31\n    \n    \n      1\n      2\n      6\n      2\n      2023\n      2\n      6\n      30\n      4\n      181\n      30\n    \n    \n      2\n      3\n      0\n      3\n      2023\n      3\n      9\n      30\n      5\n      273\n      30\n    \n    \n      3\n      4\n      1\n      4\n      2023\n      4\n      12\n      31\n      6\n      365\n      31\n    \n    \n      4\n      5\n      2\n      5\n      2024\n      1\n      3\n      31\n      6\n      91\n      31\n    \n  \n\n\n\n\nDate levels may be chosen.\n\nencoder = DatetimeEncoder(\n    levels=[DateLevel.DAY, DateLevel.HOUR, DateLevel.MONTH], fmt=date_format\n)\npd.DataFrame(encoder.fit_transform(X), columns=encoder.get_feature_names_out()).head()\n\n\n\n\n\n  \n    \n      \n      days_day\n      quarters_day\n      quarters_month\n    \n  \n  \n    \n      0\n      1\n      31\n      3\n    \n    \n      1\n      2\n      30\n      6\n    \n    \n      2\n      3\n      30\n      9\n    \n    \n      3\n      4\n      31\n      12\n    \n    \n      4\n      5\n      31\n      3"
  },
  {
    "objectID": "estimators.regression.html",
    "href": "estimators.regression.html",
    "title": "Regressor",
    "section": "",
    "text": "source\n\nPoniardRegressor\n\n PoniardRegressor (estimators:Union[Dict[str,sklearn.base.RegressorMixin],\n                   Sequence[sklearn.base.RegressorMixin],NoneType]=None, m\n                   etrics:Union[str,Dict[str,Callable],Sequence[str],NoneT\n                   ype]=None, preprocess:bool=True, custom_preprocessor:Un\n                   ion[NoneType,sklearn.pipeline.Pipeline,sklearn.base.Tra\n                   nsformerMixin]=None, cv:Union[int,sklearn.model_selecti\n                   on._split.BaseCrossValidator,sklearn.model_selection._s\n                   plit.BaseShuffleSplit,Sequence]=None, verbose:int=0,\n                   random_state:Optional[int]=None,\n                   n_jobs:Optional[int]=None,\n                   plugins:Optional[Sequence[Any]]=None, plot_options:Opti\n                   onal[poniard.plot.plot_factory.PoniardPlotFactory]=None\n                   )\n\nCross validate multiple regressors, rank them, fine tune them and ensemble them.\nPoniardRegressor takes a list/dict of scikit-learn estimators and compares their performance on a list/dict of scikit-learn metrics using a predefined scikit-learn cross-validation strategy.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimators\ntyping.Union[typing.Dict[str, sklearn.base.RegressorMixin], typing.Sequence[sklearn.base.RegressorMixin], NoneType]\nNone\nEstimators to evaluate.\n\n\nmetrics\ntyping.Union[str, typing.Dict[str, typing.Callable], typing.Sequence[str], NoneType]\nNone\nMetrics to compute for each estimator. This is more restrictive than sklearn’s scoringparameter, as it does not allow callable scorers. Single strings are cast to listsautomatically.\n\n\npreprocess\nbool\nTrue\nIf True, impute missing values, standard scale numeric data and one-hot or ordinalencode categorical data.\n\n\ncustom_preprocessor\ntyping.Union[NoneType, sklearn.pipeline.Pipeline, sklearn.base.TransformerMixin]\nNone\nPreprocessor used instead of the default preprocessing pipeline. It must be able to beincluded directly in a scikit-learn Pipeline.\n\n\ncv\ntyping.Union[int, sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, typing.Sequence]\nNone\nCross validation strategy. Either an integer, a scikit-learn cross validation object,or an iterable.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\ntyping.Optional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\ntyping.Optional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction and estimator.\n\n\nplugins\ntyping.Optional[typing.Sequence[typing.Any]]\nNone\nPlugin instances that run in set moments of setup, fit and plotting.\n\n\nplot_options\ntyping.Optional[poniard.plot.plot_factory.PoniardPlotFactory]\nNone\n:class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly formatoptions or None, which sets the default factory.\n\n\n\nPoniardRegressor implements PoniardClassifier._build_cv, PoniardClassifier._build_metrics and PoniardClassifier._default_estimators.\n\nsource\n\n\nPoniardRegressor._build_cv\n\n PoniardRegressor._build_cv ()\n\n\nsource\n\n\nPoniardRegressor._build_metrics\n\n PoniardRegressor._build_metrics ()\n\nBuild metrics.\n\nsource\n\n\nPoniardRegressor._default_estimators\n\n PoniardRegressor._default_estimators ()"
  },
  {
    "objectID": "estimators.classification.html",
    "href": "estimators.classification.html",
    "title": "Classifier",
    "section": "",
    "text": "source\n\nPoniardClassifier\n\n PoniardClassifier (estimators:Union[Dict[str,sklearn.base.ClassifierMixin\n                    ],Sequence[sklearn.base.ClassifierMixin],NoneType]=Non\n                    e, metrics:Union[str,Dict[str,Callable],Sequence[str],\n                    NoneType]=None, preprocess:bool=True, custom_preproces\n                    sor:Union[NoneType,sklearn.pipeline.Pipeline,sklearn.b\n                    ase.TransformerMixin]=None, cv:Union[int,sklearn.model\n                    _selection._split.BaseCrossValidator,sklearn.model_sel\n                    ection._split.BaseShuffleSplit,Sequence]=None,\n                    verbose:int=0, random_state:Optional[int]=None,\n                    n_jobs:Optional[int]=None,\n                    plugins:Optional[Sequence[Any]]=None, plot_options:Opt\n                    ional[poniard.plot.plot_factory.PoniardPlotFactory]=No\n                    ne)\n\nCross validate multiple classifiers, rank them, fine tune them and ensemble them.\nPoniardClassifier takes a list/dict of scikit-learn estimators and compares their performance on a list/dict of scikit-learn metrics using a predefined scikit-learn cross-validation strategy.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimators\ntyping.Union[typing.Dict[str, sklearn.base.ClassifierMixin], typing.Sequence[sklearn.base.ClassifierMixin], NoneType]\nNone\nEstimators to evaluate.\n\n\nmetrics\ntyping.Union[str, typing.Dict[str, typing.Callable], typing.Sequence[str], NoneType]\nNone\nMetrics to compute for each estimator. This is more restrictive than sklearn’s scoringparameter, as it does not allow callable scorers. Single strings are cast to listsautomatically.\n\n\npreprocess\nbool\nTrue\nIf True, impute missing values, standard scale numeric data and one-hot or ordinalencode categorical data.\n\n\ncustom_preprocessor\ntyping.Union[NoneType, sklearn.pipeline.Pipeline, sklearn.base.TransformerMixin]\nNone\nPreprocessor used instead of the default preprocessing pipeline. It must be able to beincluded directly in a scikit-learn Pipeline.\n\n\ncv\ntyping.Union[int, sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, typing.Sequence]\nNone\nCross validation strategy. Either an integer, a scikit-learn cross validation object,or an iterable.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\ntyping.Optional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\ntyping.Optional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction.\n\n\nplugins\ntyping.Optional[typing.Sequence[typing.Any]]\nNone\nPlugin instances that run in set moments of setup, fit and plotting.\n\n\nplot_options\ntyping.Optional[poniard.plot.plot_factory.PoniardPlotFactory]\nNone\n:class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly formatoptions or None, which sets the default factory.\n\n\n\nPoniardClassifier implements PoniardClassifier._build_cv, PoniardClassifier._build_metrics and PoniardClassifier._default_estimators.\n\nsource\n\n\nPoniardClassifier._build_cv\n\n PoniardClassifier._build_cv ()\n\n\nsource\n\n\nPoniardClassifier._build_metrics\n\n PoniardClassifier._build_metrics ()\n\nBuild metrics.\n\nsource\n\n\nPoniardClassifier._default_estimators\n\n PoniardClassifier._default_estimators ()"
  },
  {
    "objectID": "estimators.core.html",
    "href": "estimators.core.html",
    "title": "Base estimator",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "estimators.core.html#main-methods",
    "href": "estimators.core.html#main-methods",
    "title": "Base estimator",
    "section": "Main methods",
    "text": "Main methods\n\nsource\n\nPoniardBaseEstimator.setup\n\n PoniardBaseEstimator.setup\n                             (X:Union[pandas.core.frame.DataFrame,numpy.nd\n                             array,List], y:Union[pandas.core.frame.DataFr\n                             ame,numpy.ndarray,List], show_info:bool=True)\n\nActs as an orchestrator for Poniard estimators by setting up everything neeeded for PoniardBaseEstimator.fit.\nConverts inputs to arrays if necessary, sets metrics, preprocessor, cv and pipelines.\nAfter running PoniardBaseEstimator.setup, both X and y will be held as attributes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\n\nFeatures\n\n\ny\nUnion[pd.DataFrame, np.ndarray, List]\n\nTarget.\n\n\nshow_info\nbool\nTrue\nWhether to print information about the target, metrics and type inference.\n\n\nReturns\nPoniardBaseEstimator\n\n\n\n\n\nPoniardBaseEstimator.setup takes features and target as parameters, while PoniardBaseEstimator.fit does not accept any. This runs contrary to the established convention defined by scikit-learn where there is no setting up to do and fit takes the data as params.\nThis is because Poniard does not only fit the models, but also infer features types and create the preprocessor based on these types. While this could all be stuffed inside PoniardBaseEstimator.fit (that was the case initially), having it separated allows the user to check whether Poniard’s assumptions are correct and adjust if needed before running fit, which can take long depending on how many models were passed to estimators, the cross validation strategy and the size of the dataset.\nPoniardBaseEstimator by default includes a PoniardPreprocessor that handles building the preprocessor that will go into final estimation pipelines. However, a PoniardPreprocessor with custom parameters can be used as a custom_preprocessor.\n\nAn example\nLet’s load some random data and setup a PoniardClassifier, which inherits from PoniardBaseEstimator.\n\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame(\n    {\n        \"type\": random.choices([\"house\", \"apartment\"], k=500),\n        \"age\": rng.uniform(1, 200, 500).astype(int),\n        \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n        \"rating\": random.choices(range(50), k=500),\n        \"target\": random.choices([0, 1], k=500),\n    }\n)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      type\n      age\n      date\n      rating\n      target\n    \n  \n  \n    \n      0\n      apartment\n      127\n      2022-01-31\n      1\n      1\n    \n    \n      1\n      apartment\n      54\n      2022-02-28\n      17\n      1\n    \n    \n      2\n      house\n      9\n      2022-03-31\n      0\n      1\n    \n    \n      3\n      house\n      4\n      2022-04-30\n      48\n      1\n    \n    \n      4\n      apartment\n      162\n      2022-05-31\n      40\n      0\n    \n  \n\n\n\n\nInformation about the data will be shown so it can be reviewed and changes can be made.\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier()\npnd.setup(X, y)\n\n\n                         Setup info\n                         Target\n                             Type: binary\n                             Shape: (500,)\n                             Unique values: 2\n                             Metrics\n                             Main metric: roc_auc\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 50\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\nPoniardClassifier()\n\n\nAfter passing data to Poniard estimators through setup, multiple attributes become available.\nfeature_types is a dict that sorts features in 4 categories (numeric, categorical_high, categorical_low and datetime) using some basic heuristics. This attribute is computed in PoniardPreprocessor.build, and will not be available if a non-PoniardPreprocessor transformer is passed to custom_preprocessor.\nFeature types depend on the feature dtypes, and numeric_threshold and cardinality_threshold which are used in PoniardPreprocessor’s construction.\n\npnd.feature_types\n\n{'numeric': ['age'],\n 'categorical_high': ['rating'],\n 'categorical_low': ['type'],\n 'datetime': ['date']}\n\n\nThe preprocessor can be the transformer produced by a PoniardPreprocessor, which in turn depends on feature_types, and the scaler, numeric_imputer and high_cardinality_encoder parameters, or a user-supplied scikit-learn compatible transformer.\nAs will be seen further on, the PoniardPreprocessor can be modified significantly to fit multiple use cases and datasets.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nEach estimator has a set of default metrics, but others can be passed during construction.\n\npnd.metrics\n\n['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n\n\nLikewise, cv has sane defaults but can be modified accordingly.\n\npnd.cv\n\nStratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\n\ntarget_info lists information about y.\n\npnd.target_info\n\n{'type_': 'binary', 'ndim': 1, 'shape': (500,), 'nunique': 2}\n\n\npipelines is a dict containing each pipeline which will be trained during fit. Each Poniard estimator has a limited set of default estimators that are used if none are specified during initialization.\n\npnd.pipelines[\"SVC\"]\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())],\n                          verbose=0)),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())],\n                          verbose=0)),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()SVCSVC(kernel='linear', probability=True, random_state=0, verbose=0)\n\n\n\nsource\n\n\n\nPoniardBaseEstimator.fit\n\n PoniardBaseEstimator.fit ()\n\nThis is the main Poniard method. It uses scikit-learn’s cross_validate function to score all metrics for every pipelines, using cv for cross validation.\n\npnd.fit()\n\n\n\n\nPoniardClassifier()\n\n\nBecause features and target are passed to the Poniard estimator, fit does not take any parameters.\nAfter fitting pipelines, cross validated results can be accessed by running get_results\n\nsource\n\n\nPoniardBaseEstimator.get_results\n\n PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n                                   std:bool=False, wrt_dummy:bool=False)\n\nReturn dataframe containing scoring results. By default returns the mean score and fit and score times. Optionally returns standard deviations as well.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_train_scores\nbool\nFalse\nIf False, only return test scores.\n\n\nstd\nbool\nFalse\nWhether to return standard deviation of the scores. Default False.\n\n\nwrt_dummy\nbool\nFalse\nWhether to compute each score/time with respect to the dummy estimator results. DefaultFalse.\n\n\nReturns\nUnion[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]\n\nResults\n\n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.510256\n      0.510\n      0.531145\n      0.503846\n      0.516707\n      0.010714\n      0.007243\n    \n    \n      DummyClassifier\n      0.500000\n      0.520\n      0.520000\n      1.000000\n      0.684211\n      0.009618\n      0.007332\n    \n    \n      KNeighborsClassifier\n      0.496675\n      0.492\n      0.509150\n      0.534615\n      0.519465\n      0.009883\n      0.008536\n    \n    \n      SVC\n      0.472356\n      0.476\n      0.499007\n      0.688462\n      0.575907\n      0.715862\n      0.008426\n    \n    \n      LogisticRegression\n      0.468990\n      0.488\n      0.509234\n      0.573077\n      0.536862\n      0.019850\n      0.007661\n    \n    \n      XGBClassifier\n      0.460417\n      0.486\n      0.502401\n      0.500000\n      0.499330\n      0.046362\n      0.009421\n    \n    \n      HistGradientBoostingClassifier\n      0.456571\n      0.488\n      0.505975\n      0.484615\n      0.494283\n      0.405131\n      0.019346\n    \n    \n      RandomForestClassifier\n      0.435056\n      0.462\n      0.479861\n      0.476923\n      0.477449\n      0.070931\n      0.014314\n    \n    \n      GaussianNB\n      0.423317\n      0.468\n      0.492473\n      0.565385\n      0.525371\n      0.010134\n      0.007401\n    \n  \n\n\n\n\n\nmeans, stds = pnd.get_results(std=True, return_train_scores=True)\nstds\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      train_roc_auc\n      test_accuracy\n      train_accuracy\n      test_precision\n      train_precision\n      test_recall\n      train_recall\n      test_f1\n      train_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.060706\n      0.000000e+00\n      0.060332\n      0.000000\n      0.059942\n      0.000000\n      0.058835\n      0.000000\n      0.057785\n      0.000000\n      0.000303\n      0.000047\n    \n    \n      DummyClassifier\n      0.000000\n      0.000000e+00\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000404\n      0.000100\n    \n    \n      KNeighborsClassifier\n      0.021105\n      8.429609e-03\n      0.019391\n      0.010840\n      0.019140\n      0.008157\n      0.081043\n      0.022053\n      0.049760\n      0.012869\n      0.000341\n      0.000070\n    \n    \n      SVC\n      0.038609\n      3.600720e-02\n      0.042708\n      0.032496\n      0.031965\n      0.028405\n      0.085485\n      0.073140\n      0.036968\n      0.026864\n      0.110736\n      0.000229\n    \n    \n      LogisticRegression\n      0.068079\n      2.545484e-02\n      0.041183\n      0.027946\n      0.037992\n      0.024759\n      0.065948\n      0.021371\n      0.036585\n      0.022583\n      0.004623\n      0.000269\n    \n    \n      XGBClassifier\n      0.065278\n      0.000000e+00\n      0.035553\n      0.000000\n      0.033315\n      0.000000\n      0.091826\n      0.000000\n      0.061108\n      0.000000\n      0.001688\n      0.000196\n    \n    \n      HistGradientBoostingClassifier\n      0.059681\n      7.749323e-04\n      0.041183\n      0.007483\n      0.039938\n      0.011912\n      0.070291\n      0.005607\n      0.054859\n      0.007046\n      0.049279\n      0.005965\n    \n    \n      RandomForestClassifier\n      0.060809\n      7.021667e-17\n      0.039192\n      0.000000\n      0.038392\n      0.000000\n      0.077307\n      0.000000\n      0.056132\n      0.000000\n      0.000342\n      0.000267\n    \n    \n      GaussianNB\n      0.045845\n      2.494438e-02\n      0.042143\n      0.018303\n      0.037330\n      0.015830\n      0.031246\n      0.038051\n      0.025456\n      0.018727\n      0.000729\n      0.000126\n    \n  \n\n\n\n\nget_estimator is a convenience method that gets a pipeline from pipelines by name, and optionally trains it on X and y.\n\nsource\n\n\nPoniardBaseEstimator.get_estimator\n\n PoniardBaseEstimator.get_estimator (estimator_name:str,\n                                     include_preprocessor:bool=True,\n                                     retrain:bool=False)\n\nObtain an estimator in pipelines by name. This is useful for extracting default estimators or hyperparmeter-optimized estimators (after using PoniardBaseEstimator.tune_estimator).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_name\nstr\n\nEstimator name.\n\n\ninclude_preprocessor\nbool\nTrue\nWhether to return a pipeline with a preprocessor or just the estimator. Default True.\n\n\nretrain\nbool\nFalse\nWhether to retrain with full data. Default False.\n\n\nReturns\nUnion[Pipeline, ClassifierMixin, RegressorMixin]\n\nEstimator.\n\n\n\n\nsource\n\n\nPoniardBaseEstimator.analyze_estimator\n\n PoniardBaseEstimator.analyze_estimator (estimator_name:str,\n                                         height:int=800, width:int=800)\n\nPrint a selection of metrics and plots for a given estimator.\nBy default, orders estimators according to the first metric.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_name\nstr\n\nName of estimator to analyze.\n\n\nheight\nint\n800\nHeight of output Figure.\n\n\nwidth\nint\n800\nWidth of output Figure.\n\n\nReturns\nFigure\n\nFigure\n\n\n\nPoniardBaseEstimator.analyze_estimator provides a quick overview of an estimator’s performance.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom poniard import PoniardClassifier\n\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\npnd = PoniardClassifier().setup(X, y, show_info=False)\npnd.fit()\n\n\n\n\nPoniardClassifier()\n\n\n\npnd.analyze_estimator(\"SVC\", height=1000, width=1000)"
  },
  {
    "objectID": "estimators.core.html#modifying-estimators-after-initialization",
    "href": "estimators.core.html#modifying-estimators-after-initialization",
    "title": "Base estimator",
    "section": "Modifying estimators after initialization",
    "text": "Modifying estimators after initialization\nEstimators can be added and removed directly. Note that if other estimators have been already fit, only the added ones will be processed during PoniardBaseEstimator.fit.\n\nsource\n\nPoniardBaseEstimator.add_estimators\n\n PoniardBaseEstimator.add_estimators (estimators:Union[Dict[str,sklearn.ba\n                                      se.ClassifierMixin],Sequence[sklearn\n                                      .base.ClassifierMixin]])\n\nInclude new estimator. This is the recommended way of adding an estimator (as opposed to modifying pipelines directly), since it also injects random state, n_jobs and verbosity.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nestimators\nUnion[Dict[str, ClassifierMixin], Sequence[ClassifierMixin]]\nEstimators to add.\n\n\nReturns\nPoniardBaseEstimator\nSelf.\n\n\n\n\nsource\n\n\nPoniardBaseEstimator.remove_estimators\n\n PoniardBaseEstimator.remove_estimators (estimator_names:Sequence[str],\n                                         drop_results:bool=True)\n\nRemove estimators. This is the recommended way of removing an estimator (as opposed to modifying pipelines directly), since it also removes the associated rows from the results tables.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_names\nSequence[str]\n\nEstimators to remove.\n\n\ndrop_results\nbool\nTrue\nWhether to remove the results associated with the estimators. Default True.\n\n\nReturns\nPoniardBaseEstimator\n\nSelf.\n\n\n\n\npnd.add_estimators(ExtraTreesClassifier())\npnd.remove_estimators(\"RandomForestClassifier\")\npnd.fit()\npnd.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      0.995456\n      0.978916\n      0.975411\n      0.991549\n      0.983351\n      0.007645\n      0.002424\n    \n    \n      SVC\n      0.994139\n      0.975408\n      0.975111\n      0.985955\n      0.980477\n      0.008037\n      0.003919\n    \n    \n      HistGradientBoostingClassifier\n      0.994128\n      0.970129\n      0.967263\n      0.985955\n      0.976433\n      0.539054\n      0.016192\n    \n    \n      XGBClassifier\n      0.994123\n      0.970129\n      0.967554\n      0.985915\n      0.976469\n      0.049444\n      0.004278\n    \n    \n      ExtraTreesClassifier\n      0.991055\n      0.968359\n      0.969925\n      0.980321\n      0.974955\n      0.042767\n      0.008918\n    \n    \n      GaussianNB\n      0.988730\n      0.929700\n      0.940993\n      0.949413\n      0.944300\n      0.003169\n      0.004466\n    \n    \n      KNeighborsClassifier\n      0.980610\n      0.964881\n      0.955018\n      0.991628\n      0.972746\n      0.002539\n      0.016843\n    \n    \n      DecisionTreeClassifier\n      0.920983\n      0.926223\n      0.941672\n      0.941080\n      0.941054\n      0.005269\n      0.002359\n    \n    \n      DummyClassifier\n      0.500000\n      0.627418\n      0.627418\n      1.000000\n      0.771052\n      0.001970\n      0.002919\n    \n  \n\n\n\n\n\npnd.pipelines.keys()\n\ndict_keys(['LogisticRegression', 'GaussianNB', 'SVC', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'HistGradientBoostingClassifier', 'XGBClassifier', 'DummyClassifier', 'ExtraTreesClassifier'])"
  },
  {
    "objectID": "estimators.core.html#modifying-the-preprocessor-after-initializaiton",
    "href": "estimators.core.html#modifying-the-preprocessor-after-initializaiton",
    "title": "Base estimator",
    "section": "Modifying the preprocessor after initializaiton",
    "text": "Modifying the preprocessor after initializaiton\nThe preprocessor can be modified from within PoniardBaseEstimator in two ways after PoniardBaseEstimator.setup:\n\nreassign_types so that features are processed by other transformers, i.e., a numeric feature could be cast to a high cardinality categorical (for example, a store ID).\nadd_preprocessing_step adds a transformer or pipeline to the existing preprocessor.\n\nSee the Preprocessing guide for examples.\n\nsource\n\nPoniardBaseEstimator.reassign_types\n\n PoniardBaseEstimator.reassign_types\n                                      (numeric:Optional[List[Union[str,int\n                                      ]]]=None, categorical_high:Optional[\n                                      List[Union[str,int]]]=None, categori\n                                      cal_low:Optional[List[Union[str,int]\n                                      ]]=None, datetime:Optional[List[Unio\n                                      n[str,int]]]=None,\n                                      keep_remainder:bool=True)\n\nReassign feature types. By default, leaves ommitted features as they were.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnumeric\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_high\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_low\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ndatetime\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\nkeep_remainder\nbool\nTrue\nWhether to keep features not specified in the method parametersas is or drop them\n\n\nReturns\nPoniardBaseEstimator\n\nself.\n\n\n\n\nsource\n\n\nPoniardBaseEstimator.add_preprocessing_step\n\n PoniardBaseEstimator.add_preprocessing_step (step:Union[sklearn.pipeline.\n                                              Pipeline,sklearn.base.Transf\n                                              ormerMixin,sklearn.compose._\n                                              column_transformer.ColumnTra\n                                              nsformer,Tuple[str,Union[skl\n                                              earn.pipeline.Pipeline,sklea\n                                              rn.base.TransformerMixin,skl\n                                              earn.compose._column_transfo\n                                              rmer.ColumnTransformer]]], p\n                                              osition:Union[str,int]='end'\n                                              )\n\nAdd a preprocessing step.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstep\nUnion[Union[Pipeline, TransformerMixin, ColumnTransformer], Tuple[str, Union[Pipeline, TransformerMixin, ColumnTransformer]]]\n\nA tuple of (str, transformer) or a scikit-learn transformer. Note thatthe transformer can also be a Pipeline or ColumnTransformer.\n\n\nposition\nUnion[str, int]\nend\nEither an integer denoting before which step in the existing preprocessing pipelinethe new step should be added, or ‘start’ or ‘end’.\n\n\nReturns\nPipeline\n\nself"
  },
  {
    "objectID": "estimators.core.html#prediction-methods",
    "href": "estimators.core.html#prediction-methods",
    "title": "Base estimator",
    "section": "Prediction methods",
    "text": "Prediction methods\nCross validated predictions (using scikit-learn’s cross_val_predict) can be obtained by calling the predict, predict_proba, decision_function or predict_all methods. Each of them takes an estimator_names parameter that specifies which models should be used.\n\n\n\n\n\n\nEstimators without specific prediction methods\n\n\n\nNot every scikit-learn model includes the predict_proba method. Poniard will set them to numpy.nan instead of throwing an error.\n\n\n\nsource\n\nPoniardBaseEstimator.predict\n\n PoniardBaseEstimator.predict\n                               (estimator_names:Optional[Sequence[str]]=No\n                               ne)\n\nGet cross validated target predictions where each sample belongs to a single test set.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_names\nOptional[Sequence[str]]\nNone\nEstimators to include. If None, predict all estimators.\n\n\nReturns\nDict[str, np.ndarray]\n\nDict where keys are estimator names and values are numpy arrays of predictions.\n\n\n\n\nsource\n\n\nPoniardBaseEstimator.predict_proba\n\n PoniardBaseEstimator.predict_proba\n                                     (estimator_names:Optional[Sequence[st\n                                     r]]=None)\n\nGet cross validated target probability predictions where each sample belongs to a single test set.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_names\nOptional[Sequence[str]]\nNone\n\n\n\nReturns\nDict[str, np.ndarray]\n\nDict where keys are estimator names and values are numpy arrays of predictionprobabilities.\n\n\n\n\nsource\n\n\nPoniardBaseEstimator.decision_function\n\n PoniardBaseEstimator.decision_function\n                                         (estimator_names:Optional[Sequenc\n                                         e[str]]=None)\n\nGet cross validated decision function predictions where each sample belongs to a single test set.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_names\nOptional[Sequence[str]]\nNone\nEstimators to include. If None, predict all estimators.\n\n\nReturns\nDict[str, np.ndarray]\n\nDict where keys are estimator names and values are numpy arrays of decision functions.\n\n\n\n\nsource\n\n\nPoniardBaseEstimator.predict_all\n\n PoniardBaseEstimator.predict_all\n                                   (estimator_names:Optional[Sequence[str]\n                                   ]=None)\n\nGet cross validated target predictions, probabilities and decision functions where each sample belongs to a test set.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_names\nOptional[Sequence[str]]\nNone\nEstimators to include. If None, predict all estimators.\n\n\nReturns\nTuple[Dict[str, np.ndarray]]\n\nTuple of dicts where keys are estimator names and values are numpy arrays ofpredictions."
  },
  {
    "objectID": "estimators.core.html#ensembles-and-hyperparameter-tuning",
    "href": "estimators.core.html#ensembles-and-hyperparameter-tuning",
    "title": "Base estimator",
    "section": "Ensembles and hyperparameter tuning",
    "text": "Ensembles and hyperparameter tuning\n\nsource\n\nPoniardBaseEstimator.build_ensemble\n\n PoniardBaseEstimator.build_ensemble (method:str='stacking',\n                                      estimator_names:Optional[Sequence[st\n                                      r]]=None, top_n:Optional[int]=3,\n                                      sort_by:Optional[str]=None,\n                                      ensemble_name:Optional[str]=None,\n                                      **kwargs)\n\nCombine estimators into an ensemble.\nBy default, orders estimators according to the first metric.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmethod\nstr\nstacking\nEnsemble method. Either “stacking” or “voting”. Default “stacking”.\n\n\nestimator_names\nOptional[Sequence[str]]\nNone\nNames of estimators to include. Default None, which uses top_n\n\n\ntop_n\nOptional[int]\n3\nHow many of the best estimators to include.\n\n\nsort_by\nOptional[str]\nNone\nWhich metric to consider for ordering results. Default None, which uses the first metric.\n\n\nensemble_name\nOptional[str]\nNone\nEnsemble name when adding to pipelines. Default None.\n\n\nkwargs\n\n\nPassed to the ensemble class constructor.\n\n\nReturns\nPoniardBaseEstimator\n\nSelf.\n\n\n\n\npnd.build_ensemble(\n    method=\"stacking\",\n    estimator_names=[\"DecisionTreeClassifier\", \"KNeighborsClassifier\", \"SVC\"],\n)\npnd.get_estimator(\"StackingClassifier\")\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())],\n                          verbose=0)),\n                ('StackingClassifier',\n                 StackingClassifier(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n                                    estimators=[('DecisionTreeClassifier',\n                                                 DecisionTreeClassifier(random_state=0)),\n                                                ('KNeighborsClassifier',\n                                                 KNeighborsClassifier()),\n                                                ('SVC',\n                                                 SVC(kernel='linear',\n                                                     probability=True,\n                                                     random_state=0,\n                                                     verbose=0))]))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())],\n                          verbose=0)),\n                ('StackingClassifier',\n                 StackingClassifier(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n                                    estimators=[('DecisionTreeClassifier',\n                                                 DecisionTreeClassifier(random_state=0)),\n                                                ('KNeighborsClassifier',\n                                                 KNeighborsClassifier()),\n                                                ('SVC',\n                                                 SVC(kernel='linear',\n                                                     probability=True,\n                                                     random_state=0,\n                                                     verbose=0))]))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: PipelinePipeline(steps=[('numeric_imputer', SimpleImputer()),\n                ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()VarianceThresholdVarianceThreshold()StackingClassifier: StackingClassifierStackingClassifier(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n                   estimators=[('DecisionTreeClassifier',\n                                DecisionTreeClassifier(random_state=0)),\n                               ('KNeighborsClassifier', KNeighborsClassifier()),\n                               ('SVC',\n                                SVC(kernel='linear', probability=True,\n                                    random_state=0, verbose=0))])DecisionTreeClassifierDecisionTreeClassifierDecisionTreeClassifier(random_state=0)KNeighborsClassifierKNeighborsClassifierKNeighborsClassifier()SVCSVCSVC(kernel='linear', probability=True, random_state=0, verbose=0)final_estimatorLogisticRegressionLogisticRegression()\n\n\n\npnd.fit()\npnd.get_results()\n\n\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      0.995456\n      0.978916\n      0.975411\n      0.991549\n      0.983351\n      0.007645\n      0.002424\n    \n    \n      SVC\n      0.994139\n      0.975408\n      0.975111\n      0.985955\n      0.980477\n      0.008037\n      0.003919\n    \n    \n      HistGradientBoostingClassifier\n      0.994128\n      0.970129\n      0.967263\n      0.985955\n      0.976433\n      0.539054\n      0.016192\n    \n    \n      XGBClassifier\n      0.994123\n      0.970129\n      0.967554\n      0.985915\n      0.976469\n      0.049444\n      0.004278\n    \n    \n      StackingClassifier\n      0.993999\n      0.973653\n      0.967485\n      0.991588\n      0.979308\n      0.053218\n      0.005176\n    \n    \n      ExtraTreesClassifier\n      0.991055\n      0.968359\n      0.969925\n      0.980321\n      0.974955\n      0.042767\n      0.008918\n    \n    \n      GaussianNB\n      0.988730\n      0.929700\n      0.940993\n      0.949413\n      0.944300\n      0.003169\n      0.004466\n    \n    \n      KNeighborsClassifier\n      0.980610\n      0.964881\n      0.955018\n      0.991628\n      0.972746\n      0.002539\n      0.016843\n    \n    \n      DecisionTreeClassifier\n      0.920983\n      0.926223\n      0.941672\n      0.941080\n      0.941054\n      0.005269\n      0.002359\n    \n    \n      DummyClassifier\n      0.500000\n      0.627418\n      0.627418\n      1.000000\n      0.771052\n      0.001970\n      0.002919\n    \n  \n\n\n\n\nUse get_predictions_similarity to compute how correlated the estimators’ predictions are. This can be useful for building ensembles with PoniardBaseEstimator.build_ensemble.\n\nsource\n\n\nPoniardBaseEstimator.get_predictions_similarity\n\n PoniardBaseEstimator.get_predictions_similarity (on_errors:bool=True)\n\nCompute correlation/association between cross validated predictions for each estimator.\nThis can be useful for ensembling.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\non_errors\nbool\nTrue\nWhether to compute similarity on prediction errors instead of predictions. DefaultTrue.\n\n\nReturns\npd.DataFrame\n\nSimilarity.\n\n\n\n\npnd.get_predictions_similarity()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      LogisticRegression\n      GaussianNB\n      SVC\n      KNeighborsClassifier\n      DecisionTreeClassifier\n      HistGradientBoostingClassifier\n      XGBClassifier\n      ExtraTreesClassifier\n      StackingClassifier\n    \n  \n  \n    \n      LogisticRegression\n      1.000000\n      0.315978\n      0.726194\n      0.401876\n      0.211925\n      0.367325\n      0.294833\n      0.426033\n      0.547327\n    \n    \n      GaussianNB\n      0.315978\n      1.000000\n      0.331160\n      0.524911\n      0.354022\n      0.454955\n      0.495528\n      0.518582\n      0.489758\n    \n    \n      SVC\n      0.726194\n      0.331160\n      1.000000\n      0.368042\n      0.277664\n      0.403438\n      0.336311\n      0.390700\n      0.574735\n    \n    \n      KNeighborsClassifier\n      0.401876\n      0.524911\n      0.368042\n      1.000000\n      0.363762\n      0.497702\n      0.497702\n      0.482094\n      0.712582\n    \n    \n      DecisionTreeClassifier\n      0.211925\n      0.354022\n      0.277664\n      0.363762\n      1.000000\n      0.362908\n      0.521706\n      0.427338\n      0.392178\n    \n    \n      HistGradientBoostingClassifier\n      0.367325\n      0.454955\n      0.403438\n      0.497702\n      0.362908\n      1.000000\n      0.726570\n      0.645759\n      0.582252\n    \n    \n      XGBClassifier\n      0.294833\n      0.495528\n      0.336311\n      0.497702\n      0.521706\n      0.726570\n      1.000000\n      0.704906\n      0.517572\n    \n    \n      ExtraTreesClassifier\n      0.426033\n      0.518582\n      0.390700\n      0.482094\n      0.427338\n      0.645759\n      0.704906\n      1.000000\n      0.564618\n    \n    \n      StackingClassifier\n      0.547327\n      0.489758\n      0.574735\n      0.712582\n      0.392178\n      0.582252\n      0.517572\n      0.564618\n      1.000000\n    \n  \n\n\n\n\nPoniard offers light hyperparameter tuning through tune_estimator, as well as hyperparameter grids for its default estimators. You are however free to specify whichever grid you want.\n\nsource\n\n\nPoniardBaseEstimator.tune_estimator\n\n PoniardBaseEstimator.tune_estimator (estimator_name:str,\n                                      grid:Optional[Dict]=None,\n                                      mode:str='grid', tuned_estimator_nam\n                                      e:Optional[str]=None, **kwargs)\n\nHyperparameter tuning for a single estimator.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator_name\nstr\n\nEstimator to tune.\n\n\ngrid\nOptional[Dict]\nNone\nHyperparameter grid. Default None, which uses the grids available for defaultestimators.\n\n\nmode\nstr\ngrid\nType of search. Eitherr “grid”, “halving” or “random”. Default “grid”.\n\n\ntuned_estimator_name\nOptional[str]\nNone\nEstimator name when adding to pipelines. Default None.\n\n\nkwargs\n\n\nPassed to the search class constructor.\n\n\nReturns\nUnion[GridSearchCV, RandomizedSearchCV]\n\nSelf."
  },
  {
    "objectID": "preprocessing.categorical.html",
    "href": "preprocessing.categorical.html",
    "title": "Categorical preprocessors",
    "section": "",
    "text": "source\n\nTargetEncoder\n\n TargetEncoder (task:str, handle_unknown='error', handle_missing='')\n\nEncode categorical features considering the effect that it has in the target variable.\nNote that implementation and docstrings are largely taken from Dirty Cat.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntask\nstr\n\nThe type of problem. Either “classification” or “regression”.\n\n\nhandle_unknown\nstr\nerror\nEither “error” or “ignore”. Whether to raise an error or ignore if a unknowncategorical feature is present during transform (default is to raise). If ‘ignore’,unknown categories will be set to the mean of the target.\n\n\nhandle_missing\nstr\n\nEither “error” or ““. Whether to raise an error or impute with blank string”” if missingvalues (NaN) are present during fit (default is to impute).\n\n\n\nIn general, TargetEncoder takes the ratio between the mean of the target for a given category and the mean of the target. In addition, it takes an empirical Bayes approach to shrink the estimate.\nIt is particularly useful with high cardinality categoricals, as it will not expand the feature space as much as one hot encoding, but retains more information than ordinal encoding.\nFor more details, see Micci-Barreca, 2001: A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems.\n\nsource\n\n\nTargetEncoder.fit\n\n TargetEncoder.fit\n                    (X:Union[pandas.core.frame.DataFrame,numpy.ndarray,Lis\n                    t], y:Union[pandas.core.frame.DataFrame,numpy.ndarray,\n                    List])\n\nFit the TargetEncoder to X.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nThe data to determine the categories of each feature.\n\n\ny\nUnion[pd.DataFrame, np.ndarray, List]\nThe associated target vector.\n\n\nReturns\nTargetEncoder\nFitted TargetEncoder.\n\n\n\nAfter fitting, the categories of each feature are held in the categories_ attribute.\n\nsource\n\n\nTargetEncoder.transform\n\n TargetEncoder.transform\n                          (X:Union[pandas.core.frame.DataFrame,numpy.ndarr\n                          ay,List])\n\nTransform X using specified encoding scheme.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nTransformed input.\n\n\n\n\n\n\nTransformerMixin.fit_transform\n\n TransformerMixin.fit_transform (X, y=None, **fit_params)\n\nFit to data, then transform it.\nFits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\narray-like of shape (n_samples, n_features)\n\nInput samples.\n\n\ny\nNoneType\nNone\nTarget values (None for unsupervised transformations).\n\n\nfit_params\n\n\n\n\n\nReturns\nndarray array of shape (n_samples, n_features_new)\n\nTransformed array.\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\nrng = np.random.default_rng(0)\n\nX = pd.DataFrame(\n    {\n        \"sex\": rng.choice([\"female\", \"male\", \"other\"], size=10),\n        \"status\": rng.choice(\n            [\"employed\", \"unemployed\", \"retired\", \"inactive\"], size=10\n        ),\n    }\n)\ny = rng.choice([\"low\", \"high\"], size=10)\n\nencoder = TargetEncoder(task=\"classification\", handle_unknown=\"ignore\")\npd.DataFrame(encoder.fit_transform(X, y), columns=encoder.get_feature_names_out())\n\n\n\n\n\n  \n    \n      \n      sex\n      status\n    \n  \n  \n    \n      0\n      0.437500\n      0.416667\n    \n    \n      1\n      0.250000\n      0.375000\n    \n    \n      2\n      0.250000\n      0.416667\n    \n    \n      3\n      0.464286\n      0.416667\n    \n    \n      4\n      0.464286\n      0.375000\n    \n    \n      5\n      0.464286\n      0.416667\n    \n    \n      6\n      0.464286\n      0.416667\n    \n    \n      7\n      0.464286\n      0.416667\n    \n    \n      8\n      0.464286\n      0.416667\n    \n    \n      9\n      0.437500\n      0.375000\n    \n  \n\n\n\n\nIn the case of a multiclass target, the encodings are computed separately for each label, meaning that each feature will be expanded to as many unique levels in the target.\n\ny = rng.choice([\"low\", \"mid\", \"high\"], size=10)\n\nencoder = TargetEncoder(task=\"classification\", handle_unknown=\"ignore\")\npd.DataFrame(encoder.fit_transform(X, y), columns=encoder.get_feature_names_out())\n\n\n\n\n\n  \n    \n      \n      sex_high\n      sex_low\n      sex_mid\n      status_high\n      status_low\n      status_mid\n    \n  \n  \n    \n      0\n      0.312500\n      0.3125\n      0.375000\n      0.250\n      0.458333\n      0.291667\n    \n    \n      1\n      0.125000\n      0.6875\n      0.187500\n      0.125\n      0.562500\n      0.312500\n    \n    \n      2\n      0.125000\n      0.6875\n      0.187500\n      0.250\n      0.458333\n      0.291667\n    \n    \n      3\n      0.178571\n      0.5000\n      0.321429\n      0.250\n      0.458333\n      0.291667\n    \n    \n      4\n      0.178571\n      0.5000\n      0.321429\n      0.125\n      0.562500\n      0.312500\n    \n    \n      5\n      0.178571\n      0.5000\n      0.321429\n      0.250\n      0.458333\n      0.291667\n    \n    \n      6\n      0.178571\n      0.5000\n      0.321429\n      0.250\n      0.458333\n      0.291667\n    \n    \n      7\n      0.178571\n      0.5000\n      0.321429\n      0.250\n      0.458333\n      0.291667\n    \n    \n      8\n      0.178571\n      0.5000\n      0.321429\n      0.250\n      0.458333\n      0.291667\n    \n    \n      9\n      0.312500\n      0.3125\n      0.375000\n      0.125\n      0.562500\n      0.312500"
  },
  {
    "objectID": "plugins.wandb.html",
    "href": "plugins.wandb.html",
    "title": "Weights and Biases",
    "section": "",
    "text": "WandBPlugin logs data and training information to a Weights and Biases project, making it easy to keep track of preprocessing strategies and models, as well as creating experiment reports.\n\nsource\n\nWandBPlugin\n\n WandBPlugin (project:Optional[str]=None, entity:Optional[str]=None,\n              **kwargs)\n\nWeights and Biases plugin. Kwargs from the constructor are passed to wandb.init().\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nproject\ntyping.Optional[str]\nNone\nName of the Weights and Biases project.\n\n\nentity\ntyping.Optional[str]\nNone\nName of the Weights and Biases entity (username).\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nWandBPlugin.on_setup_data\n\n WandBPlugin.on_setup_data ()\n\nLog features and target(s) as a single dataset artifact.\n\nsource\n\n\nWandBPlugin.on_setup_end\n\n WandBPlugin.on_setup_end ()\n\nLog WandB config that contains the parameters passed to the Poniard estimator constructor.\n\nsource\n\n\nWandBPlugin.on_infer_types\n\n WandBPlugin.on_infer_types ()\n\nLog the inferred types dataframe as a table.\n\nsource\n\n\nWandBPlugin.on_setup_preprocessor\n\n WandBPlugin.on_setup_preprocessor ()\n\nLog the preprocessor’s HTML repr.\n\nsource\n\n\nWandBPlugin.on_plot\n\n WandBPlugin.on_plot (figure:plotly.graph_objs._figure.Figure, name:str)\n\nLog a Plotly plot.\n\nsource\n\n\nWandBPlugin.on_fit_end\n\n WandBPlugin.on_fit_end ()\n\nLog the results dataframe as a table.\n\nsource\n\n\nWandBPlugin.on_analyze_estimator\n\n WandBPlugin.on_analyze_estimator (estimator:sklearn.base.BaseEstimator,\n                                   name:str)\n\nLog fitted estimator as an artifact."
  },
  {
    "objectID": "preprocessing.core.html",
    "href": "preprocessing.core.html",
    "title": "Poniard Preprocessor",
    "section": "",
    "text": "source\n\nPoniardPreprocessor\n\n PoniardPreprocessor (task:Optional[str]=None,\n                      scaler:Optional[Union[str,TransformerMixin]]=None, h\n                      igh_cardinality_encoder:Optional[Union[str,Transform\n                      erMixin]]=None, numeric_imputer:Optional[Union[str,T\n                      ransformerMixin]]=None, custom_preprocessor:Union[No\n                      ne,Pipeline,TransformerMixin]=None,\n                      numeric_threshold:Union[int,float]=0.1,\n                      cardinality_threshold:Union[int,float]=20,\n                      verbose:int=0, random_state:Optional[int]=None,\n                      n_jobs:Optional[int]=None,\n                      cache_transformations:bool=False)\n\nBase preprocessor that builds an easily modifiable pipeline based on feature data types.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntask\nOptional[str]\nNone\n\n\n\nscaler\nOptional[Union[str, TransformerMixin]]\nNone\nNumeric scaler method. Either “standard”, “minmax”, “robust” or scikit-learn Transformer.\n\n\nhigh_cardinality_encoder\nOptional[Union[str, TransformerMixin]]\nNone\nEncoder for categorical features with high cardinality. Either “target” or “ordinal”,or scikit-learn Transformer.\n\n\nnumeric_imputer\nOptional[Union[str, TransformerMixin]]\nNone\nImputation method. Either “simple”, “iterative” or scikit-learn Transformer.\n\n\ncustom_preprocessor\nUnion[None, Pipeline, TransformerMixin]\nNone\n\n\n\nnumeric_threshold\nUnion[int, float]\n0.1\nNumber features with unique values above a certain threshold will be treated as numeric. Iffloat, the threshold is numeric_threshold * samples.\n\n\ncardinality_threshold\nUnion[int, float]\n20\nNon-number features with cardinality above a certain threshold will be treated asordinal encoded instead of one-hot encoded. If float, the threshold iscardinality_threshold * samples.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\nOptional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\nOptional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction.\n\n\ncache_transformations\nbool\nFalse\nWhether to cache transformations and set the memory parameter for Pipelines. This canspeed up slow transformations as they are not recalculated for each estimator.\n\n\n\nPoniardPreprocessor’s job is to build a preprocessing pipeline that fits the input data, both features and target. It does this by inferring the types of the features and selecting appropiate family of transformers for each group. The user is free to select which particular transformer to choose for each group, for example, by changing the default numeric scaler from StandardScaler to RobustScaler.\nCustomization is done through 3 parameters related to transformers (scaler, high_cardinality_encoder and numeric_imputer), which take standard sklearn-compatible transformers, and 2 parameters related to type inference (numeric_threshold and cardinality_threshold).\nThe latter work by separating features into buckets. In particular, numeric (int, float) features can be left as numeric or cast to a high cardinality categorical (if the number of unique values is below numeric_threshold), while categoricals can either by low or high cardinality (if the number of unique values exceeds categorical_threshold).\n\nsource\n\n\nPoniardPreprocessor.build\n\n PoniardPreprocessor.build\n                            (X:Union[pandas.core.frame.DataFrame,numpy.nda\n                            rray,List,NoneType]=None, y:Union[pandas.core.\n                            frame.DataFrame,numpy.ndarray,List,NoneType]=N\n                            one)\n\nBuilds the preprocessor according to the input data.\nGets the data from the main PoniardBaseEstimator (if available) or processes the input data, calls the type inference method, sets up the transformers and builds the pipeline.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nOptional[Union[pd.DataFrame, np.ndarray, List]]\nNone\nFeatures\n\n\ny\nOptional[Union[pd.DataFrame, np.ndarray, List]]\nNone\nTarget.\n\n\nReturns\nPoniardPreprocessor\n\n\n\n\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame(\n    {\n        \"type\": random.choices([\"house\", \"apartment\"], k=500),\n        \"age\": rng.uniform(1, 200, 500).astype(int),\n        \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n        \"rating\": random.choices(range(50), k=500),\n        \"target\": random.choices([0, 1], k=500),\n    }\n)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      type\n      age\n      date\n      rating\n      target\n    \n  \n  \n    \n      0\n      apartment\n      127\n      2022-01-31\n      1\n      1\n    \n    \n      1\n      apartment\n      54\n      2022-02-28\n      17\n      1\n    \n    \n      2\n      house\n      9\n      2022-03-31\n      0\n      1\n    \n    \n      3\n      house\n      4\n      2022-04-30\n      48\n      1\n    \n    \n      4\n      apartment\n      162\n      2022-05-31\n      40\n      0\n    \n  \n\n\n\n\nIf running a standalone PoniardPreprocessor, task (either “classification” or “regression”) has to be specified in the constructor.\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\nprep = PoniardPreprocessor(task=\"classification\").build(X, y)\n\nThe actual preprocessing pipeline is held within the preprocessor attribute.\n\nprep.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nPoniardPreprocessor is included by default in and tightly coupled with PoniardBaseEstimator. During PoniardBaseEstimator.setup a preprocessor instance will be initialized, and the whole estimator instance will be passed to the preprocessor’s _poniard attribute, giving it access to the data. Likewise, running PoniardBaseEstimator.reassign_types and PoniardBaseEstimator.add_preprocessing_step will trigger changes in PoniardPreprocessor.\n\nfrom poniard import PoniardClassifier\nfrom poniard.preprocessing import PoniardPreprocessor\n\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\nclf = PoniardClassifier().setup(X, y)\n\n\n                         Setup info\n                         Target\n                             Type: binary\n                             Shape: (500,)\n                             Unique values: 2\n                             Metrics\n                             Main metric: roc_auc\n                             \n\n\n\n Feature type inference\n                                Minimum unique values to consider a number-like feature numeric: 50\n                                Minimum unique values to consider a categorical feature high cardinality: 20\n                                Inferred feature types:\n                                \n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\nclf._poniard_preprocessor\n\nPoniardPreprocessor()\n\n\n\nclf._poniard_preprocessor.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())],\n         verbose=0)type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nHowever, a custom instance of PoniardPreprocessor can be passed to estimators.\n\ncustom = PoniardPreprocessor(scaler=\"robust\", numeric_imputer=\"iterative\")\nclf = PoniardClassifier(custom_preprocessor=custom).setup(X, y, show_info=False)\nclf.fit()\n\n\n\n\nPoniardClassifier(custom_preprocessor=PoniardPreprocessor(scaler='robust', numeric_imputer='iterative'))"
  }
]