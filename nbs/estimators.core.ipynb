{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base estimator\n",
    "\n",
    "> `PoniardBaseEstimator` is where the magic happens. As a user, you should be using `PoniardClassifier` and `PoniardRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp estimators.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "import itertools\n",
    "import inspect\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Union, Callable, Dict, Tuple, Any, Sequence, Iterable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    import ipywidgets\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin, TransformerMixin, clone\n",
    "from sklearn.model_selection._split import BaseCrossValidator, BaseShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import (\n",
    "    VotingClassifier,\n",
    "    VotingRegressor,\n",
    "    StackingClassifier,\n",
    "    StackingRegressor,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.model_selection import (\n",
    "    cross_validate,\n",
    "    cross_val_predict,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "from poniard.preprocessing import DatetimeEncoder, TargetEncoder\n",
    "from poniard.utils.stats import cramers_v\n",
    "from poniard.utils.hyperparameters import GRID\n",
    "from poniard.utils.estimate import get_target_info, element_to_list_maybe\n",
    "from poniard.plot import PoniardPlotFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PoniardBaseEstimator(ABC):\n",
    "    \"\"\"Base estimator that sets up all the functionality for the classifier and regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimators :\n",
    "        Estimators to evaluate.\n",
    "    metrics :\n",
    "        Metrics to compute for each estimator. This is more restrictive than sklearn's scoring\n",
    "        parameter, as it does not allow callable scorers. Single strings are cast to lists\n",
    "        automatically.\n",
    "    preprocess : bool, optional\n",
    "        If True, impute missing values, standard scale numeric data and one-hot or ordinal\n",
    "        encode categorical data.\n",
    "    scaler :\n",
    "        Numeric scaler method. Either \"standard\", \"minmax\", \"robust\" or scikit-learn Transformer.\n",
    "    high_cardinality_encoder :\n",
    "        Encoder for categorical features with high cardinality. Either \"target\" or \"ordinal\",\n",
    "        or scikit-learn Transformer.\n",
    "    numeric_imputer :\n",
    "        Imputation method. Either \"simple\", \"iterative\" or scikit-learn Transformer.\n",
    "    custom_preprocessor :\n",
    "        Preprocessor used instead of the default preprocessing pipeline. It must be able to be\n",
    "        included directly in a scikit-learn Pipeline.\n",
    "    numeric_threshold :\n",
    "        Number features with unique values above a certain threshold will be treated as numeric. If\n",
    "        float, the threshold is `numeric_threshold * samples`.\n",
    "    cardinality_threshold :\n",
    "        Non-number features with cardinality above a certain threshold will be treated as\n",
    "        ordinal encoded instead of one-hot encoded. If float, the threshold is\n",
    "        `cardinality_threshold * samples`.\n",
    "    cv :\n",
    "        Cross validation strategy. Either an integer, a scikit-learn cross validation object,\n",
    "        or an iterable.\n",
    "    verbose :\n",
    "        Verbosity level. Propagated to every scikit-learn function and estimator.\n",
    "    random_state :\n",
    "        RNG. Propagated to every scikit-learn function and estimator. The default None sets\n",
    "        random_state to 0 so that cross_validate results are comparable.\n",
    "    n_jobs :\n",
    "        Controls parallel processing. -1 uses all cores. Propagated to every scikit-learn\n",
    "        function.\n",
    "    plugins :\n",
    "        Plugin instances that run in set moments of setup, fit and plotting.\n",
    "    plot_options :\n",
    "        :class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly format\n",
    "        options or None, which sets the default factory.\n",
    "    cache_transformations :\n",
    "        Whether to cache transformations and set the `memory` parameter for Pipelines. This can speed up slow transformations as they are not recalculated for each estimator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators: Optional[\n",
    "            Union[\n",
    "                Sequence[ClassifierMixin],\n",
    "                Dict[str, ClassifierMixin],\n",
    "                Sequence[RegressorMixin],\n",
    "                Dict[str, RegressorMixin],\n",
    "            ]\n",
    "        ] = None,\n",
    "        metrics: Optional[Union[str, Dict[str, Callable], Sequence[str]]] = None,\n",
    "        preprocess: bool = True,\n",
    "        scaler: Optional[Union[str, TransformerMixin]] = None,\n",
    "        high_cardinality_encoder: Optional[Union[str, TransformerMixin]] = None,\n",
    "        numeric_imputer: Optional[Union[str, TransformerMixin]] = None,\n",
    "        custom_preprocessor: Union[None, Pipeline, TransformerMixin] = None,\n",
    "        numeric_threshold: Union[int, float] = 0.1,\n",
    "        cardinality_threshold: Union[int, float] = 20,\n",
    "        cv: Union[int, BaseCrossValidator, BaseShuffleSplit, Sequence] = None,\n",
    "        verbose: int = 0,\n",
    "        random_state: Optional[int] = None,\n",
    "        n_jobs: Optional[int] = None,\n",
    "        plugins: Optional[Sequence[Any]] = None,\n",
    "        plot_options: Optional[PoniardPlotFactory] = None,\n",
    "        cache_transformations: bool = False,\n",
    "    ):\n",
    "        # TODO: Ugly check that metrics conforms to expected types. Should improve.\n",
    "        if metrics and (\n",
    "            (\n",
    "                isinstance(metrics, Sequence)\n",
    "                and not all(isinstance(m, str) for m in metrics)\n",
    "            )\n",
    "            or (\n",
    "                isinstance(metrics, Dict)\n",
    "                and not all(isinstance(m, str) for m in metrics.keys())\n",
    "                and not all(isinstance(m, Callable) for m in metrics.values())\n",
    "            )\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"metrics can only be a string, a sequence of strings, a dict with \"\n",
    "                \"strings as keys and callables as values, or None.\"\n",
    "            )\n",
    "        self.metrics = metrics\n",
    "        self.preprocess = preprocess\n",
    "        self.scaler = scaler or \"standard\"\n",
    "        self.high_cardinality_encoder = high_cardinality_encoder or \"target\"\n",
    "        self.numeric_imputer = numeric_imputer or \"simple\"\n",
    "        self.numeric_threshold = numeric_threshold\n",
    "        self.custom_preprocessor = custom_preprocessor\n",
    "        self.cardinality_threshold = cardinality_threshold\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state or 0\n",
    "        self.estimators = element_to_list_maybe(estimators)\n",
    "        self.n_jobs = n_jobs\n",
    "        if cache_transformations:\n",
    "            self._memory = joblib.Memory(\"transformation_cache\", verbose=self.verbose)\n",
    "        else:\n",
    "            self._memory = None\n",
    "\n",
    "        self._init_plugins(plugins)\n",
    "        self._init_plots(plot_options)\n",
    "\n",
    "    def _init_plugins(self, plugins: Optional[Sequence[Any]] = None) -> None:\n",
    "        self.plugins = element_to_list_maybe(plugins)\n",
    "        if self.plugins:\n",
    "            [setattr(plugin, \"_poniard\", self) for plugin in self.plugins]\n",
    "        return\n",
    "\n",
    "    def _init_plots(self, plot_options: Optional[PoniardPlotFactory] = None) -> None:\n",
    "        self.plot_options = plot_options or PoniardPlotFactory()\n",
    "        self.plot = self.plot_options\n",
    "        self.plot._poniard = self\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def poniard_task(self) -> Optional[str]:\n",
    "        \"\"\"Check whether self is a Poniard regressor or classifier.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[str]\n",
    "            \"regression\", \"classification\" or None\n",
    "        \"\"\"\n",
    "        from poniard import PoniardRegressor, PoniardClassifier\n",
    "\n",
    "        if isinstance(self, PoniardRegressor):\n",
    "            return \"regression\"\n",
    "        elif isinstance(self, PoniardClassifier):\n",
    "            return \"classification\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def setup(\n",
    "        self,\n",
    "        X: Union[pd.DataFrame, np.ndarray, List],\n",
    "        y: Union[pd.DataFrame, np.ndarray, List],\n",
    "    ) -> PoniardBaseEstimator:\n",
    "        \"\"\"Orchestrator.\n",
    "\n",
    "        Converts inputs to arrays if necessary, sets `metrics`,\n",
    "        `preprocessor`, `cv` and `pipelines`.\n",
    "        \n",
    "        After running `setup`, both `X` and `y` will be held as attributes.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X :\n",
    "            Features.\n",
    "        y :\n",
    "            Target\n",
    "\n",
    "        \"\"\"\n",
    "        self._run_plugin_method(\"on_setup_start\")\n",
    "        if not isinstance(X, (pd.DataFrame, pd.Series, np.ndarray)):\n",
    "            X = np.array(X)\n",
    "        if not isinstance(y, (pd.DataFrame, pd.Series, np.ndarray)):\n",
    "            y = np.array(y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self._run_plugin_method(\"on_setup_data\")\n",
    "\n",
    "        self.target_info = get_target_info(self.y, self.poniard_task)\n",
    "        print(\"Target info\", \"-----------\", sep=\"\\n\")\n",
    "        print(\n",
    "            f\"Type: {self.target_info['type_']}\",\n",
    "            f\"Shape: {self.target_info['shape']}\",\n",
    "            f\"Unique values: {self.target_info['nunique']}\",\n",
    "            sep=\"\\n\",\n",
    "            end=\"\\n\\n\",\n",
    "        )\n",
    "        if self.target_info[\"type_\"] == \"multiclass-multioutput\":\n",
    "            raise NotImplementedError(\n",
    "                \"multiclass-multioutput targets are not supported as \"\n",
    "                \"no sklearn metrics support them.\"\n",
    "            )\n",
    "\n",
    "        if self.metrics:\n",
    "            self.metrics = element_to_list_maybe(self.metrics)\n",
    "        else:\n",
    "            self.metrics = self._build_metrics()\n",
    "        print(\n",
    "            \"Main metric\",\n",
    "            \"-----------\",\n",
    "            self._first_scorer(sklearn_scorer=False),\n",
    "            sep=\"\\n\",\n",
    "            end=\"\\n\\n\",\n",
    "        )\n",
    "\n",
    "        if self.preprocess:\n",
    "            if self.custom_preprocessor:\n",
    "                self.preprocessor = self.custom_preprocessor\n",
    "            else:\n",
    "                self.preprocessor = self._build_preprocessor()\n",
    "        self._run_plugin_method(\"on_setup_preprocessor\")\n",
    "\n",
    "        self.pipelines = self._build_pipelines()\n",
    "\n",
    "        self.cv = self._build_cv()\n",
    "\n",
    "        self._run_plugin_method(\"on_setup_end\")\n",
    "        return self\n",
    "\n",
    "    def _infer_dtypes(self) -> Tuple[List[str], List[str], List[str]]:\n",
    "        \"\"\"Infer feature types (numeric, low-cardinality categorical or high-cardinality\n",
    "        categorical).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str], List[str], List[str]\n",
    "            Three lists with column names or indices.\n",
    "        \"\"\"\n",
    "        X = self.X\n",
    "        numeric = []\n",
    "        categorical_high = []\n",
    "        categorical_low = []\n",
    "        datetime = []\n",
    "        if not isinstance(self.cardinality_threshold, int):\n",
    "            self.cardinality_threshold = int(self.cardinality_threshold * X.shape[0])\n",
    "        if not isinstance(self.numeric_threshold, int):\n",
    "            self.numeric_threshold = int(self.numeric_threshold * X.shape[0])\n",
    "        print(\n",
    "            \"Thresholds\",\n",
    "            \"----------\",\n",
    "            f\"Minimum unique values to consider a feature numeric: {self.numeric_threshold}\",\n",
    "            f\"Minimum unique values to consider a categorical high cardinality: {self.cardinality_threshold}\",\n",
    "            sep=\"\\n\",\n",
    "            end=\"\\n\\n\",\n",
    "        )\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            datetime = X.select_dtypes(\n",
    "                include=[\"datetime64[ns]\", \"datetimetz\"]\n",
    "            ).columns.tolist()\n",
    "            numbers = X.select_dtypes(include=\"number\").columns\n",
    "            for column in numbers:\n",
    "                if X[column].nunique() > self.numeric_threshold:\n",
    "                    numeric.append(column)\n",
    "                elif X[column].nunique() > self.cardinality_threshold:\n",
    "                    categorical_high.append(column)\n",
    "                else:\n",
    "                    categorical_low.append(column)\n",
    "            strings = X.select_dtypes(exclude=[\"number\", \"datetime\"]).columns\n",
    "            for column in strings:\n",
    "                if X[column].nunique() > self.cardinality_threshold:\n",
    "                    categorical_high.append(column)\n",
    "                else:\n",
    "                    categorical_low.append(column)\n",
    "        else:\n",
    "            if np.issubdtype(X.dtype, np.datetime64):\n",
    "                datetime.extend(range(X.shape[1]))\n",
    "            if np.issubdtype(X.dtype, np.number):\n",
    "                for i in range(X.shape[1]):\n",
    "                    if np.unique(X[:, i]).shape[0] > self.numeric_threshold:\n",
    "                        numeric.append(i)\n",
    "                    elif np.unique(X[:, i]).shape[0] > self.cardinality_threshold:\n",
    "                        categorical_high.append(i)\n",
    "                    else:\n",
    "                        categorical_low.append(i)\n",
    "            else:\n",
    "                for i in range(X.shape[1]):\n",
    "                    if np.unique(X[:, i]).shape[0] > self.cardinality_threshold:\n",
    "                        categorical_high.append(i)\n",
    "                    else:\n",
    "                        categorical_low.append(i)\n",
    "        self._inferred_types = {\n",
    "            \"numeric\": numeric,\n",
    "            \"categorical_high\": categorical_high,\n",
    "            \"categorical_low\": categorical_low,\n",
    "            \"datetime\": datetime,\n",
    "        }\n",
    "        print(\"Inferred feature types\", \"----------------------\", sep=\"\\n\")\n",
    "        self.inferred_types = pd.DataFrame.from_dict(\n",
    "            self._inferred_types, orient=\"index\"\n",
    "        ).T.fillna(\"\")\n",
    "        try:\n",
    "            # Try to print the table nicely\n",
    "            from IPython.display import display, HTML\n",
    "\n",
    "            display(HTML(self.inferred_types.to_html()))\n",
    "            print(\"\\n\")\n",
    "        except ImportError:\n",
    "            print(self.inferred_types)\n",
    "        self._run_plugin_method(\"on_infer_types\")\n",
    "        return numeric, categorical_high, categorical_low, datetime\n",
    "\n",
    "    def _build_preprocessor(\n",
    "        self, assigned_types: Optional[Dict[str, List[Union[str, int]]]] = None\n",
    "    ) -> Pipeline:\n",
    "        \"\"\"Build default preprocessor.\n",
    "\n",
    "        The preprocessor imputes missing values, scales numeric features and encodes categorical\n",
    "        features according to inferred types.\n",
    "\n",
    "        \"\"\"\n",
    "        X = self.X\n",
    "        if hasattr(self, \"preprocessor\") and not assigned_types:\n",
    "            return self.preprocessor\n",
    "        if assigned_types:\n",
    "            numeric = assigned_types[\"numeric\"]\n",
    "            categorical_high = assigned_types[\"categorical_high\"]\n",
    "            categorical_low = assigned_types[\"categorical_low\"]\n",
    "            datetime = assigned_types[\"datetime\"]\n",
    "        else:\n",
    "            numeric, categorical_high, categorical_low, datetime = self._infer_dtypes()\n",
    "\n",
    "        if isinstance(self.scaler, TransformerMixin):\n",
    "            scaler = self.scaler\n",
    "        elif self.scaler == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "        elif self.scaler == \"minmax\":\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            scaler = RobustScaler()\n",
    "\n",
    "        target_is_multilabel = self.target_info[\"type_\"] in [\n",
    "            \"multilabel-indicator\",\n",
    "            \"multiclass-multioutput\",\n",
    "            \"continuous-multioutput\",\n",
    "        ]\n",
    "        if isinstance(self.high_cardinality_encoder, TransformerMixin):\n",
    "            high_cardinality_encoder = self.high_cardinality_encoder\n",
    "        elif self.high_cardinality_encoder == \"target\":\n",
    "            if target_is_multilabel:\n",
    "                warnings.warn(\n",
    "                    \"TargetEncoder is not supported for multilabel or multioutput targets. \"\n",
    "                    \"Switching to OrdinalEncoder.\",\n",
    "                    stacklevel=2,\n",
    "                )\n",
    "                high_cardinality_encoder = OrdinalEncoder(\n",
    "                    handle_unknown=\"use_encoded_value\", unknown_value=99999\n",
    "                )\n",
    "            else:\n",
    "                high_cardinality_encoder = TargetEncoder(\n",
    "                    task=self.poniard_task, handle_unknown=\"ignore\"\n",
    "                )\n",
    "        else:\n",
    "            high_cardinality_encoder = OrdinalEncoder(\n",
    "                handle_unknown=\"use_encoded_value\", unknown_value=99999\n",
    "            )\n",
    "\n",
    "        cat_date_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "        if isinstance(self.numeric_imputer, TransformerMixin):\n",
    "            num_imputer = self.numeric_imputer\n",
    "        elif self.numeric_imputer == \"iterative\":\n",
    "            from sklearn.experimental import enable_iterative_imputer\n",
    "            from sklearn.impute import IterativeImputer\n",
    "\n",
    "            num_imputer = IterativeImputer(random_state=self.random_state)\n",
    "        else:\n",
    "            num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "        numeric_preprocessor = Pipeline(\n",
    "            [(\"numeric_imputer\", num_imputer), (\"scaler\", scaler)]\n",
    "        )\n",
    "        cat_low_preprocessor = Pipeline(\n",
    "            [\n",
    "                (\"categorical_imputer\", cat_date_imputer),\n",
    "                (\n",
    "                    \"one-hot_encoder\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"if_binary\", handle_unknown=\"ignore\", sparse=False\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        cat_high_preprocessor = Pipeline(\n",
    "            [\n",
    "                (\"categorical_imputer\", cat_date_imputer),\n",
    "                (\n",
    "                    \"high_cardinality_encoder\",\n",
    "                    high_cardinality_encoder,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "        datetime_preprocessor = Pipeline(\n",
    "            [\n",
    "                (\n",
    "                    \"datetime_encoder\",\n",
    "                    DatetimeEncoder(),\n",
    "                ),\n",
    "                (\"datetime_imputer\", cat_date_imputer),\n",
    "            ],\n",
    "        )\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            type_preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    (\"numeric_preprocessor\", numeric_preprocessor, numeric),\n",
    "                    (\n",
    "                        \"categorical_low_preprocessor\",\n",
    "                        cat_low_preprocessor,\n",
    "                        categorical_low,\n",
    "                    ),\n",
    "                    (\n",
    "                        \"categorical_high_preprocessor\",\n",
    "                        cat_high_preprocessor,\n",
    "                        categorical_high,\n",
    "                    ),\n",
    "                    (\"datetime_preprocessor\", datetime_preprocessor, datetime),\n",
    "                ],\n",
    "                n_jobs=self.n_jobs,\n",
    "            )\n",
    "        else:\n",
    "            if np.issubdtype(X.dtype, np.datetime64):\n",
    "                type_preprocessor = datetime_preprocessor\n",
    "            elif np.issubdtype(X.dtype, np.number):\n",
    "                type_preprocessor = ColumnTransformer(\n",
    "                    [\n",
    "                        (\"numeric_preprocessor\", numeric_preprocessor, numeric),\n",
    "                        (\n",
    "                            \"categorical_low_preprocessor\",\n",
    "                            cat_low_preprocessor,\n",
    "                            categorical_low,\n",
    "                        ),\n",
    "                        (\n",
    "                            \"categorical_high_preprocessor\",\n",
    "                            cat_high_preprocessor,\n",
    "                            categorical_high,\n",
    "                        ),\n",
    "                    ],\n",
    "                    n_jobs=self.n_jobs,\n",
    "                )\n",
    "            else:\n",
    "                type_preprocessor = ColumnTransformer(\n",
    "                    [\n",
    "                        (\n",
    "                            \"categorical_low_preprocessor\",\n",
    "                            cat_low_preprocessor,\n",
    "                            categorical_low,\n",
    "                        ),\n",
    "                        (\n",
    "                            \"categorical_high_preprocessor\",\n",
    "                            cat_high_preprocessor,\n",
    "                            categorical_high,\n",
    "                        ),\n",
    "                    ],\n",
    "                    n_jobs=self.n_jobs,\n",
    "                )\n",
    "        # Some transformers might not be applied to any features, so we remove them.\n",
    "        non_empty_transformers = [\n",
    "            x for x in type_preprocessor.transformers if x[2] != []\n",
    "        ]\n",
    "        type_preprocessor.transformers = non_empty_transformers\n",
    "        # If type_preprocessor has a single transformer, use the transformer directly.\n",
    "        # This transformer generally is a Pipeline.\n",
    "        if len(type_preprocessor.transformers) == 1:\n",
    "            type_preprocessor = type_preprocessor.transformers[0][1]\n",
    "        preprocessor = Pipeline(\n",
    "            [\n",
    "                (\"type_preprocessor\", type_preprocessor),\n",
    "                (\"remove_invariant\", VarianceThreshold()),\n",
    "            ],\n",
    "            memory=self._memory,\n",
    "        )\n",
    "        return preprocessor\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _default_estimators(self) -> List[ClassifierMixin]:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def estimators_(self):\n",
    "        warnings.warn(\n",
    "            \"'estimators_' has been renamed to 'pipelines'\",\n",
    "            DeprecationWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        return self.pipelines\n",
    "\n",
    "    @property\n",
    "    def preprocessor_(self):\n",
    "        warnings.warn(\n",
    "            \"'preprocessor_' has been renamed to 'preprocessor'\",\n",
    "            DeprecationWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        return self.preprocessor\n",
    "\n",
    "    @property\n",
    "    def metrics_(self):\n",
    "        warnings.warn(\n",
    "            \"'metrics_' has been renamed to 'metrics'\", DeprecationWarning, stacklevel=2\n",
    "        )\n",
    "        return self.metrics\n",
    "\n",
    "    @property\n",
    "    def cv_(self):\n",
    "        warnings.warn(\n",
    "            \"'cv_' has been renamed to 'cv'\", DeprecationWarning, stacklevel=2\n",
    "        )\n",
    "        return self.cv\n",
    "\n",
    "    def show_results(\n",
    "        self,\n",
    "        std: bool = False,\n",
    "        wrt_dummy: bool = False,\n",
    "    ):\n",
    "        warnings.warn(\n",
    "            \"'show_results' has been renamed to 'get_results'\",\n",
    "            DeprecationWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        return self.get_results(return_train_scores=False, std=std, wrt_dummy=wrt_dummy)\n",
    "\n",
    "    def _build_pipelines(\n",
    "        self,\n",
    "    ) -> Dict[str, Union[ClassifierMixin, RegressorMixin]]:\n",
    "        \"\"\"Build :attr:`pipelines` dict where keys are the estimator class names.\n",
    "\n",
    "        Adds dummy estimators if not included during construction. Does nothing if\n",
    "        :attr:`pipelines` exists.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(self.estimators, dict):\n",
    "            estimators = self.estimators.copy()\n",
    "        elif self.estimators:\n",
    "            estimators = {\n",
    "                estimator.__class__.__name__: estimator for estimator in self.estimators\n",
    "            }\n",
    "        else:\n",
    "            estimators = {\n",
    "                estimator.__class__.__name__: estimator\n",
    "                for estimator in self._default_estimators\n",
    "            }\n",
    "        estimators = self._add_dummy_estimators(estimators)\n",
    "\n",
    "        for estimator in estimators.values():\n",
    "            self._pass_instance_attrs(estimator)\n",
    "\n",
    "        pipelines = {}\n",
    "        if self.preprocess:\n",
    "            pipelines.update(\n",
    "                {\n",
    "                    name: Pipeline(\n",
    "                        [(\"preprocessor\", self.preprocessor), (name, estimator)],\n",
    "                        memory=self._memory,\n",
    "                    )\n",
    "                    for name, estimator in estimators.items()\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            pipelines.update(\n",
    "                {\n",
    "                    name: Pipeline([(name, estimator)])\n",
    "                    for name, estimator in estimators.items()\n",
    "                }\n",
    "            )\n",
    "        self._fitted_pipeline_ids = []\n",
    "        return pipelines\n",
    "\n",
    "    def _add_dummy_estimators(self, estimators: dict):\n",
    "        if (\n",
    "            \"DummyClassifier\" in estimators.keys()\n",
    "            or \"DummyRegressor\" in estimators.keys()\n",
    "        ):\n",
    "            return estimators\n",
    "        if self.poniard_task == \"classification\":\n",
    "            estimators.update({\"DummyClassifier\": DummyClassifier(strategy=\"prior\")})\n",
    "        elif self.poniard_task == \"regression\":\n",
    "            estimators.update({\"DummyRegressor\": DummyRegressor(strategy=\"mean\")})\n",
    "        return estimators\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_metrics(self) -> Union[Dict[str, Callable], List[str]]:\n",
    "        \"\"\"Build metrics.\"\"\"\n",
    "        return [\"accuracy\"]\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_cv(self):\n",
    "        return self.cv\n",
    "\n",
    "    def fit(self) -> PoniardBaseEstimator:\n",
    "        \"\"\"This is the main Poniard method. It uses scikit-learn's `cross_validate` function to\n",
    "        score all `metrics` for every `pipelines`, using `cv` for cross validation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X :\n",
    "            Features.\n",
    "        y :\n",
    "            Target.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            Self.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"cv\"):\n",
    "            raise ValueError(\"`setup` must be called before `fit`.\")\n",
    "        self._run_plugin_method(\"on_fit_start\")\n",
    "\n",
    "        results = {}\n",
    "        filtered_pipelines = {\n",
    "            name: pipeline\n",
    "            for name, pipeline in self.pipelines.items()\n",
    "            if id(pipeline) not in self._fitted_pipeline_ids\n",
    "        }\n",
    "        pbar = tqdm(filtered_pipelines.items())\n",
    "        for i, (name, pipeline) in enumerate(pbar):\n",
    "            pbar.set_description(f\"{name}\")\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "                warnings.filterwarnings(\n",
    "                    \"ignore\", message=\".*will be encoded as all zeros\"\n",
    "                )\n",
    "                result = cross_validate(\n",
    "                    pipeline,\n",
    "                    self.X,\n",
    "                    self.y,\n",
    "                    scoring=self.metrics,\n",
    "                    cv=self.cv,\n",
    "                    return_train_score=True,\n",
    "                    verbose=self.verbose,\n",
    "                    n_jobs=self.n_jobs,\n",
    "                )\n",
    "            results.update({name: result})\n",
    "            self._fitted_pipeline_ids.append(id(pipeline))\n",
    "            if i == len(pbar) - 1:\n",
    "                pbar.set_description(\"Completed\")\n",
    "        if hasattr(self, \"_experiment_results\"):\n",
    "            self._experiment_results.update(results)\n",
    "        else:\n",
    "            self._experiment_results = results\n",
    "\n",
    "        self._process_results()\n",
    "        self._process_long_results()\n",
    "        self._run_plugin_method(\"on_fit_end\")\n",
    "        return self\n",
    "\n",
    "    def _predict(\n",
    "        self, method: str, estimator_names: Optional[Sequence[str]] = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Helper method for predicting targets or target probabilities with cross validation.\n",
    "        Accepts predict, predict_proba, predict_log_proba or decision_function.\"\"\"\n",
    "        if not hasattr(self, \"cv\"):\n",
    "            raise ValueError(\"`setup` must be called before `predict`.\")\n",
    "        X, y = self.X, self.y\n",
    "        estimator_names = element_to_list_maybe(estimator_names)\n",
    "        if not estimator_names:\n",
    "            estimator_names = [estimator for estimator in self.pipelines.keys()]\n",
    "        results = {}\n",
    "        pbar = tqdm(estimator_names)\n",
    "        for i, name in enumerate(pbar):\n",
    "            pbar.set_description(f\"{name}\")\n",
    "            pipeline = self.pipelines[name]\n",
    "            try:\n",
    "                result = cross_val_predict(\n",
    "                    pipeline,\n",
    "                    X,\n",
    "                    y,\n",
    "                    cv=self.cv,\n",
    "                    method=method,\n",
    "                    verbose=self.verbose,\n",
    "                    n_jobs=self.n_jobs,\n",
    "                )\n",
    "            except AttributeError:\n",
    "                warnings.warn(\n",
    "                    f\"{name} does not support `{method}` method. Filling with nan.\",\n",
    "                    stacklevel=2,\n",
    "                )\n",
    "                result = np.empty(self.y.shape)\n",
    "                result[:] = np.nan\n",
    "            results.update({name: result})\n",
    "\n",
    "            if not hasattr(self, \"_experiment_results\"):\n",
    "                self._experiment_results = {}\n",
    "                self._experiment_results.update({name: {method: result}})\n",
    "            elif name not in self._experiment_results:\n",
    "                self._experiment_results.update({name: {method: result}})\n",
    "            else:\n",
    "                self._experiment_results[name][method] = result\n",
    "\n",
    "            if i == len(pbar) - 1:\n",
    "                pbar.set_description(\"Completed\")\n",
    "        return results\n",
    "\n",
    "    def predict(\n",
    "        self, estimator_names: Optional[Sequence[str]] = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Get cross validated target predictions where each sample belongs to a single test set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator_names :\n",
    "            Estimators to include. If None, predict all estimators.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict where keys are estimator names and values are numpy arrays of predictions.\n",
    "        \"\"\"\n",
    "        return self._predict(method=\"predict\", estimator_names=estimator_names)\n",
    "\n",
    "    def predict_proba(\n",
    "        self, estimator_names: Optional[Sequence[str]] = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Get cross validated target probability predictions where each sample belongs to a\n",
    "        single test set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict where keys are estimator names and values are numpy arrays of prediction\n",
    "            probabilities.\n",
    "        \"\"\"\n",
    "        return self._predict(method=\"predict_proba\", estimator_names=estimator_names)\n",
    "\n",
    "    def decision_function(\n",
    "        self, estimator_names: Optional[Sequence[str]] = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Get cross validated decision function predictions where each sample belongs to a\n",
    "        single test set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator_names :\n",
    "            Estimators to include. If None, predict all estimators.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict where keys are estimator names and values are numpy arrays of prediction\n",
    "            probabilities.\n",
    "        \"\"\"\n",
    "        return self._predict(\n",
    "            method=\"decision_function\", estimator_names=estimator_names\n",
    "        )\n",
    "\n",
    "    def predict_all(\n",
    "        self, estimator_names: Optional[Sequence[str]] = None\n",
    "    ) -> Tuple[Dict[str, np.ndarray]]:\n",
    "        \"\"\"Get cross validated target predictions, probabilities and decision functions\n",
    "        where each sample belongs to all test sets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator_names :\n",
    "            Estimators to include. If None, predict all estimators.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            Dict where keys are estimator names and values are numpy arrays of prediction\n",
    "            probabilities.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self._predict(method=\"predict\", estimator_names=estimator_names),\n",
    "            self._predict(method=\"predict_proba\", estimator_names=estimator_names),\n",
    "            self._predict(method=\"decision_function\", estimator_names=estimator_names),\n",
    "        )\n",
    "\n",
    "    def reassign_types(\n",
    "        self,\n",
    "        numeric: Optional[List[Union[str, int]]] = None,\n",
    "        categorical_high: Optional[List[Union[str, int]]] = None,\n",
    "        categorical_low: Optional[List[Union[str, int]]] = None,\n",
    "        datetime: Optional[List[Union[str, int]]] = None,\n",
    "    ) -> PoniardBaseEstimator:\n",
    "        \"\"\"Reassign feature types.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        numeric :\n",
    "            List of column names or indices. Default None.\n",
    "        categorical_high :\n",
    "            List of column names or indices. Default None.\n",
    "        categorical_low :\n",
    "            List of column names or indices. Default None.\n",
    "        datetime :\n",
    "            List of column names or indices. Default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            self.\n",
    "        \"\"\"\n",
    "        assigned_types = {\n",
    "            \"numeric\": numeric or [],\n",
    "            \"categorical_high\": categorical_high or [],\n",
    "            \"categorical_low\": categorical_low or [],\n",
    "            \"datetime\": datetime or [],\n",
    "        }\n",
    "        self._inferred_types = assigned_types\n",
    "        print(\"Assigned feature types\", \"----------------------\", sep=\"\\n\")\n",
    "        assigned_types_df = pd.DataFrame.from_dict(\n",
    "            self._inferred_types, orient=\"index\"\n",
    "        ).T.fillna(\"\")\n",
    "        try:\n",
    "            # Try to print the table nicely\n",
    "            from IPython.display import display, HTML\n",
    "\n",
    "            display(HTML(assigned_types_df.to_html()))\n",
    "            print(\"\\n\")\n",
    "        except ImportError:\n",
    "            print(assigned_types_df)\n",
    "        # Don't build the preprocessor if no preprocessing should be done or a\n",
    "        # custom preprocessor was set.\n",
    "        if not self.preprocess or self.custom_preprocessor is not None:\n",
    "            return self\n",
    "        self.preprocessor = self._build_preprocessor(assigned_types=assigned_types)\n",
    "        self._run_plugin_method(\"on_reassign_types\")\n",
    "        self.pipelines = self._build_pipelines()\n",
    "        return self\n",
    "\n",
    "    def add_preprocessing_step(\n",
    "        self,\n",
    "        step: Union[\n",
    "            Union[Pipeline, TransformerMixin, ColumnTransformer],\n",
    "            Tuple[str, Union[Pipeline, TransformerMixin, ColumnTransformer]],\n",
    "        ],\n",
    "        position: Union[str, int] = \"end\",\n",
    "    ) -> Pipeline:\n",
    "        \"\"\"Add a preprocessing step to :attr:`preprocessor`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        step :\n",
    "            A tuple of (str, transformer) or a scikit-learn transformer. Note that\n",
    "            the transformer can also be a Pipeline or ColumnTransformer.\n",
    "        position :\n",
    "            Either an integer denoting before which step in the existing preprocessing pipeline\n",
    "            the new step should be added, or 'start' or 'end'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            self\n",
    "        \"\"\"\n",
    "        if not isinstance(position, int) and position not in [\"start\", \"end\"]:\n",
    "            raise ValueError(\"`position` can only be int, 'start' or 'end'.\")\n",
    "        existing_preprocessor = self.preprocessor\n",
    "        if not isinstance(step, Tuple):\n",
    "            step = (f\"step_{step.__class__.__name__.lower()}\", step)\n",
    "        if isinstance(position, str) and isinstance(existing_preprocessor, Pipeline):\n",
    "            if position == \"start\":\n",
    "                position = 0\n",
    "            elif position == \"end\":\n",
    "                position = len(existing_preprocessor.steps)\n",
    "        if isinstance(existing_preprocessor, Pipeline):\n",
    "            existing_preprocessor.steps.insert(position, step)\n",
    "        else:\n",
    "            if isinstance(position, int):\n",
    "                raise ValueError(\n",
    "                    \"If the existing preprocessor is not a Pipeline, only 'start' and \"\n",
    "                    \"'end' are accepted as `position`.\"\n",
    "                )\n",
    "            if position == \"start\":\n",
    "                self.preprocessor = Pipeline(\n",
    "                    [step, (\"initial_preprocessor\", self.preprocessor)],\n",
    "                    memory=self._memory,\n",
    "                )\n",
    "            else:\n",
    "                self.preprocessor = Pipeline(\n",
    "                    [(\"initial_preprocessor\", self.preprocessor), step],\n",
    "                    memory=self._memory,\n",
    "                )\n",
    "        self.pipelines = self._build_pipelines()\n",
    "        self._run_plugin_method(\"on_add_preprocessing_step\")\n",
    "        return self\n",
    "\n",
    "    def get_results(\n",
    "        self,\n",
    "        return_train_scores: bool = False,\n",
    "        std: bool = False,\n",
    "        wrt_dummy: bool = False,\n",
    "    ) -> Union[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]:\n",
    "        \"\"\"Return dataframe containing scoring results. By default returns the mean score and fit\n",
    "        and score times. Optionally returns standard deviations as well.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        return_train_scores :\n",
    "            If False, only return test scores.\n",
    "        std :\n",
    "            Whether to return standard deviation of the scores. Default False.\n",
    "        wrt_dummy :\n",
    "            Whether to compute each score/time with respect to the dummy estimator results. Default\n",
    "            False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Union[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]\n",
    "            Results\n",
    "        \"\"\"\n",
    "        means = self._means\n",
    "        stds = self._stds\n",
    "        if not return_train_scores:\n",
    "            means = means.loc[\n",
    "                :, means.columns.str.contains(\"test_|fit|score\", regex=True)\n",
    "            ]\n",
    "            stds = stds.loc[:, stds.columns.str.contains(\"test_|fit|score\", regex=True)]\n",
    "        if wrt_dummy:\n",
    "            dummy_means = means.loc[means.index.str.contains(\"Dummy\")]\n",
    "            dummy_stds = stds.loc[stds.index.str.contains(\"Dummy\")]\n",
    "            means = means / dummy_means.squeeze()\n",
    "            stds = stds / dummy_stds.squeeze()\n",
    "        if std:\n",
    "            return means, stds\n",
    "        else:\n",
    "            return means\n",
    "\n",
    "    def add_estimators(\n",
    "        self, estimators: Union[Dict[str, ClassifierMixin], Sequence[ClassifierMixin]]\n",
    "    ) -> PoniardBaseEstimator:\n",
    "        \"\"\"Include new estimator. This is the recommended way of adding an estimator (as opposed\n",
    "        to modifying :attr:`pipelines` directly), since it also injects random state, n_jobs\n",
    "        and verbosity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimators :\n",
    "            Estimators to add.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            Self.\n",
    "\n",
    "        \"\"\"\n",
    "        estimators = element_to_list_maybe(estimators)\n",
    "        if not isinstance(estimators, dict):\n",
    "            new_estimators = {\n",
    "                estimator.__class__.__name__: estimator for estimator in estimators\n",
    "            }\n",
    "        else:\n",
    "            new_estimators = estimators\n",
    "        for new_estimator in new_estimators.values():\n",
    "            self._pass_instance_attrs(new_estimator)\n",
    "        self.pipelines.update(new_estimators)\n",
    "        self._run_plugin_method(\"on_add_estimators\")\n",
    "        return self\n",
    "\n",
    "    def remove_estimators(\n",
    "        self, estimator_names: Sequence[str], drop_results: bool = True\n",
    "    ) -> PoniardBaseEstimator:\n",
    "        \"\"\"Remove estimators. This is the recommended way of removing an estimator (as opposed\n",
    "        to modifying :attr:`pipelines` directly), since it also removes the associated rows from\n",
    "        the results tables.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator_names :\n",
    "            Estimators to remove.\n",
    "        drop_results :\n",
    "            Whether to remove the results associated with the estimators. Default True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            Self.\n",
    "        \"\"\"\n",
    "        estimator_names = element_to_list_maybe(estimator_names)\n",
    "        pruned_estimators = {\n",
    "            k: v for k, v in self.pipelines.items() if k not in estimator_names\n",
    "        }\n",
    "        if len(pruned_estimators) == 0:\n",
    "            raise ValueError(\"Cannot remove all estimators.\")\n",
    "        self.pipelines = pruned_estimators\n",
    "        if drop_results and hasattr(self, \"_means\"):\n",
    "            self._means = self._means.loc[~self._means.index.isin(estimator_names)]\n",
    "            self._stds = self._stds.loc[~self._stds.index.isin(estimator_names)]\n",
    "            self._experiment_results = {\n",
    "                k: v\n",
    "                for k, v in self._experiment_results.items()\n",
    "                if k not in estimator_names\n",
    "            }\n",
    "            self._process_long_results()\n",
    "        self._run_plugin_method(\"on_remove_estimators\")\n",
    "        return self\n",
    "\n",
    "    def get_estimator(\n",
    "        self,\n",
    "        estimator_name: str,\n",
    "        include_preprocessor: bool = True,\n",
    "        retrain: bool = False,\n",
    "    ) -> Union[Pipeline, ClassifierMixin, RegressorMixin]:\n",
    "        \"\"\"Obtain an estimator in :attr:`pipelines` by name. This is useful for extracting default\n",
    "        estimators or hyperparmeter-optimized estimators (after using :meth:`tune_estimator`).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator_name :\n",
    "            Estimator name.\n",
    "        include_preprocessor :\n",
    "            Whether to return a pipeline with a preprocessor or just the estimator. Default True.\n",
    "        retrain :\n",
    "            Whether to retrain with full data. Default False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ClassifierMixin\n",
    "            Estimator.\n",
    "        \"\"\"\n",
    "        model = self.pipelines[estimator_name]\n",
    "        if not include_preprocessor:\n",
    "            model = model._final_estimator\n",
    "        model = clone(model)\n",
    "        if retrain:\n",
    "            model.fit(self.X, self.y)\n",
    "        self._run_plugin_method(\n",
    "            \"on_get_estimator\", estimator=model, name=estimator_name\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def build_ensemble(\n",
    "        self,\n",
    "        method: str = \"stacking\",\n",
    "        estimator_names: Optional[Sequence[str]] = None,\n",
    "        top_n: Optional[int] = 3,\n",
    "        sort_by: Optional[str] = None,\n",
    "        ensemble_name: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> PoniardBaseEstimator:\n",
    "        \"\"\"Combine estimators into an ensemble.\n",
    "\n",
    "        By default, orders estimators according to the first metric.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method :\n",
    "            Ensemble method. Either \"stacking\" or \"voring\". Default \"stacking\".\n",
    "        estimator_names :\n",
    "            Names of estimators to include. Default None, which uses `top_n`\n",
    "        top_n :\n",
    "            How many of the best estimators to include.\n",
    "        sort_by :\n",
    "            Which metric to consider for ordering results. Default None, which uses the first metric.\n",
    "        ensemble_name :\n",
    "            Ensemble name when adding to :attr:`pipelines`. Default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            Self.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `method` is not \"stacking\" or \"voting\".\n",
    "        \"\"\"\n",
    "        if method not in [\"voting\", \"stacking\"]:\n",
    "            raise ValueError(\"Method must be either voting or stacking.\")\n",
    "        estimator_names = element_to_list_maybe(estimator_names)\n",
    "        if estimator_names:\n",
    "            models = [\n",
    "                (name, self.pipelines[name]._final_estimator)\n",
    "                for name in estimator_names\n",
    "            ]\n",
    "        else:\n",
    "            if sort_by:\n",
    "                sorter = sort_by\n",
    "            else:\n",
    "                sorter = self._means.columns[0]\n",
    "            models = [\n",
    "                (name, self.pipelines[name]._final_estimator)\n",
    "                for name in self._means.sort_values(sorter, ascending=False).index[\n",
    "                    :top_n\n",
    "                ]\n",
    "            ]\n",
    "        if method == \"voting\":\n",
    "            if self.poniard_task == \"classification\":\n",
    "                ensemble = VotingClassifier(\n",
    "                    estimators=models, verbose=self.verbose, **kwargs\n",
    "                )\n",
    "            else:\n",
    "                ensemble = VotingRegressor(\n",
    "                    estimators=models, verbose=self.verbose, **kwargs\n",
    "                )\n",
    "        else:\n",
    "            if self.poniard_task == \"classification\":\n",
    "                ensemble = StackingClassifier(\n",
    "                    estimators=models, verbose=self.verbose, cv=self.cv, **kwargs\n",
    "                )\n",
    "            else:\n",
    "                ensemble = StackingRegressor(\n",
    "                    estimators=models, verbose=self.verbose, cv=self.cv, **kwargs\n",
    "                )\n",
    "        ensemble_name = ensemble_name or ensemble.__class__.__name__\n",
    "        self.add_estimators(estimators={ensemble_name: ensemble})\n",
    "        return self\n",
    "\n",
    "    def get_predictions_similarity(\n",
    "        self,\n",
    "        on_errors: bool = True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Compute correlation/association between cross validated predictions for each estimator.\n",
    "\n",
    "        This can be useful for ensembling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        on_errors :\n",
    "            Whether to compute similarity on prediction errors instead of predictions. Default\n",
    "            True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Similarity.\n",
    "        \"\"\"\n",
    "        if self.y.ndim > 1:\n",
    "            raise ValueError(\"y must be a 1-dimensional array.\")\n",
    "        raw_results = self.predict()\n",
    "        results = raw_results.copy()\n",
    "        for name, result in raw_results.items():\n",
    "            if on_errors:\n",
    "                if self.poniard_task == \"regression\":\n",
    "                    results[name] = self.y - result\n",
    "                else:\n",
    "                    results[name] = np.where(result == self.y, 1, 0)\n",
    "        results = pd.DataFrame(results)\n",
    "        if self.poniard_task == \"classification\":\n",
    "            estimator_names = [x for x in results.columns if x != \"DummyClassifier\"]\n",
    "            table = pd.DataFrame(\n",
    "                data=np.nan, index=estimator_names, columns=estimator_names\n",
    "            )\n",
    "            for row, col in itertools.combinations_with_replacement(\n",
    "                table.index[::-1], 2\n",
    "            ):\n",
    "                cramer = cramers_v(results[row], results[col])\n",
    "                if row == col:\n",
    "                    table.loc[row, col] = 1\n",
    "                else:\n",
    "                    table.loc[row, col] = cramer\n",
    "                    table.loc[col, row] = cramer\n",
    "        else:\n",
    "            table = results.drop(\"DummyRegressor\", axis=1).corr()\n",
    "        return table\n",
    "\n",
    "    def tune_estimator(\n",
    "        self,\n",
    "        estimator_name: str,\n",
    "        grid: Optional[Dict] = None,\n",
    "        mode: str = \"grid\",\n",
    "        tuned_estimator_name: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[GridSearchCV, RandomizedSearchCV]:\n",
    "        \"\"\"Hyperparameter tuning for a single estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator_name :\n",
    "            Estimator to tune.\n",
    "        grid :\n",
    "            Hyperparameter grid. Default None, which uses the grids available for default\n",
    "            estimators.\n",
    "        mode :\n",
    "            Type of search. Eithe \"grid\", \"halving\" or \"random\". Default \"grid\".\n",
    "        tuned_estimator_name :\n",
    "            Estimator name when adding to :attr:`pipelines`. Default None.\n",
    "        kwargs :\n",
    "            Passed to the search instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            Self.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        KeyError\n",
    "            If no grid is defined and the estimator is not a default one.\n",
    "        \"\"\"\n",
    "        X, y = self.X, self.y\n",
    "        estimator = clone(self.pipelines[estimator_name])\n",
    "        if not grid:\n",
    "            try:\n",
    "                grid = GRID[estimator_name]\n",
    "                grid = {f\"{estimator_name}__{k}\": v for k, v in grid.items()}\n",
    "            except KeyError:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Estimator {estimator_name} has no predefined hyperparameter grid, so it has to be supplied.\"\n",
    "                )\n",
    "        self._pass_instance_attrs(estimator)\n",
    "\n",
    "        scoring = self._first_scorer(sklearn_scorer=True)\n",
    "        if mode == \"random\":\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator,\n",
    "                grid,\n",
    "                scoring=scoring,\n",
    "                cv=self.cv,\n",
    "                verbose=self.verbose,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "                **kwargs,\n",
    "            )\n",
    "        elif mode == \"halving\":\n",
    "            from sklearn.experimental import enable_halving_search_cv\n",
    "            from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "            search = HalvingGridSearchCV(\n",
    "                estimator,\n",
    "                grid,\n",
    "                scoring=scoring,\n",
    "                cv=self.cv,\n",
    "                verbose=self.verbose,\n",
    "                n_jobs=self.n_jobs,\n",
    "                random_state=self.random_state,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            search = GridSearchCV(\n",
    "                estimator,\n",
    "                grid,\n",
    "                scoring=scoring,\n",
    "                cv=self.cv,\n",
    "                verbose=self.verbose,\n",
    "                n_jobs=self.n_jobs,\n",
    "                **kwargs,\n",
    "            )\n",
    "        search.fit(X, y)\n",
    "        tuned_estimator_name = tuned_estimator_name or f\"{estimator_name}_tuned\"\n",
    "        self.add_estimators(\n",
    "            estimators={\n",
    "                tuned_estimator_name: clone(search.best_estimator_._final_estimator)\n",
    "            }\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def _process_results(self) -> None:\n",
    "        \"\"\"Compute mean and standard deviations of  experiment results.\"\"\"\n",
    "        # TODO: This processes every result, even those that were processed\n",
    "        # in previous runs (before add_estimators). Should be made more efficient\n",
    "        results = pd.DataFrame(self._experiment_results).T\n",
    "        results = results.loc[\n",
    "            :,\n",
    "            [\n",
    "                x\n",
    "                for x in results.columns\n",
    "                if x not in [\"predict\", \"predict_proba\", \"decision_function\"]\n",
    "            ],\n",
    "        ]\n",
    "        means = results.apply(lambda x: np.mean(x.values.tolist(), axis=1))\n",
    "        stds = results.apply(lambda x: np.std(x.values.tolist(), axis=1))\n",
    "        means = means[list(means.columns[2:]) + [\"fit_time\", \"score_time\"]]\n",
    "        stds = stds[list(stds.columns[2:]) + [\"fit_time\", \"score_time\"]]\n",
    "        self._means = means.sort_values(means.columns[0], ascending=False)\n",
    "        self._stds = stds.reindex(self._means.index)\n",
    "        return\n",
    "\n",
    "    def _process_long_results(self) -> None:\n",
    "        \"\"\"Prepare experiment results for plotting.\"\"\"\n",
    "        base = pd.DataFrame(self._experiment_results).T\n",
    "        melted = (\n",
    "            base.rename_axis(\"Model\")\n",
    "            .reset_index()\n",
    "            .melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "            .explode(\"Score\")\n",
    "        )\n",
    "        melted[\"Type\"] = \"Fold\"\n",
    "        means = melted.groupby([\"Model\", \"Metric\"])[\"Score\"].mean().reset_index()\n",
    "        means[\"Type\"] = \"Mean\"\n",
    "        melted = pd.concat([melted, means])\n",
    "        melted[\"Model\"] = melted[\"Model\"].str.replace(\n",
    "            \"Classifier|Regressor\", \"\", regex=True\n",
    "        )\n",
    "\n",
    "        self._long_results = melted\n",
    "        return\n",
    "\n",
    "    def _first_scorer(self, sklearn_scorer: bool) -> Union[str, Callable]:\n",
    "        \"\"\"Helper method to get the first scoring function or name.\"\"\"\n",
    "        if isinstance(self.metrics, Sequence):\n",
    "            return self.metrics[0]\n",
    "        elif isinstance(self.metrics, dict):\n",
    "            if sklearn_scorer:\n",
    "                return list(self.metrics.values())[0]\n",
    "            else:\n",
    "                return list(self.metrics.keys())[0]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"self.metrics can only be a sequence of str or dict of str: callable.\"\n",
    "            )\n",
    "\n",
    "    def _train_test_split_from_cv(self):\n",
    "        \"\"\"Split data in a 80/20 fashion following the cross-validation strategy defined in the constructor.\"\"\"\n",
    "        if isinstance(self.cv, (int, Iterable)):\n",
    "            cv_params_for_split = {}\n",
    "        else:\n",
    "            cv_params_for_split = {\n",
    "                k: v\n",
    "                for k, v in vars(self.cv).items()\n",
    "                if k in [\"shuffle\", \"random_state\"]\n",
    "            }\n",
    "            stratify = self.y if \"Stratified\" in self.cv.__class__.__name__ else None\n",
    "            cv_params_for_split.update({\"stratify\": stratify})\n",
    "        return train_test_split(self.X, self.y, test_size=0.2, **cv_params_for_split)\n",
    "\n",
    "    def _pass_instance_attrs(self, obj: Union[ClassifierMixin, RegressorMixin]):\n",
    "        \"\"\"Helper method to propagate instance attributes to objects.\"\"\"\n",
    "        for attr, value in zip(\n",
    "            [\"random_state\", \"verbose\", \"verbosity\"],\n",
    "            [self.random_state, self.verbose, self.verbose],\n",
    "        ):\n",
    "            if hasattr(obj, attr):\n",
    "                setattr(obj, attr, value)\n",
    "        return\n",
    "\n",
    "    def _run_plugin_method(self, method: str, **kwargs):\n",
    "        \"\"\"Helper method to run plugin methods by name.\"\"\"\n",
    "        if not self.plugins:\n",
    "            return\n",
    "        for plugin in self.plugins:\n",
    "            fetched_method = getattr(plugin, method, None)\n",
    "            if callable(fetched_method):\n",
    "                accepted_kwargs = inspect.getargs(fetched_method.__code__).args\n",
    "                matched_kwargs = {\n",
    "                    k: v for k, v in kwargs.items() if k in accepted_kwargs\n",
    "                }\n",
    "                fetched_method(**matched_kwargs)\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"{self.__class__.__name__}(estimators={self.estimators}, metrics={self.metrics},\n",
    "    preprocess={self.preprocess}, scaler={self.scaler}, numeric_imputer={self.numeric_imputer},\n",
    "    custom_preprocessor={self.custom_preprocessor}, numeric_threshold={self.numeric_threshold},\n",
    "    cardinality_threshold={self.cardinality_threshold}, cv={self.cv}, verbose={self.verbose},\n",
    "    random_state={self.random_state}, n_jobs={self.n_jobs}, plugins={self.plugins},\n",
    "    plot_options={str(self.plot_options)})\n",
    "            \"\"\"\n",
    "\n",
    "    def __add__(\n",
    "        self,\n",
    "        estimators: Union[\n",
    "            Dict[str, Union[ClassifierMixin, RegressorMixin]],\n",
    "            Sequence[Union[ClassifierMixin, RegressorMixin]],\n",
    "        ],\n",
    "    ) -> PoniardBaseEstimator:\n",
    "        \"\"\"Add estimators to a Poniard Estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimators :\n",
    "            List or dict of estimators to add.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            Self.\n",
    "        \"\"\"\n",
    "        estimators = element_to_list_maybe(estimators)\n",
    "        return self.add_estimators(estimators)\n",
    "\n",
    "    def __sub__(self, estimator_names: Sequence[str]) -> PoniardBaseEstimator:\n",
    "        \"\"\"Remove an estimator and its results.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator :\n",
    "            List of estimators names.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        PoniardBaseEstimator\n",
    "            Self.\n",
    "        \"\"\"\n",
    "        estimator_names = element_to_list_maybe(estimator_names)\n",
    "        return self.remove_estimators(estimator_names, drop_results=True)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, estimator_name: str\n",
    "    ) -> Union[Pipeline, ClassifierMixin, RegressorMixin]:\n",
    "        \"\"\"Get an estimator by indexing with its name\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator_name :\n",
    "            Estimator name as string.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Union[Pipeline, ClassifierMixin, RegressorMixin]\n",
    "            Built estimator.\n",
    "        \"\"\"\n",
    "        return self.get_estimator(estimator_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `estimators`, `metrics` and `cv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniard estimators' main parameters can be grouped in the following way:\n",
    "\n",
    "1. Estimators.\n",
    "2. Preprocessing parameters.\n",
    "    * Imputers\n",
    "    * Numeric scaler\n",
    "    * Categorical encoder\n",
    "    * Custom preprocessor\n",
    "3. Metrics.\n",
    "4. Cross validation strategy.\n",
    "5. Rest.\n",
    "\n",
    "These give a good amount of flexibility while providing sane defaults, so that after initialization only `setup` and `fit` have to be called in order to train multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`estimators` takes a scikit-learn-compatible estimator, array of estimators or dict of *name: estimators*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poniard import PoniardRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoniardRegressor(estimators=[LinearRegression(), Ridge()], metrics=None,\n",
       "    preprocess=True, scaler=standard, numeric_imputer=simple,\n",
       "    custom_preprocessor=None, numeric_threshold=0.1,\n",
       "    cardinality_threshold=20, cv=None, verbose=0,\n",
       "    random_state=0, n_jobs=None, plugins=None,\n",
       "    plot_options=PoniardPlotFactory())\n",
       "            "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [LinearRegression(), Ridge()]\n",
    "PoniardRegressor(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `setup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`setup` takes features and target as parameters, while `fit` does not accept any. This runs contrary to the established convention defined by scikit-learn where there is no setting up to do and `fit` takes the data as params.\n",
    "\n",
    "This is because Poniard does not only fit the models, but also infer features types and create the `preprocesor` based on these types. While this could all be stuffed inside `fit` (that was the case initially), having it separated allows the user to check whether Poniard's assumptions are correct and adjust if needed before running `fit`, which can take long depending on how many models were passed to `estimators`, the cross validation strategy and the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PoniardBaseEstimator.setup\n",
       "\n",
       ">      PoniardBaseEstimator.setup\n",
       ">                                  (X:Union[pandas.core.frame.DataFrame,numpy.nd\n",
       ">                                  array,List], y:Union[pandas.core.frame.DataFr\n",
       ">                                  ame,numpy.ndarray,List])\n",
       "\n",
       "Orchestrator.\n",
       "\n",
       "Converts inputs to arrays if necessary, sets `metrics`,\n",
       "`preprocessor`, `cv` and `pipelines`.\n",
       "\n",
       "After running `setup`, both `X` and `y` will be held as attributes.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | Union[pd.DataFrame, np.ndarray, List] | Features. |\n",
       "| y | Union[pd.DataFrame, np.ndarray, List] | Target |\n",
       "| **Returns** | **PoniardBaseEstimator** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PoniardBaseEstimator.setup\n",
       "\n",
       ">      PoniardBaseEstimator.setup\n",
       ">                                  (X:Union[pandas.core.frame.DataFrame,numpy.nd\n",
       ">                                  array,List], y:Union[pandas.core.frame.DataFr\n",
       ">                                  ame,numpy.ndarray,List])\n",
       "\n",
       "Orchestrator.\n",
       "\n",
       "Converts inputs to arrays if necessary, sets `metrics`,\n",
       "`preprocessor`, `cv` and `pipelines`.\n",
       "\n",
       "After running `setup`, both `X` and `y` will be held as attributes.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | Union[pd.DataFrame, np.ndarray, List] | Features. |\n",
       "| y | Union[pd.DataFrame, np.ndarray, List] | Target |\n",
       "| **Returns** | **PoniardBaseEstimator** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PoniardBaseEstimator.setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example\n",
    "\n",
    "Let's load some random data and setup a `PoniardClassifier`, which inherits from `PoniardBaseEstimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poniard import PoniardClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apartment</td>\n",
       "      <td>127</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apartment</td>\n",
       "      <td>54</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>house</td>\n",
       "      <td>9</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>house</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apartment</td>\n",
       "      <td>162</td>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type  age       date  rating  target\n",
       "0  apartment  127 2022-01-31       1       1\n",
       "1  apartment   54 2022-02-28      17       1\n",
       "2      house    9 2022-03-31       0       1\n",
       "3      house    4 2022-04-30      48       1\n",
       "4  apartment  162 2022-05-31      40       0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "data = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n",
    "                     \"age\": rng.uniform(1, 200, 500).astype(int),\n",
    "                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n",
    "                     \"rating\": random.choices(range(50), k=500),\n",
    "                     \"target\": random.choices([0, 1], k=500)})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`setup` will conveniently output information about the data so it can be reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target info\n",
      "-----------\n",
      "Type: binary\n",
      "Shape: (500,)\n",
      "Unique values: 2\n",
      "\n",
      "Main metric\n",
      "-----------\n",
      "roc_auc\n",
      "\n",
      "Thresholds\n",
      "----------\n",
      "Minimum unique values to consider a feature numeric: 50\n",
      "Minimum unique values to consider a categorical high cardinality: 20\n",
      "\n",
      "Inferred feature types\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric</th>\n",
       "      <th>categorical_high</th>\n",
       "      <th>categorical_low</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>rating</td>\n",
       "      <td>type</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n",
       "    preprocess=True, scaler=standard, numeric_imputer=simple,\n",
       "    custom_preprocessor=None, numeric_threshold=50,\n",
       "    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n",
       "    random_state=0, n_jobs=None, plugins=None,\n",
       "    plot_options=PoniardPlotFactory())\n",
       "            "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = data.drop(\"target\", axis=1), data[\"target\"]\n",
    "pnd = PoniardClassifier()\n",
    "pnd.setup(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes available after `setup`\n",
    "\n",
    "After passing data to Poniard estimators through `setup`, multiple attributes become available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inferred_types` is a `DataFrame` that sorts features in 4 categories (numeric, categorical_high, categorical_low and datetime) using some basic heuristics.\n",
    "\n",
    "These depend on the feature `dtypes`, and `numeric_threshold` and `cardinality_threshold` which are set during `PoniardBaseEstimator`'s construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric</th>\n",
       "      <th>categorical_high</th>\n",
       "      <th>categorical_low</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>rating</td>\n",
       "      <td>type</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  numeric categorical_high categorical_low datetime\n",
       "0     age           rating            type     date"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.inferred_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocessor` in turn depends on `inferred_types`, and the `scaler`, `numeric_imputer` and `high_cardinality_encoder` parameters passed to the Poniard estimator init.\n",
    "\n",
    "As will be seen further on, this preprocessor can be modified significantly to fit multiple use cases and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;type_preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numeric_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;numeric_imputer&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;age&#x27;]),\n",
       "                                                 (&#x27;categorical_low_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;one-hot_encoder&#x27;,\n",
       "                                                                   OneHotEncoder(drop=&#x27;if_binary&#x27;,\n",
       "                                                                                 hand...\n",
       "                                                  Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;high_cardinality_encoder&#x27;,\n",
       "                                                                   TargetEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 task=&#x27;classification&#x27;))]),\n",
       "                                                  [&#x27;rating&#x27;]),\n",
       "                                                 (&#x27;datetime_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;datetime_encoder&#x27;,\n",
       "                                                                   DatetimeEncoder()),\n",
       "                                                                  (&#x27;datetime_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),\n",
       "                                                  [&#x27;date&#x27;])])),\n",
       "                (&#x27;remove_invariant&#x27;, VarianceThreshold())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;type_preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numeric_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;numeric_imputer&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;age&#x27;]),\n",
       "                                                 (&#x27;categorical_low_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;one-hot_encoder&#x27;,\n",
       "                                                                   OneHotEncoder(drop=&#x27;if_binary&#x27;,\n",
       "                                                                                 hand...\n",
       "                                                  Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;high_cardinality_encoder&#x27;,\n",
       "                                                                   TargetEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 task=&#x27;classification&#x27;))]),\n",
       "                                                  [&#x27;rating&#x27;]),\n",
       "                                                 (&#x27;datetime_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;datetime_encoder&#x27;,\n",
       "                                                                   DatetimeEncoder()),\n",
       "                                                                  (&#x27;datetime_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),\n",
       "                                                  [&#x27;date&#x27;])])),\n",
       "                (&#x27;remove_invariant&#x27;, VarianceThreshold())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">type_preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;numeric_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;numeric_imputer&#x27;,\n",
       "                                                  SimpleImputer()),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;age&#x27;]),\n",
       "                                (&#x27;categorical_low_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;one-hot_encoder&#x27;,\n",
       "                                                  OneHotEncoder(drop=&#x27;if_binary&#x27;,\n",
       "                                                                handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse=False))]),...\n",
       "                                (&#x27;categorical_high_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;high_cardinality_encoder&#x27;,\n",
       "                                                  TargetEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                task=&#x27;classification&#x27;))]),\n",
       "                                 [&#x27;rating&#x27;]),\n",
       "                                (&#x27;datetime_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;datetime_encoder&#x27;,\n",
       "                                                  DatetimeEncoder()),\n",
       "                                                 (&#x27;datetime_imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),\n",
       "                                 [&#x27;date&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numeric_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;age&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_low_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;type&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(drop=&#x27;if_binary&#x27;, handle_unknown=&#x27;ignore&#x27;, sparse=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_high_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;rating&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(handle_unknown=&#x27;ignore&#x27;, task=&#x27;classification&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">datetime_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;date&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DatetimeEncoder</label><div class=\"sk-toggleable__content\"><pre>DatetimeEncoder()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VarianceThreshold</label><div class=\"sk-toggleable__content\"><pre>VarianceThreshold()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('type_preprocessor',\n",
       "                 ColumnTransformer(transformers=[('numeric_preprocessor',\n",
       "                                                  Pipeline(steps=[('numeric_imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['age']),\n",
       "                                                 ('categorical_low_preprocessor',\n",
       "                                                  Pipeline(steps=[('categorical_imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('one-hot_encoder',\n",
       "                                                                   OneHotEncoder(drop='if_binary',\n",
       "                                                                                 hand...\n",
       "                                                  Pipeline(steps=[('categorical_imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('high_cardinality_encoder',\n",
       "                                                                   TargetEncoder(handle_unknown='ignore',\n",
       "                                                                                 task='classification'))]),\n",
       "                                                  ['rating']),\n",
       "                                                 ('datetime_preprocessor',\n",
       "                                                  Pipeline(steps=[('datetime_encoder',\n",
       "                                                                   DatetimeEncoder()),\n",
       "                                                                  ('datetime_imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent'))]),\n",
       "                                                  ['date'])])),\n",
       "                ('remove_invariant', VarianceThreshold())])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each estimator has a set of default `metrics`, but others can be passed during construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roc_auc', 'accuracy', 'precision', 'recall', 'f1']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, `cv` has sane defaults but can be modified accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=5, random_state=0, shuffle=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target_info` lists information about `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type_': 'binary', 'ndim': 1, 'shape': (500,), 'nunique': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.target_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipelines` is a dict containing each pipeline which will be trained during `fit`. Each Poniard estimator has a limited set of default estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;type_preprocessor&#x27;,\n",
       "                                  ColumnTransformer(transformers=[(&#x27;numeric_preprocessor&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;numeric_imputer&#x27;,\n",
       "                                                                                    SimpleImputer()),\n",
       "                                                                                   (&#x27;scaler&#x27;,\n",
       "                                                                                    StandardScaler())]),\n",
       "                                                                   [&#x27;age&#x27;]),\n",
       "                                                                  (&#x27;categorical_low_preprocessor&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                                    SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                                   (&#x27;one-hot_encoder&#x27;,\n",
       "                                                                                    One...\n",
       "                                                                                   (&#x27;high_cardinality_encoder&#x27;,\n",
       "                                                                                    TargetEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                                  task=&#x27;classification&#x27;))]),\n",
       "                                                                   [&#x27;rating&#x27;]),\n",
       "                                                                  (&#x27;datetime_preprocessor&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;datetime_encoder&#x27;,\n",
       "                                                                                    DatetimeEncoder()),\n",
       "                                                                                   (&#x27;datetime_imputer&#x27;,\n",
       "                                                                                    SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),\n",
       "                                                                   [&#x27;date&#x27;])])),\n",
       "                                 (&#x27;remove_invariant&#x27;, VarianceThreshold())])),\n",
       "                (&#x27;SVC&#x27;,\n",
       "                 SVC(kernel=&#x27;linear&#x27;, probability=True, random_state=0,\n",
       "                     verbose=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;type_preprocessor&#x27;,\n",
       "                                  ColumnTransformer(transformers=[(&#x27;numeric_preprocessor&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;numeric_imputer&#x27;,\n",
       "                                                                                    SimpleImputer()),\n",
       "                                                                                   (&#x27;scaler&#x27;,\n",
       "                                                                                    StandardScaler())]),\n",
       "                                                                   [&#x27;age&#x27;]),\n",
       "                                                                  (&#x27;categorical_low_preprocessor&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                                    SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                                   (&#x27;one-hot_encoder&#x27;,\n",
       "                                                                                    One...\n",
       "                                                                                   (&#x27;high_cardinality_encoder&#x27;,\n",
       "                                                                                    TargetEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                                  task=&#x27;classification&#x27;))]),\n",
       "                                                                   [&#x27;rating&#x27;]),\n",
       "                                                                  (&#x27;datetime_preprocessor&#x27;,\n",
       "                                                                   Pipeline(steps=[(&#x27;datetime_encoder&#x27;,\n",
       "                                                                                    DatetimeEncoder()),\n",
       "                                                                                   (&#x27;datetime_imputer&#x27;,\n",
       "                                                                                    SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),\n",
       "                                                                   [&#x27;date&#x27;])])),\n",
       "                                 (&#x27;remove_invariant&#x27;, VarianceThreshold())])),\n",
       "                (&#x27;SVC&#x27;,\n",
       "                 SVC(kernel=&#x27;linear&#x27;, probability=True, random_state=0,\n",
       "                     verbose=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;type_preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numeric_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;numeric_imputer&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;age&#x27;]),\n",
       "                                                 (&#x27;categorical_low_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;one-hot_encoder&#x27;,\n",
       "                                                                   OneHotEncoder(drop=&#x27;if_binary&#x27;,\n",
       "                                                                                 hand...\n",
       "                                                  Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;high_cardinality_encoder&#x27;,\n",
       "                                                                   TargetEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 task=&#x27;classification&#x27;))]),\n",
       "                                                  [&#x27;rating&#x27;]),\n",
       "                                                 (&#x27;datetime_preprocessor&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;datetime_encoder&#x27;,\n",
       "                                                                   DatetimeEncoder()),\n",
       "                                                                  (&#x27;datetime_imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),\n",
       "                                                  [&#x27;date&#x27;])])),\n",
       "                (&#x27;remove_invariant&#x27;, VarianceThreshold())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">type_preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;numeric_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;numeric_imputer&#x27;,\n",
       "                                                  SimpleImputer()),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;age&#x27;]),\n",
       "                                (&#x27;categorical_low_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;one-hot_encoder&#x27;,\n",
       "                                                  OneHotEncoder(drop=&#x27;if_binary&#x27;,\n",
       "                                                                handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse=False))]),...\n",
       "                                (&#x27;categorical_high_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;categorical_imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;high_cardinality_encoder&#x27;,\n",
       "                                                  TargetEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                task=&#x27;classification&#x27;))]),\n",
       "                                 [&#x27;rating&#x27;]),\n",
       "                                (&#x27;datetime_preprocessor&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;datetime_encoder&#x27;,\n",
       "                                                  DatetimeEncoder()),\n",
       "                                                 (&#x27;datetime_imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;))]),\n",
       "                                 [&#x27;date&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numeric_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;age&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_low_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;type&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(drop=&#x27;if_binary&#x27;, handle_unknown=&#x27;ignore&#x27;, sparse=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_high_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;rating&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(handle_unknown=&#x27;ignore&#x27;, task=&#x27;classification&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">datetime_preprocessor</label><div class=\"sk-toggleable__content\"><pre>[&#x27;date&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DatetimeEncoder</label><div class=\"sk-toggleable__content\"><pre>DatetimeEncoder()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VarianceThreshold</label><div class=\"sk-toggleable__content\"><pre>VarianceThreshold()</pre></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True, random_state=0, verbose=0)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 Pipeline(steps=[('type_preprocessor',\n",
       "                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n",
       "                                                                   Pipeline(steps=[('numeric_imputer',\n",
       "                                                                                    SimpleImputer()),\n",
       "                                                                                   ('scaler',\n",
       "                                                                                    StandardScaler())]),\n",
       "                                                                   ['age']),\n",
       "                                                                  ('categorical_low_preprocessor',\n",
       "                                                                   Pipeline(steps=[('categorical_imputer',\n",
       "                                                                                    SimpleImputer(strategy='most_frequent')),\n",
       "                                                                                   ('one-hot_encoder',\n",
       "                                                                                    One...\n",
       "                                                                                   ('high_cardinality_encoder',\n",
       "                                                                                    TargetEncoder(handle_unknown='ignore',\n",
       "                                                                                                  task='classification'))]),\n",
       "                                                                   ['rating']),\n",
       "                                                                  ('datetime_preprocessor',\n",
       "                                                                   Pipeline(steps=[('datetime_encoder',\n",
       "                                                                                    DatetimeEncoder()),\n",
       "                                                                                   ('datetime_imputer',\n",
       "                                                                                    SimpleImputer(strategy='most_frequent'))]),\n",
       "                                                                   ['date'])])),\n",
       "                                 ('remove_invariant', VarianceThreshold())])),\n",
       "                ('SVC',\n",
       "                 SVC(kernel='linear', probability=True, random_state=0,\n",
       "                     verbose=0))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.pipelines[\"SVC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fit` and `get_results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because features and target are passed to the Poniard estimator, `fit` does not take any parameters. Its main purpose is to run sklearn's `cross_validate` function on each `pipeline`, scoring each `metrics` with the `cv` strategy, and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completed: 100%|██████████| 9/9 [00:15<00:00,  1.69s/it]                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n",
       "    preprocess=True, scaler=standard, numeric_imputer=simple,\n",
       "    custom_preprocessor=None, numeric_threshold=50,\n",
       "    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n",
       "    random_state=0, n_jobs=None, plugins=None,\n",
       "    plot_options=PoniardPlotFactory())\n",
       "            "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting `pipelines`, cross validated results can be accessed by running `get_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PoniardBaseEstimator.get_results\n",
       "\n",
       ">      PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n",
       ">                                        std:bool=False, wrt_dummy:bool=False)\n",
       "\n",
       "Return dataframe containing scoring results. By default returns the mean score and fit\n",
       "and score times. Optionally returns standard deviations as well.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| return_train_scores | bool | False | If False, only return test scores. |\n",
       "| std | bool | False | Whether to return standard deviation of the scores. Default False. |\n",
       "| wrt_dummy | bool | False | Whether to compute each score/time with respect to the dummy estimator results. Default<br>False. |\n",
       "| **Returns** | **Union[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]** |  | **Results** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PoniardBaseEstimator.get_results\n",
       "\n",
       ">      PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n",
       ">                                        std:bool=False, wrt_dummy:bool=False)\n",
       "\n",
       "Return dataframe containing scoring results. By default returns the mean score and fit\n",
       "and score times. Optionally returns standard deviations as well.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| return_train_scores | bool | False | If False, only return test scores. |\n",
       "| std | bool | False | Whether to return standard deviation of the scores. Default False. |\n",
       "| wrt_dummy | bool | False | Whether to compute each score/time with respect to the dummy estimator results. Default<br>False. |\n",
       "| **Returns** | **Union[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]** |  | **Results** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PoniardBaseEstimator.get_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.510256</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.531145</td>\n",
       "      <td>0.503846</td>\n",
       "      <td>0.516707</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.013337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.014292</td>\n",
       "      <td>0.010856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.496675</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.509150</td>\n",
       "      <td>0.534615</td>\n",
       "      <td>0.519465</td>\n",
       "      <td>0.016555</td>\n",
       "      <td>0.016932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.472356</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.499007</td>\n",
       "      <td>0.688462</td>\n",
       "      <td>0.575907</td>\n",
       "      <td>0.821230</td>\n",
       "      <td>0.013713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.468990</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.509234</td>\n",
       "      <td>0.573077</td>\n",
       "      <td>0.536862</td>\n",
       "      <td>0.039569</td>\n",
       "      <td>0.037738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.460417</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.502401</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.499330</td>\n",
       "      <td>0.071057</td>\n",
       "      <td>0.015129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingClassifier</th>\n",
       "      <td>0.456571</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.505975</td>\n",
       "      <td>0.484615</td>\n",
       "      <td>0.494283</td>\n",
       "      <td>1.556914</td>\n",
       "      <td>0.042196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.435056</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.479861</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.477449</td>\n",
       "      <td>0.107180</td>\n",
       "      <td>0.020063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.423317</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.492473</td>\n",
       "      <td>0.565385</td>\n",
       "      <td>0.525371</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.010184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                test_roc_auc  test_accuracy  test_precision  \\\n",
       "DecisionTreeClassifier              0.510256          0.510        0.531145   \n",
       "DummyClassifier                     0.500000          0.520        0.520000   \n",
       "KNeighborsClassifier                0.496675          0.492        0.509150   \n",
       "SVC                                 0.472356          0.476        0.499007   \n",
       "LogisticRegression                  0.468990          0.488        0.509234   \n",
       "XGBClassifier                       0.460417          0.486        0.502401   \n",
       "HistGradientBoostingClassifier      0.456571          0.488        0.505975   \n",
       "RandomForestClassifier              0.435056          0.462        0.479861   \n",
       "GaussianNB                          0.423317          0.468        0.492473   \n",
       "\n",
       "                                test_recall   test_f1  fit_time  score_time  \n",
       "DecisionTreeClassifier             0.503846  0.516707  0.016876    0.013337  \n",
       "DummyClassifier                    1.000000  0.684211  0.014292    0.010856  \n",
       "KNeighborsClassifier               0.534615  0.519465  0.016555    0.016932  \n",
       "SVC                                0.688462  0.575907  0.821230    0.013713  \n",
       "LogisticRegression                 0.573077  0.536862  0.039569    0.037738  \n",
       "XGBClassifier                      0.500000  0.499330  0.071057    0.015129  \n",
       "HistGradientBoostingClassifier     0.484615  0.494283  1.556914    0.042196  \n",
       "RandomForestClassifier             0.476923  0.477449  0.107180    0.020063  \n",
       "GaussianNB                         0.565385  0.525371  0.015108    0.010184  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pnd.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.060706</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.060332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.001982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>0.001764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.021105</td>\n",
       "      <td>8.429609e-03</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.019140</td>\n",
       "      <td>0.008157</td>\n",
       "      <td>0.081043</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.049760</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.007211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.038609</td>\n",
       "      <td>3.600720e-02</td>\n",
       "      <td>0.042708</td>\n",
       "      <td>0.032496</td>\n",
       "      <td>0.031965</td>\n",
       "      <td>0.028405</td>\n",
       "      <td>0.085485</td>\n",
       "      <td>0.073140</td>\n",
       "      <td>0.036968</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.110120</td>\n",
       "      <td>0.004044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.068079</td>\n",
       "      <td>2.545484e-02</td>\n",
       "      <td>0.041183</td>\n",
       "      <td>0.027946</td>\n",
       "      <td>0.037992</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>0.065948</td>\n",
       "      <td>0.021371</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.044312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.065278</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.035553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>0.003623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingClassifier</th>\n",
       "      <td>0.059681</td>\n",
       "      <td>7.749323e-04</td>\n",
       "      <td>0.041183</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>0.039938</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.054859</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.600654</td>\n",
       "      <td>0.006484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.060809</td>\n",
       "      <td>7.021667e-17</td>\n",
       "      <td>0.039192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025762</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.045845</td>\n",
       "      <td>2.494438e-02</td>\n",
       "      <td>0.042143</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.037330</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>0.031246</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>0.025456</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.001748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                test_roc_auc  train_roc_auc  test_accuracy  \\\n",
       "DecisionTreeClassifier              0.060706   0.000000e+00       0.060332   \n",
       "DummyClassifier                     0.000000   0.000000e+00       0.000000   \n",
       "KNeighborsClassifier                0.021105   8.429609e-03       0.019391   \n",
       "SVC                                 0.038609   3.600720e-02       0.042708   \n",
       "LogisticRegression                  0.068079   2.545484e-02       0.041183   \n",
       "XGBClassifier                       0.065278   0.000000e+00       0.035553   \n",
       "HistGradientBoostingClassifier      0.059681   7.749323e-04       0.041183   \n",
       "RandomForestClassifier              0.060809   7.021667e-17       0.039192   \n",
       "GaussianNB                          0.045845   2.494438e-02       0.042143   \n",
       "\n",
       "                                train_accuracy  test_precision  \\\n",
       "DecisionTreeClassifier                0.000000        0.059942   \n",
       "DummyClassifier                       0.000000        0.000000   \n",
       "KNeighborsClassifier                  0.010840        0.019140   \n",
       "SVC                                   0.032496        0.031965   \n",
       "LogisticRegression                    0.027946        0.037992   \n",
       "XGBClassifier                         0.000000        0.033315   \n",
       "HistGradientBoostingClassifier        0.007483        0.039938   \n",
       "RandomForestClassifier                0.000000        0.038392   \n",
       "GaussianNB                            0.018303        0.037330   \n",
       "\n",
       "                                train_precision  test_recall  train_recall  \\\n",
       "DecisionTreeClassifier                 0.000000     0.058835      0.000000   \n",
       "DummyClassifier                        0.000000     0.000000      0.000000   \n",
       "KNeighborsClassifier                   0.008157     0.081043      0.022053   \n",
       "SVC                                    0.028405     0.085485      0.073140   \n",
       "LogisticRegression                     0.024759     0.065948      0.021371   \n",
       "XGBClassifier                          0.000000     0.091826      0.000000   \n",
       "HistGradientBoostingClassifier         0.011912     0.070291      0.005607   \n",
       "RandomForestClassifier                 0.000000     0.077307      0.000000   \n",
       "GaussianNB                             0.015830     0.031246      0.038051   \n",
       "\n",
       "                                 test_f1  train_f1  fit_time  score_time  \n",
       "DecisionTreeClassifier          0.057785  0.000000  0.001712    0.001982  \n",
       "DummyClassifier                 0.000000  0.000000  0.004226    0.001764  \n",
       "KNeighborsClassifier            0.049760  0.012869  0.002909    0.007211  \n",
       "SVC                             0.036968  0.026864  0.110120    0.004044  \n",
       "LogisticRegression              0.036585  0.022583  0.009066    0.044312  \n",
       "XGBClassifier                   0.061108  0.000000  0.008010    0.003623  \n",
       "HistGradientBoostingClassifier  0.054859  0.007046  0.600654    0.006484  \n",
       "RandomForestClassifier          0.056132  0.000000  0.025762    0.002754  \n",
       "GaussianNB                      0.025456  0.018727  0.004558    0.001748  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, stds = pnd.get_results(std=True, return_train_scores=True)\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PoniardBaseEstimator.reassign_types\n",
       "\n",
       ">      PoniardBaseEstimator.reassign_types\n",
       ">                                           (numeric:Union[List[Union[str,int]],\n",
       ">                                           NoneType]=None, categorical_high:Uni\n",
       ">                                           on[List[Union[str,int]],NoneType]=No\n",
       ">                                           ne, categorical_low:Union[List[Union\n",
       ">                                           [str,int]],NoneType]=None, datetime:\n",
       ">                                           Union[List[Union[str,int]],NoneType]\n",
       ">                                           =None)\n",
       "\n",
       "Reassign feature types.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| numeric | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| categorical_high | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| categorical_low | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| datetime | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| **Returns** | **PoniardBaseEstimator** |  | **self.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PoniardBaseEstimator.reassign_types\n",
       "\n",
       ">      PoniardBaseEstimator.reassign_types\n",
       ">                                           (numeric:Union[List[Union[str,int]],\n",
       ">                                           NoneType]=None, categorical_high:Uni\n",
       ">                                           on[List[Union[str,int]],NoneType]=No\n",
       ">                                           ne, categorical_low:Union[List[Union\n",
       ">                                           [str,int]],NoneType]=None, datetime:\n",
       ">                                           Union[List[Union[str,int]],NoneType]\n",
       ">                                           =None)\n",
       "\n",
       "Reassign feature types.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| numeric | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| categorical_high | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| categorical_low | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| datetime | Optional[List[Union[str, int]]] | None | List of column names or indices. Default None. |\n",
       "| **Returns** | **PoniardBaseEstimator** |  | **self.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PoniardBaseEstimator.reassign_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poniard",
   "language": "python",
   "name": "poniard"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
