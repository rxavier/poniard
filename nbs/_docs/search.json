[
  {
    "objectID": "estimators.core_nb.html",
    "href": "estimators.core_nb.html",
    "title": "Basic usage",
    "section": "",
    "text": "PoniardBaseEstimator (estimators:Optional[Union[Sequence[ClassifierMixin]\n                       ,Dict[str,ClassifierMixin],Sequence[RegressorMixin]\n                       ,Dict[str,RegressorMixin]]]=None, metrics:Optional[\n                       Union[str,Dict[str,Callable],Sequence[str]]]=None,\n                       preprocess:bool=True,\n                       scaler:Optional[Union[str,TransformerMixin]]=None, \n                       high_cardinality_encoder:Optional[Union[str,Transfo\n                       rmerMixin]]=None, numeric_imputer:Optional[Union[st\n                       r,TransformerMixin]]=None, custom_preprocessor:Unio\n                       n[None,Pipeline,TransformerMixin]=None,\n                       numeric_threshold:Union[int,float]=0.1,\n                       cardinality_threshold:Union[int,float]=20, cv:Union\n                       [int,BaseCrossValidator,BaseShuffleSplit,Sequence]=\n                       None, verbose:int=0,\n                       random_state:Optional[int]=None,\n                       n_jobs:Optional[int]=None,\n                       plugins:Optional[Sequence[Any]]=None,\n                       plot_options:Optional[PoniardPlotFactory]=None,\n                       cache_transformations:bool=False)\n\nBase estimator that sets up all the functionality for the classifier and regressor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimators\nOptional[Union[Sequence[ClassifierMixin], Dict[str, ClassifierMixin], Sequence[RegressorMixin], Dict[str, RegressorMixin]]]\nNone\nEstimators to evaluate.\n\n\nmetrics\nOptional[Union[str, Dict[str, Callable], Sequence[str]]]\nNone\nMetrics to compute for each estimator. This is more restrictive than sklearn’s scoringparameter, as it does not allow callable scorers. Single strings are cast to listsautomatically.\n\n\npreprocess\nbool\nTrue\nIf True, impute missing values, standard scale numeric data and one-hot or ordinalencode categorical data.\n\n\nscaler\nOptional[Union[str, TransformerMixin]]\nNone\nNumeric scaler method. Either “standard”, “minmax”, “robust” or scikit-learn Transformer.\n\n\nhigh_cardinality_encoder\nOptional[Union[str, TransformerMixin]]\nNone\nEncoder for categorical features with high cardinality. Either “target” or “ordinal”,or scikit-learn Transformer.\n\n\nnumeric_imputer\nOptional[Union[str, TransformerMixin]]\nNone\nImputation method. Either “simple”, “iterative” or scikit-learn Transformer.\n\n\ncustom_preprocessor\nUnion[None, Pipeline, TransformerMixin]\nNone\nPreprocessor used instead of the default preprocessing pipeline. It must be able to beincluded directly in a scikit-learn Pipeline.\n\n\nnumeric_threshold\nUnion[int, float]\n0.1\nNumber features with unique values above a certain threshold will be treated as numeric. Iffloat, the threshold is numeric_threshold * samples.\n\n\ncardinality_threshold\nUnion[int, float]\n20\nNon-number features with cardinality above a certain threshold will be treated asordinal encoded instead of one-hot encoded. If float, the threshold iscardinality_threshold * samples.\n\n\ncv\nUnion[int, BaseCrossValidator, BaseShuffleSplit, Sequence]\nNone\nCross validation strategy. Either an integer, a scikit-learn cross validation object,or an iterable.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\nOptional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\nOptional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction.\n\n\nplugins\nOptional[Sequence[Any]]\nNone\nPlugin instances that run in set moments of setup, fit and plotting.\n\n\nplot_options\nOptional[PoniardPlotFactory]\nNone\n:class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly formatoptions or None, which sets the default factory.\n\n\ncache_transformations\nbool\nFalse\nWhether to cache transformations and set the memory parameter for Pipelines. This can speed up slow transformations as they are not recalculated for each estimator."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "poniard",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "poniard",
    "section": "Install",
    "text": "Install\npip install poniard"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "poniard",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "estimators.core_nb.html#setup",
    "href": "estimators.core_nb.html#setup",
    "title": "Basic usage",
    "section": "setup",
    "text": "setup\nsetup takes features and target as parameters, while fit does not accept any. This runs contrary to the established convention defined by scikit-learn where there is no setting up to do and fit takes the data as params.\nThis is because Poniard does not only fit the models, but also infer features types and create the preprocesor based on these types. While this could all be stuffed inside fit (that was the case initially), having it separated allows the user to check whether Poniard’s assumptions are correct and adjust if needed before running fit, which can take long depending on how many models were passed to estimators, the cross validation strategy and the size of the dataset.\n\n\nPoniardBaseEstimator.setup\n\n PoniardBaseEstimator.setup\n                             (X:Union[pandas.core.frame.DataFrame,numpy.nd\n                             array,List], y:Union[pandas.core.frame.DataFr\n                             ame,numpy.ndarray,List])\n\nOrchestrator.\nConverts inputs to arrays if necessary, sets metrics, preprocessor, cv and pipelines.\nAfter running setup, both X and y will be held as attributes.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nFeatures.\n\n\ny\nUnion[pd.DataFrame, np.ndarray, List]\nTarget\n\n\nReturns\nPoniardBaseEstimator\n\n\n\n\n\n\nAn example\nLet’s load some random data and setup a PoniardClassifier, which inherits from PoniardBaseEstimator.\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n                     \"age\": rng.uniform(1, 200, 500).astype(int),\n                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n                     \"rating\": random.choices(range(50), k=500),\n                     \"target\": random.choices([0, 1], k=500)})\ndata.head()\n\n\n\n\n\n  \n    \n      \n      type\n      age\n      date\n      rating\n      target\n    \n  \n  \n    \n      0\n      apartment\n      127\n      2022-01-31\n      1\n      1\n    \n    \n      1\n      apartment\n      54\n      2022-02-28\n      17\n      1\n    \n    \n      2\n      house\n      9\n      2022-03-31\n      0\n      1\n    \n    \n      3\n      house\n      4\n      2022-04-30\n      48\n      1\n    \n    \n      4\n      apartment\n      162\n      2022-05-31\n      40\n      0\n    \n  \n\n\n\n\nsetup will conveniently output information about the data so it can be reviewed.\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: binary\nShape: (500,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\n\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\n\n\nAttributes available after setup\nAfter passing data to Poniard estimators through setup, multiple attributes become available.\ninferred_types is a DataFrame that sorts features in 4 categories (numeric, categorical_high, categorical_low and datetime) using some basic heuristics.\nThese depend on the feature dtypes, and numeric_threshold and cardinality_threshold which are set during PoniardBaseEstimator’s construction.\n\npnd.inferred_types\n\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\nThe preprocessor in turn depends on inferred_types, and the scaler, numeric_imputer and high_cardinality_encoder parameters passed to the Poniard estimator init.\nAs will be seen further on, this preprocessor can be modified significantly to fit multiple use cases and datasets.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nEach estimator has a set of default metrics, but others can be passed during construction.\n\npnd.metrics\n\n['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n\n\nLikewise, cv has sane defaults but can be modified accordingly.\n\npnd.cv\n\nStratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\n\ntarget_info lists information about y.\n\npnd.target_info\n\n{'type_': 'binary', 'ndim': 1, 'shape': (500,), 'nunique': 2}\n\n\npipelines is a dict containing each pipeline which will be trained during fit. Each Poniard estimator has a limited set of default estimators.\n\npnd.pipelines[\"SVC\"]\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()SVCSVC(kernel='linear', probability=True, random_state=0, verbose=0)"
  },
  {
    "objectID": "estimators.core_nb.html#fit-and-get_results",
    "href": "estimators.core_nb.html#fit-and-get_results",
    "title": "Basic usage",
    "section": "fit and get_results",
    "text": "fit and get_results\nBecause features and target are passed to the Poniard estimator, fit does not take any parameters. Its main purpose is to run sklearn’s cross_validate function on each pipeline, scoring each metrics with the cv strategy, and store the results.\n\npnd.fit()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:07<00:00,  1.14it/s]\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nAfter fitting pipelines, cross validated results can be accessed by running get_results\n\n\nPoniardBaseEstimator.get_results\n\n PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n                                   std:bool=False, wrt_dummy:bool=False)\n\nReturn dataframe containing scoring results. By default returns the mean score and fit and score times. Optionally returns standard deviations as well.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_train_scores\nbool\nFalse\nIf False, only return test scores.\n\n\nstd\nbool\nFalse\nWhether to return standard deviation of the scores. Default False.\n\n\nwrt_dummy\nbool\nFalse\nWhether to compute each score/time with respect to the dummy estimator results. DefaultFalse.\n\n\nReturns\nUnion[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]\n\nResults\n\n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.510256\n      0.510\n      0.531145\n      0.503846\n      0.516707\n      0.011685\n      0.007774\n    \n    \n      DummyClassifier\n      0.500000\n      0.520\n      0.520000\n      1.000000\n      0.684211\n      0.009682\n      0.007357\n    \n    \n      KNeighborsClassifier\n      0.496675\n      0.492\n      0.509150\n      0.534615\n      0.519465\n      0.013338\n      0.010852\n    \n    \n      SVC\n      0.472356\n      0.476\n      0.499007\n      0.688462\n      0.575907\n      0.752767\n      0.009136\n    \n    \n      LogisticRegression\n      0.468990\n      0.488\n      0.509234\n      0.573077\n      0.536862\n      0.024696\n      0.008803\n    \n    \n      XGBClassifier\n      0.460417\n      0.486\n      0.502401\n      0.500000\n      0.499330\n      0.046835\n      0.009680\n    \n    \n      HistGradientBoostingClassifier\n      0.456571\n      0.488\n      0.505975\n      0.484615\n      0.494283\n      0.414287\n      0.017435\n    \n    \n      RandomForestClassifier\n      0.435056\n      0.462\n      0.479861\n      0.476923\n      0.477449\n      0.075519\n      0.016002\n    \n    \n      GaussianNB\n      0.423317\n      0.468\n      0.492473\n      0.565385\n      0.525371\n      0.010960\n      0.008040\n    \n  \n\n\n\n\n\nmeans, stds = pnd.get_results(std=True, return_train_scores=True)\nstds\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      train_roc_auc\n      test_accuracy\n      train_accuracy\n      test_precision\n      train_precision\n      test_recall\n      train_recall\n      test_f1\n      train_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.060706\n      0.000000e+00\n      0.060332\n      0.000000\n      0.059942\n      0.000000\n      0.058835\n      0.000000\n      0.057785\n      0.000000\n      0.000498\n      0.000202\n    \n    \n      DummyClassifier\n      0.000000\n      0.000000e+00\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000309\n      0.000131\n    \n    \n      KNeighborsClassifier\n      0.021105\n      8.429609e-03\n      0.019391\n      0.010840\n      0.019140\n      0.008157\n      0.081043\n      0.022053\n      0.049760\n      0.012869\n      0.002560\n      0.001735\n    \n    \n      SVC\n      0.038609\n      3.600720e-02\n      0.042708\n      0.032496\n      0.031965\n      0.028405\n      0.085485\n      0.073140\n      0.036968\n      0.026864\n      0.120598\n      0.000308\n    \n    \n      LogisticRegression\n      0.068079\n      2.545484e-02\n      0.041183\n      0.027946\n      0.037992\n      0.024759\n      0.065948\n      0.021371\n      0.036585\n      0.022583\n      0.009217\n      0.000333\n    \n    \n      XGBClassifier\n      0.065278\n      0.000000e+00\n      0.035553\n      0.000000\n      0.033315\n      0.000000\n      0.091826\n      0.000000\n      0.061108\n      0.000000\n      0.001461\n      0.000244\n    \n    \n      HistGradientBoostingClassifier\n      0.059681\n      7.749323e-04\n      0.041183\n      0.007483\n      0.039938\n      0.011912\n      0.070291\n      0.005607\n      0.054859\n      0.007046\n      0.146359\n      0.003724\n    \n    \n      RandomForestClassifier\n      0.060809\n      7.021667e-17\n      0.039192\n      0.000000\n      0.038392\n      0.000000\n      0.077307\n      0.000000\n      0.056132\n      0.000000\n      0.000826\n      0.000739\n    \n    \n      GaussianNB\n      0.045845\n      2.494438e-02\n      0.042143\n      0.018303\n      0.037330\n      0.015830\n      0.031246\n      0.038051\n      0.025456\n      0.018727\n      0.000656\n      0.000235"
  },
  {
    "objectID": "01_preprocessing.html",
    "href": "01_preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Poniard tries to apply minimal preprocessing to data. In general, it just tries to make sure that models fit correctly without introducing signifcant transformation overhead. In particular, there is no anomaly detection, dimensionality reduction, clustering, resampling, feature creation from polynomial interactions, feature selection, etc.\nThis is so the user always knows what’s going on.\nHowever, the default options may not be suitable for your data or objectives, so these can be set during initialization or modified afterwards.\nThe list of default transformations is: * Missing data imputation. * Z-score scaling for numeric variables. * One-hot encoding for low cardinality categorical variables. * Target encoding for the remaining categorical variables. This is a custom transformer based on Micci-Barreca, 2001, with implementation heavily based on Dirty Cat. * Datetime encoding for datetime variables. This also uses a custom transformer that extracts multiple datetime levels. * Zero-variance feature elimination.\nThis includes some type inference logic that decides whether a given feature is either numeric, categorical high cardinality, categorical low cardinality or datetime (see inferred_types).\nDuring init of either PoniardRegressor or PoniardClassifier (see docs for PoniardBaseEstimator which sets up most of the functionality), preprocess=False disables preprocessing altogether, while custom_preprocessor accepts a scikit-learn transformer (or pipeline/column transformer) that replaces the default Poniard transformation pipeline.\nLogically, there is no type inference involved when these options are used and full control is given to the user.\nIn the following example, we use TfidfVectorizer() to process the 20 News Groups dataset.\n\nX, y = fetch_20newsgroups(return_X_y=True, remove=(\"headers\", \"footers\", \"quotes\"),\n                          categories=(\"sci.crypt\", \"sci.electronics\", \"sci.med\"))\npnd = PoniardClassifier(estimators=[LogisticRegression()], custom_preprocessor=TfidfVectorizer())\npnd.setup(X, y)\npnd.preprocessor\n\nTarget info\n-----------\nType: multiclass\nShape: (1780,)\nUnique values: 3\n\nMain metric\n-----------\nroc_auc_ovr\n\n\n\nTfidfVectorizer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.TfidfVectorizerTfidfVectorizer()\n\n\n\npnd.pipelines[\"LogisticRegression\"]\n\nPipeline(steps=[('preprocessor', TfidfVectorizer()),\n                ('LogisticRegression', LogisticRegression(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor', TfidfVectorizer()),\n                ('LogisticRegression', LogisticRegression(random_state=0))])TfidfVectorizerTfidfVectorizer()LogisticRegressionLogisticRegression(random_state=0)\n\n\n\npnd.fit()\npnd.get_results()\n\n0it [00:00, ?it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc_ovr\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      0.976337\n      0.888202\n      0.896170\n      0.888335\n      0.888822\n      0.717878\n      0.148148\n    \n    \n      DummyClassifier\n      0.500000\n      0.334270\n      0.111423\n      0.333333\n      0.167018\n      0.341453\n      0.143987"
  },
  {
    "objectID": "estimators.core_nb.html#init",
    "href": "estimators.core_nb.html#init",
    "title": "Basic usage",
    "section": "init",
    "text": "init\nPoniard estimators’ main parameters can be grouped in the following way:\n\nEstimators.\nPreprocessing parameters.\n\nImputers\nNumeric scaler\nCategorical encoder\nCustom preprocessor\n\nMetrics.\nCross validation strategy.\nRest.\n\nThese give a good amount of flexibility while providing sane defaults, so that after initialization only setup and fit have to be called in order to train multiple models."
  },
  {
    "objectID": "estimators.core_nb.html#estimators-metrics-and-cv",
    "href": "estimators.core_nb.html#estimators-metrics-and-cv",
    "title": "Basic usage",
    "section": "estimators, metrics and cv",
    "text": "estimators, metrics and cv\nPoniard estimators’ main parameters can be grouped in the following way:\n\nEstimators.\nPreprocessing parameters.\n\nImputers\nNumeric scaler\nCategorical encoder\nCustom preprocessor\n\nMetrics.\nCross validation strategy.\nRest.\n\nThese give a good amount of flexibility while providing sane defaults, so that after initialization only setup and fit have to be called in order to train multiple models.\nestimators takes a scikit-learn-compatible estimator, array of estimators or dict of name: estimators.\n\nPoniardRegressor(estimators)"
  },
  {
    "objectID": "nutshell.html",
    "href": "nutshell.html",
    "title": "Poniard in a nutshell - getting started",
    "section": "",
    "text": "Essentially, a Poniard estimator is a set of scikit-learn estimators, a preprocessing strategy, a cross validation strategy and one or more metrics with which to score models.\nThe idea behind Poniard is to abstract away some of the boilerplate involved in fitting multiple models and comparing their cross validated results. However, a significant effort is made to keep everything flexible and as close to scikit-learn as possible.\nPoniard includes a PoniardClassifier and a PoniardRegressor, aligned with scikit-learn classifiers and regressors.\n\nfrom poniard import PoniardRegressor\n\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\npnd = PoniardRegressor()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: continuous\nShape: (442,)\nUnique values: 214\n\nMain metric\n-----------\nneg_mean_squared_error\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 44\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      \n      sex\n      \n    \n    \n      1\n      bmi\n      \n      \n      \n    \n    \n      2\n      bp\n      \n      \n      \n    \n    \n      3\n      s1\n      \n      \n      \n    \n    \n      4\n      s2\n      \n      \n      \n    \n    \n      5\n      s3\n      \n      \n      \n    \n    \n      6\n      s4\n      \n      \n      \n    \n    \n      7\n      s5\n      \n      \n      \n    \n    \n      8\n      s6\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPoniardRegressor(estimators=None, metrics=['neg_mean_squared_error', 'neg_mean_absolute_percentage_error', 'neg_median_absolute_error', 'r2'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=44,\n    cardinality_threshold=20, cv=KFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())"
  },
  {
    "objectID": "00_nutshell.html",
    "href": "00_nutshell.html",
    "title": "Poniard in a nutshell: getting started",
    "section": "",
    "text": "Essentially, a Poniard estimator is a set of scikit-learn estimators, a preprocessing strategy, a cross validation strategy and one or more metrics with which to score models.\nThe idea behind Poniard is to abstract away some of the boilerplate involved in fitting multiple models and comparing their cross validated results. However, a significant effort is made to keep everything flexible and as close to scikit-learn as possible.\nPoniard includes a PoniardClassifier and a PoniardRegressor, aligned with scikit-learn classifiers and regressors."
  },
  {
    "objectID": "02_preprocessing.html",
    "href": "02_preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Poniard tries to apply minimal preprocessing to data. In general, it just tries to make sure that models fit correctly without introducing signifcant transformation overhead. In particular, there is no anomaly detection, dimensionality reduction, clustering, resampling, feature creation from polynomial interactions, feature selection, etc.\nThis is so the user always knows what’s going on.\nHowever, the default options may not be suitable for your data or objectives, so these can be set during initialization or modified afterwards."
  },
  {
    "objectID": "00_nutshell.html#new-heading",
    "href": "00_nutshell.html#new-heading",
    "title": "Poniard in a nutshell - getting started",
    "section": "New heading",
    "text": "New heading\nIn those two lines, 9 different regression models were trained with cross validation (including a dumb DummyRegressor) and the average score for multiple metrics was printed.\nPoniard tries to provide good defaults everywhere * While sklearn includes many more regression models, it is generally enough to train a few select ones that have proven to deliver good results. * Different metrics capture different aspects of the relationship between predictions and ground truth, so Poniard includes multiple suitable ones. * Cross validation is a key aspect of the Poniard flow, and by default 5-fold validation is used.\nHowever, ssaand"
  },
  {
    "objectID": "00_nutshell.html#intended-uses-and-shortcomings",
    "href": "00_nutshell.html#intended-uses-and-shortcomings",
    "title": "Poniard in a nutshell: getting started",
    "section": "Intended uses and shortcomings",
    "text": "Intended uses and shortcomings"
  },
  {
    "objectID": "00_nutshell.html#basic-use",
    "href": "00_nutshell.html#basic-use",
    "title": "Poniard in a nutshell: getting started",
    "section": "Basic use",
    "text": "Basic use\nIn the following example we will load a toy dataset (sklearn’s diabetes dataset, a simple regression task) and have at it with default parameters.\n\nfrom poniard import PoniardRegressor\n\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\npnd = PoniardRegressor()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: continuous\nShape: (442,)\nUnique values: 214\n\nMain metric\n-----------\nneg_mean_squared_error\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 44\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      \n      sex\n      \n    \n    \n      1\n      bmi\n      \n      \n      \n    \n    \n      2\n      bp\n      \n      \n      \n    \n    \n      3\n      s1\n      \n      \n      \n    \n    \n      4\n      s2\n      \n      \n      \n    \n    \n      5\n      s3\n      \n      \n      \n    \n    \n      6\n      s4\n      \n      \n      \n    \n    \n      7\n      s5\n      \n      \n      \n    \n    \n      8\n      s6\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPoniardRegressor(estimators=None, metrics=['neg_mean_squared_error', 'neg_mean_absolute_percentage_error', 'neg_median_absolute_error', 'r2'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=44,\n    cardinality_threshold=20, cv=KFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nOut of the box, you will get some useful information regarding the target variable and the features, as well information regarding current Poniard settings (first metric and thresholds). These are covered in detail later.\nOnce Poniard has parsed the data and built the preprocessing pipeline, we are free to fit and get_results.\n\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:04<00:00,  2.01it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.004290\n      0.002018\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003509\n      0.001815\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.107415\n      0.005347\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.630318\n      0.011181\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003486\n      0.002222\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.060673\n      0.003658\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.003387\n      0.001729\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003231\n      0.001731\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004645\n      0.001941\n    \n  \n\n\n\n\nIn those two lines, 9 different regression models were trained with cross validation and the average score for multiple metrics was printed.\n\n\n\n\n\n\nDummy estimators\n\n\n\nPoniard always include a DummyClassifier with strategy=\"prior\" or DummyRegressor with strategy=\"mean\" in order to have the absolute minimum baseline scores. Models should easily beat these, but you could be surprised.\n\n\nPoniard tries to provide good defaults everywhere.\n\nestimators: sklearn provides more than 40 classifiers and 50 regressors, but for most problems you can make do with a limited list of the most battle tested models. Poniard reflects that.\nmetrics: different metrics capture different aspects of the relationship between predictions and ground truth, so Poniard includes multiple suitable ones.\ncv: cross validation is a key aspect of the Poniard flow, and by default 5-fold validation is used.\n\n\n\n\n\n\n\nrandom_seed behavior\n\n\n\nPoniard estimators’ random_seed parameter is always set (if random_seed=None at initialization, it will be forced to 0) and injected into models and cross validators. The idea is to get a reproducible environment, including using the same cross validation folds for each model.\n\n\nDefault preprocessing deserves its own mention. By default, type inference will be run on the datasets’ features and transformations will be applied accordingly. The end goal of the default preprocessor is to make models run without raising any errors.\nAs with most things in Poniard, the preprocessing pipeline can be modified or replaced entirely with the scikit-learn transformers you are used to.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4',\n                                  's5', 's6']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['sex'])])numeric_preprocessor['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['sex']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)VarianceThresholdVarianceThreshold()\n\n\nPoniard keeps track of which models it has cross validated, which means that\n\nIf a new one is added, it will not fit the existing ones.\nIf the preprocessor is changed after training, it will fit everything again.\n\n\npnd.add_estimators(SGDRegressor(max_iter=10000))\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00,  1.70it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.004290\n      0.002018\n    \n    \n      SGDRegressor\n      -3007.445505\n      -0.402132\n      -42.121968\n      0.484970\n      0.115586\n      0.000889\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003509\n      0.001815\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.107415\n      0.005347\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.630318\n      0.011181\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003486\n      0.002222\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.060673\n      0.003658\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.003387\n      0.001729\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003231\n      0.001731\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004645\n      0.001941\n    \n  \n\n\n\n\n\n\n\n\n\n\nSingle or array-like inputs\n\n\n\nAnywhere Poniard takes estimators or metrics, or strings representing them, a single element or a sequence of elements can be safely passed and will be handed gracefully."
  },
  {
    "objectID": "00_nutshell.html#going-deeper-than-get_results",
    "href": "00_nutshell.html#going-deeper-than-get_results",
    "title": "Poniard in a nutshell: getting started",
    "section": "Going deeper than get_results",
    "text": "Going deeper than get_results\nWhile a nicely formatted table is useful, graphical aid can make things go a lot smoother. Poniard estimators include a plot accessor that gives access to multiple prebuilt plots.\n\nfig = pnd.plot.metrics(kind=\"bar\", metrics=[\"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\"])\nfig.show(\"notebook\")\n\n\n                                                \n\n\n\nfig = pnd.plot.residuals_histogram(estimator_names=[\"LinearRegression\", \"SGDRegressor\"])\nfig.show(\"notebook\")\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 34.04it/s]\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00,  1.78it/s]\n\n\n\n                                                \n\n\nBy using Plotly, users can have an easier time exploring charts, zooming in, selecting specific models, etc."
  },
  {
    "objectID": "00_nutshell.html#basic-usage",
    "href": "00_nutshell.html#basic-usage",
    "title": "Poniard in a nutshell: getting started",
    "section": "Basic usage",
    "text": "Basic usage\nIn the following example we will load a toy dataset (sklearn’s diabetes dataset, a simple regression task) and have at it with default parameters.\n\nfrom poniard import PoniardRegressor\nfrom sklearn.datasets import load_diabetes\n\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\npnd = PoniardRegressor()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: continuous\nShape: (442,)\nUnique values: 214\n\nMain metric\n-----------\nneg_mean_squared_error\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 44\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      \n      sex\n      \n    \n    \n      1\n      bmi\n      \n      \n      \n    \n    \n      2\n      bp\n      \n      \n      \n    \n    \n      3\n      s1\n      \n      \n      \n    \n    \n      4\n      s2\n      \n      \n      \n    \n    \n      5\n      s3\n      \n      \n      \n    \n    \n      6\n      s4\n      \n      \n      \n    \n    \n      7\n      s5\n      \n      \n      \n    \n    \n      8\n      s6\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPoniardRegressor(estimators=None, metrics=['neg_mean_squared_error', 'neg_mean_absolute_percentage_error', 'neg_median_absolute_error', 'r2'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=44,\n    cardinality_threshold=20, cv=KFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nOut of the box, you will get some useful information regarding the target variable and the features, as well information regarding current Poniard settings (first metric and thresholds). These are covered in detail later.\nOnce Poniard has parsed the data and built the preprocessing pipeline, we are free to fit and get_results.\n\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:02<00:00,  3.70it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.004133\n      0.001948\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003554\n      0.001801\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.101755\n      0.004725\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.248171\n      0.005931\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003473\n      0.002178\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.056660\n      0.003020\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.003440\n      0.001719\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003119\n      0.001703\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004548\n      0.001736\n    \n  \n\n\n\n\nIn those two lines, 9 different regression models were trained with cross validation and the average score for multiple metrics was printed.\n\n\n\n\n\n\nDummy estimators\n\n\n\nPoniard always include a DummyClassifier with strategy=\"prior\" or DummyRegressor with strategy=\"mean\" in order to have the absolute minimum baseline scores. Models should easily beat these, but you could be surprised.\n\n\nPoniard tries to provide good defaults everywhere.\n\nestimators: sklearn provides more than 40 classifiers and 50 regressors, but for most problems you can make do with a limited list of the most battle tested models. Poniard reflects that.\nmetrics: different metrics capture different aspects of the relationship between predictions and ground truth, so Poniard includes multiple suitable ones.\ncv: cross validation is a key aspect of the Poniard flow, and by default 5-fold validation is used.\n\n\n\n\n\n\n\nrandom_seed behavior\n\n\n\nPoniard estimators’ random_seed parameter is always set (if random_seed=None at initialization, it will be forced to 0) and injected into models and cross validators. The idea is to get a reproducible environment, including using the same cross validation folds for each model.\n\n\nDefault preprocessing deserves its own mention. By default, type inference will be run on the datasets’ features and transformations will be applied accordingly. The end goal of the default preprocessor is to make models run without raising any errors.\nAs with most things in Poniard, the preprocessing pipeline can be modified or replaced entirely with the scikit-learn transformers you are used to.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4',\n                                  's5', 's6']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['sex'])])numeric_preprocessor['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['sex']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)VarianceThresholdVarianceThreshold()\n\n\nPoniard keeps track of which models it has cross validated, which means that\n\nIf a new one is added, it will not fit the existing ones.\nIf the preprocessor is changed after training, it will fit everything again.\n\n\nfrom sklearn.linear_model import SGDRegressor\n\n\npnd.add_estimators(SGDRegressor(max_iter=10000))\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00,  1.74it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.004133\n      0.001948\n    \n    \n      SGDRegressor\n      -3007.445505\n      -0.402132\n      -42.121968\n      0.484970\n      0.113252\n      0.000743\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003554\n      0.001801\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.101755\n      0.004725\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.248171\n      0.005931\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003473\n      0.002178\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.056660\n      0.003020\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.003440\n      0.001719\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003119\n      0.001703\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004548\n      0.001736\n    \n  \n\n\n\n\n\n\n\n\n\n\nSingle or array-like inputs\n\n\n\nAnywhere Poniard takes estimators or metrics, or strings representing them, a single element or a sequence of elements can be safely passed and will be handed gracefully."
  },
  {
    "objectID": "00_nutshell.html#a-reasonably-unified-api",
    "href": "00_nutshell.html#a-reasonably-unified-api",
    "title": "Poniard in a nutshell: getting started",
    "section": "A reasonably unified API",
    "text": "A reasonably unified API\nSo far we have analyzed a regression task. Luckily, PoniardClassifier and PoniardRegressor differ only in default models, default cross validation strategy and default metrics.\n\nfrom poniard import PoniardClassifier\nfrom sklearn.datasets import load_wine\n\n\nX, y = load_wine(return_X_y=True, as_frame=True)\nclf = PoniardClassifier().setup(X, y)\n\nTarget info\n-----------\nType: multiclass\nShape: (178,)\nUnique values: 3\n\nMain metric\n-----------\nroc_auc_ovr\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 17\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      alcohol\n      \n      \n      \n    \n    \n      1\n      malic_acid\n      \n      \n      \n    \n    \n      2\n      ash\n      \n      \n      \n    \n    \n      3\n      alcalinity_of_ash\n      \n      \n      \n    \n    \n      4\n      magnesium\n      \n      \n      \n    \n    \n      5\n      total_phenols\n      \n      \n      \n    \n    \n      6\n      flavanoids\n      \n      \n      \n    \n    \n      7\n      nonflavanoid_phenols\n      \n      \n      \n    \n    \n      8\n      proanthocyanins\n      \n      \n      \n    \n    \n      9\n      color_intensity\n      \n      \n      \n    \n    \n      10\n      hue\n      \n      \n      \n    \n    \n      11\n      od280/od315_of_diluted_wines\n      \n      \n      \n    \n    \n      12\n      proline\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\n\nclf.fit()\nclf.get_results()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:02<00:00,  4.17it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc_ovr\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      1.000000\n      0.983175\n      0.982828\n      0.983810\n      0.982571\n      0.003325\n      0.002601\n    \n    \n      RandomForestClassifier\n      0.999336\n      0.971905\n      0.973216\n      0.974098\n      0.972726\n      0.045176\n      0.007993\n    \n    \n      HistGradientBoostingClassifier\n      0.999311\n      0.971746\n      0.970350\n      0.976508\n      0.972109\n      0.243912\n      0.021794\n    \n    \n      SVC\n      0.999128\n      0.960635\n      0.960133\n      0.965079\n      0.960681\n      0.002728\n      0.002654\n    \n    \n      GaussianNB\n      0.998855\n      0.971905\n      0.973533\n      0.974098\n      0.972720\n      0.001754\n      0.002812\n    \n    \n      XGBClassifier\n      0.998213\n      0.949206\n      0.956410\n      0.950548\n      0.950512\n      0.021561\n      0.003983\n    \n    \n      KNeighborsClassifier\n      0.995903\n      0.960794\n      0.959845\n      0.965079\n      0.960468\n      0.001666\n      0.003280\n    \n    \n      DecisionTreeClassifier\n      0.945058\n      0.927302\n      0.933483\n      0.927961\n      0.928931\n      0.001852\n      0.002390\n    \n    \n      DummyClassifier\n      0.500000\n      0.399048\n      0.133016\n      0.333333\n      0.190095\n      0.001453\n      0.002623\n    \n  \n\n\n\n\n\nfig = clf.plot.confusion_matrix(estimator_name=\"LogisticRegression\")\nfig.show(\"notebook\")\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 39.65it/s]"
  },
  {
    "objectID": "00_nutshell.html#intended-use",
    "href": "00_nutshell.html#intended-use",
    "title": "Poniard in a nutshell: getting started",
    "section": "Intended use",
    "text": "Intended use\nIn the real world where real data lives, building machine learning models is not as simple as running an abstraction in 2 lines and calling it a day.\nOur preferred way of working is splitting the data in train-test sets before touching Poniard. That way you can pass the training data to setup and let the inbuilt cross validation handle model evaluation.\nWhen you are done, get_estimator simply returns a pipeline by name and optionally trains it with full data (which should be just training data), making it easy to continue working on models while preserving an unseen test set which can now be used to assess generalization power.\n\nclf.get_estimator(\"RandomForestClassifier\", retrain=False)\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(random_state=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: PipelinePipeline(steps=[('numeric_imputer', SimpleImputer()),\n                ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()VarianceThresholdVarianceThreshold()RandomForestClassifierRandomForestClassifier(random_state=0)"
  },
  {
    "objectID": "estimators.core.html",
    "href": "estimators.core.html",
    "title": "Base estimator",
    "section": "",
    "text": "PoniardBaseEstimator (estimators:Optional[Union[Sequence[ClassifierMixin]\n                       ,Dict[str,ClassifierMixin],Sequence[RegressorMixin]\n                       ,Dict[str,RegressorMixin]]]=None, metrics:Optional[\n                       Union[str,Dict[str,Callable],Sequence[str]]]=None,\n                       preprocess:bool=True,\n                       scaler:Optional[Union[str,TransformerMixin]]=None, \n                       high_cardinality_encoder:Optional[Union[str,Transfo\n                       rmerMixin]]=None, numeric_imputer:Optional[Union[st\n                       r,TransformerMixin]]=None, custom_preprocessor:Unio\n                       n[None,Pipeline,TransformerMixin]=None,\n                       numeric_threshold:Union[int,float]=0.1,\n                       cardinality_threshold:Union[int,float]=20, cv:Union\n                       [int,BaseCrossValidator,BaseShuffleSplit,Sequence]=\n                       None, verbose:int=0,\n                       random_state:Optional[int]=None,\n                       n_jobs:Optional[int]=None,\n                       plugins:Optional[Sequence[Any]]=None,\n                       plot_options:Optional[PoniardPlotFactory]=None,\n                       cache_transformations:bool=False)\n\nBase estimator that sets up all the functionality for the classifier and regressor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimators\nOptional[Union[Sequence[ClassifierMixin], Dict[str, ClassifierMixin], Sequence[RegressorMixin], Dict[str, RegressorMixin]]]\nNone\nEstimators to evaluate.\n\n\nmetrics\nOptional[Union[str, Dict[str, Callable], Sequence[str]]]\nNone\nMetrics to compute for each estimator. This is more restrictive than sklearn’s scoringparameter, as it does not allow callable scorers. Single strings are cast to listsautomatically.\n\n\npreprocess\nbool\nTrue\nIf True, impute missing values, standard scale numeric data and one-hot or ordinalencode categorical data.\n\n\nscaler\nOptional[Union[str, TransformerMixin]]\nNone\nNumeric scaler method. Either “standard”, “minmax”, “robust” or scikit-learn Transformer.\n\n\nhigh_cardinality_encoder\nOptional[Union[str, TransformerMixin]]\nNone\nEncoder for categorical features with high cardinality. Either “target” or “ordinal”,or scikit-learn Transformer.\n\n\nnumeric_imputer\nOptional[Union[str, TransformerMixin]]\nNone\nImputation method. Either “simple”, “iterative” or scikit-learn Transformer.\n\n\ncustom_preprocessor\nUnion[None, Pipeline, TransformerMixin]\nNone\nPreprocessor used instead of the default preprocessing pipeline. It must be able to beincluded directly in a scikit-learn Pipeline.\n\n\nnumeric_threshold\nUnion[int, float]\n0.1\nNumber features with unique values above a certain threshold will be treated as numeric. Iffloat, the threshold is numeric_threshold * samples.\n\n\ncardinality_threshold\nUnion[int, float]\n20\nNon-number features with cardinality above a certain threshold will be treated asordinal encoded instead of one-hot encoded. If float, the threshold iscardinality_threshold * samples.\n\n\ncv\nUnion[int, BaseCrossValidator, BaseShuffleSplit, Sequence]\nNone\nCross validation strategy. Either an integer, a scikit-learn cross validation object,or an iterable.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\nOptional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\nOptional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction.\n\n\nplugins\nOptional[Sequence[Any]]\nNone\nPlugin instances that run in set moments of setup, fit and plotting.\n\n\nplot_options\nOptional[PoniardPlotFactory]\nNone\n:class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly formatoptions or None, which sets the default factory.\n\n\ncache_transformations\nbool\nFalse\nWhether to cache transformations and set the memory parameter for Pipelines. This can speed up slow transformations as they are not recalculated for each estimator."
  },
  {
    "objectID": "estimators.core.html#estimators-metrics-and-cv",
    "href": "estimators.core.html#estimators-metrics-and-cv",
    "title": "Base estimator",
    "section": "estimators, metrics and cv",
    "text": "estimators, metrics and cv\nPoniard estimators’ main parameters can be grouped in the following way:\n\nEstimators.\nPreprocessing parameters.\n\nImputers\nNumeric scaler\nCategorical encoder\nCustom preprocessor\n\nMetrics.\nCross validation strategy.\nRest.\n\nThese give a good amount of flexibility while providing sane defaults, so that after initialization only setup and fit have to be called in order to train multiple models.\nestimators takes a scikit-learn-compatible estimator, array of estimators or dict of name: estimators.\n\nfrom poniard import PoniardRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n\nestimators = [LinearRegression(), Ridge()]\nPoniardRegressor(estimators)\n\nPoniardRegressor(estimators=[LinearRegression(), Ridge()], metrics=None,\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=0.1,\n    cardinality_threshold=20, cv=None, verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())"
  },
  {
    "objectID": "estimators.core.html#setup",
    "href": "estimators.core.html#setup",
    "title": "Base estimator",
    "section": "setup",
    "text": "setup\nsetup takes features and target as parameters, while fit does not accept any. This runs contrary to the established convention defined by scikit-learn where there is no setting up to do and fit takes the data as params.\nThis is because Poniard does not only fit the models, but also infer features types and create the preprocesor based on these types. While this could all be stuffed inside fit (that was the case initially), having it separated allows the user to check whether Poniard’s assumptions are correct and adjust if needed before running fit, which can take long depending on how many models were passed to estimators, the cross validation strategy and the size of the dataset.\n\n\nPoniardBaseEstimator.setup\n\n PoniardBaseEstimator.setup\n                             (X:Union[pandas.core.frame.DataFrame,numpy.nd\n                             array,List], y:Union[pandas.core.frame.DataFr\n                             ame,numpy.ndarray,List])\n\nOrchestrator.\nConverts inputs to arrays if necessary, sets metrics, preprocessor, cv and pipelines.\nAfter running setup, both X and y will be held as attributes.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nFeatures.\n\n\ny\nUnion[pd.DataFrame, np.ndarray, List]\nTarget\n\n\nReturns\nPoniardBaseEstimator\n\n\n\n\n\n\nAn example\nLet’s load some random data and setup a PoniardClassifier, which inherits from PoniardBaseEstimator.\n\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n                     \"age\": rng.uniform(1, 200, 500).astype(int),\n                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n                     \"rating\": random.choices(range(50), k=500),\n                     \"target\": random.choices([0, 1], k=500)})\ndata.head()\n\n\n\n\n\n  \n    \n      \n      type\n      age\n      date\n      rating\n      target\n    \n  \n  \n    \n      0\n      apartment\n      127\n      2022-01-31\n      1\n      1\n    \n    \n      1\n      apartment\n      54\n      2022-02-28\n      17\n      1\n    \n    \n      2\n      house\n      9\n      2022-03-31\n      0\n      1\n    \n    \n      3\n      house\n      4\n      2022-04-30\n      48\n      1\n    \n    \n      4\n      apartment\n      162\n      2022-05-31\n      40\n      0\n    \n  \n\n\n\n\nsetup will conveniently output information about the data so it can be reviewed.\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: binary\nShape: (500,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\n\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\n\n\nAttributes available after setup\nAfter passing data to Poniard estimators through setup, multiple attributes become available.\ninferred_types is a DataFrame that sorts features in 4 categories (numeric, categorical_high, categorical_low and datetime) using some basic heuristics.\nThese depend on the feature dtypes, and numeric_threshold and cardinality_threshold which are set during PoniardBaseEstimator’s construction.\n\npnd.inferred_types\n\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\nThe preprocessor in turn depends on inferred_types, and the scaler, numeric_imputer and high_cardinality_encoder parameters passed to the Poniard estimator init.\nAs will be seen further on, this preprocessor can be modified significantly to fit multiple use cases and datasets.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nEach estimator has a set of default metrics, but others can be passed during construction.\n\npnd.metrics\n\n['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n\n\nLikewise, cv has sane defaults but can be modified accordingly.\n\npnd.cv\n\nStratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\n\ntarget_info lists information about y.\n\npnd.target_info\n\n{'type_': 'binary', 'ndim': 1, 'shape': (500,), 'nunique': 2}\n\n\npipelines is a dict containing each pipeline which will be trained during fit. Each Poniard estimator has a limited set of default estimators.\n\npnd.pipelines[\"SVC\"]\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()SVCSVC(kernel='linear', probability=True, random_state=0, verbose=0)"
  },
  {
    "objectID": "estimators.core.html#fit-and-get_results",
    "href": "estimators.core.html#fit-and-get_results",
    "title": "Base estimator",
    "section": "fit and get_results",
    "text": "fit and get_results\nBecause features and target are passed to the Poniard estimator, fit does not take any parameters. Its main purpose is to run sklearn’s cross_validate function on each pipeline, scoring each metrics with the cv strategy, and store the results.\n\npnd.fit()\n\nCompleted: 100%|██████████| 9/9 [00:15<00:00,  1.69s/it]                     \n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nAfter fitting pipelines, cross validated results can be accessed by running get_results\n\n\nPoniardBaseEstimator.get_results\n\n PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n                                   std:bool=False, wrt_dummy:bool=False)\n\nReturn dataframe containing scoring results. By default returns the mean score and fit and score times. Optionally returns standard deviations as well.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_train_scores\nbool\nFalse\nIf False, only return test scores.\n\n\nstd\nbool\nFalse\nWhether to return standard deviation of the scores. Default False.\n\n\nwrt_dummy\nbool\nFalse\nWhether to compute each score/time with respect to the dummy estimator results. DefaultFalse.\n\n\nReturns\nUnion[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]\n\nResults\n\n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.510256\n      0.510\n      0.531145\n      0.503846\n      0.516707\n      0.016876\n      0.013337\n    \n    \n      DummyClassifier\n      0.500000\n      0.520\n      0.520000\n      1.000000\n      0.684211\n      0.014292\n      0.010856\n    \n    \n      KNeighborsClassifier\n      0.496675\n      0.492\n      0.509150\n      0.534615\n      0.519465\n      0.016555\n      0.016932\n    \n    \n      SVC\n      0.472356\n      0.476\n      0.499007\n      0.688462\n      0.575907\n      0.821230\n      0.013713\n    \n    \n      LogisticRegression\n      0.468990\n      0.488\n      0.509234\n      0.573077\n      0.536862\n      0.039569\n      0.037738\n    \n    \n      XGBClassifier\n      0.460417\n      0.486\n      0.502401\n      0.500000\n      0.499330\n      0.071057\n      0.015129\n    \n    \n      HistGradientBoostingClassifier\n      0.456571\n      0.488\n      0.505975\n      0.484615\n      0.494283\n      1.556914\n      0.042196\n    \n    \n      RandomForestClassifier\n      0.435056\n      0.462\n      0.479861\n      0.476923\n      0.477449\n      0.107180\n      0.020063\n    \n    \n      GaussianNB\n      0.423317\n      0.468\n      0.492473\n      0.565385\n      0.525371\n      0.015108\n      0.010184\n    \n  \n\n\n\n\n\nmeans, stds = pnd.get_results(std=True, return_train_scores=True)\nstds\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      train_roc_auc\n      test_accuracy\n      train_accuracy\n      test_precision\n      train_precision\n      test_recall\n      train_recall\n      test_f1\n      train_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.060706\n      0.000000e+00\n      0.060332\n      0.000000\n      0.059942\n      0.000000\n      0.058835\n      0.000000\n      0.057785\n      0.000000\n      0.001712\n      0.001982\n    \n    \n      DummyClassifier\n      0.000000\n      0.000000e+00\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.004226\n      0.001764\n    \n    \n      KNeighborsClassifier\n      0.021105\n      8.429609e-03\n      0.019391\n      0.010840\n      0.019140\n      0.008157\n      0.081043\n      0.022053\n      0.049760\n      0.012869\n      0.002909\n      0.007211\n    \n    \n      SVC\n      0.038609\n      3.600720e-02\n      0.042708\n      0.032496\n      0.031965\n      0.028405\n      0.085485\n      0.073140\n      0.036968\n      0.026864\n      0.110120\n      0.004044\n    \n    \n      LogisticRegression\n      0.068079\n      2.545484e-02\n      0.041183\n      0.027946\n      0.037992\n      0.024759\n      0.065948\n      0.021371\n      0.036585\n      0.022583\n      0.009066\n      0.044312\n    \n    \n      XGBClassifier\n      0.065278\n      0.000000e+00\n      0.035553\n      0.000000\n      0.033315\n      0.000000\n      0.091826\n      0.000000\n      0.061108\n      0.000000\n      0.008010\n      0.003623\n    \n    \n      HistGradientBoostingClassifier\n      0.059681\n      7.749323e-04\n      0.041183\n      0.007483\n      0.039938\n      0.011912\n      0.070291\n      0.005607\n      0.054859\n      0.007046\n      0.600654\n      0.006484\n    \n    \n      RandomForestClassifier\n      0.060809\n      7.021667e-17\n      0.039192\n      0.000000\n      0.038392\n      0.000000\n      0.077307\n      0.000000\n      0.056132\n      0.000000\n      0.025762\n      0.002754\n    \n    \n      GaussianNB\n      0.045845\n      2.494438e-02\n      0.042143\n      0.018303\n      0.037330\n      0.015830\n      0.031246\n      0.038051\n      0.025456\n      0.018727\n      0.004558\n      0.001748\n    \n  \n\n\n\n\n\n\n\nPoniardBaseEstimator.reassign_types\n\n PoniardBaseEstimator.reassign_types\n                                      (numeric:Union[List[Union[str,int]],\n                                      NoneType]=None, categorical_high:Uni\n                                      on[List[Union[str,int]],NoneType]=No\n                                      ne, categorical_low:Union[List[Union\n                                      [str,int]],NoneType]=None, datetime:\n                                      Union[List[Union[str,int]],NoneType]\n                                      =None)\n\nReassign feature types.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnumeric\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_high\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_low\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ndatetime\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\nReturns\nPoniardBaseEstimator\n\nself."
  },
  {
    "objectID": "01_main_parameters.html",
    "href": "01_main_parameters.html",
    "title": "Main parameters",
    "section": "",
    "text": "At the core of Poniard lie the choice of estimators, metrics and CV strategy. While defaults might work for most cases, we try to keep it flexible."
  },
  {
    "objectID": "01_main_parameters.html#estimators",
    "href": "01_main_parameters.html#estimators",
    "title": "Main parameters",
    "section": "estimators",
    "text": "estimators\nEstimators can be passed as a dict of estimator_name: estimator_instance, as a sequence of estimator_instance or as a single estimator. When not specifying names, they will be obtained directly from the class.\nUsing a dictionary allows passing multiple instances of the same estimator with different parameters.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom poniard import PoniardClassifier\n\n\nX, y = make_classification(n_classes=3, n_features=5, n_informative=3)\npnd = PoniardClassifier(estimators={\"lr\": LogisticRegression(max_iter=5000),\n                                    \"lr_no_penalty\": LogisticRegression(max_iter=5000, penalty=\"none\"),\n                                    \"lda\": LinearDiscriminantAnalysis()})\npnd.setup(X, y)\npnd.fit()\n\nTarget info\n-----------\nType: multiclass\nShape: (100,)\nUnique values: 3\n\nMain metric\n-----------\nroc_auc_ovr\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 10\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      \n      \n      \n    \n    \n      1\n      1.0\n      \n      \n      \n    \n    \n      2\n      2.0\n      \n      \n      \n    \n    \n      3\n      3.0\n      \n      \n      \n    \n    \n      4\n      4.0\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nCompleted: 100%|██████████████████████████████████| 4/4 [00:00<00:00, 39.74it/s]\n\n\nPoniardClassifier(estimators={'lr': LogisticRegression(max_iter=5000, random_state=0), 'lr_no_penalty': LogisticRegression(max_iter=5000, penalty='none', random_state=0), 'lda': LinearDiscriminantAnalysis()}, metrics=['roc_auc_ovr', 'accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=10,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nSince we are in scikit-learn-land, most of the stuff you expect to work still works. For example, multilabel classification.\nHere we had to use a dictionary because estimator.__class__.__name__, which is used for assigning a name to each estimator when a list is passed, would be the same for both OneVsRestClassifier and they would be overwritten.\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nX, y = make_multilabel_classification(n_samples=1000, n_features=6)\npnd = PoniardClassifier(estimators={\"rf\": OneVsRestClassifier(RandomForestClassifier()),\n                                    \"lr\": OneVsRestClassifier(LogisticRegression(max_iter=5000))})\npnd.setup(X, y)\npnd.fit()\n\nTarget info\n-----------\nType: multilabel-indicator\nShape: (1000, 5)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 100\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      \n      0.0\n      \n      \n    \n    \n      1\n      \n      1.0\n      \n      \n    \n    \n      2\n      \n      2.0\n      \n      \n    \n    \n      3\n      \n      3.0\n      \n      \n    \n    \n      4\n      \n      4.0\n      \n      \n    \n    \n      5\n      \n      5.0\n      \n      \n    \n  \n\n\n\n/Users/rafxavier/Documents/Repos/personal/poniard/poniard/estimators/core.py:265: UserWarning: TargetEncoder is not supported for multilabel or multioutput targets. Switching to OrdinalEncoder.\n  self.preprocessor = self._build_preprocessor()\n\n\n\n\n\n\nCompleted: 100%|██████████████████████████████████| 3/3 [00:02<00:00,  1.14it/s]\n\n\nPoniardClassifier(estimators={'rf': OneVsRestClassifier(estimator=RandomForestClassifier()), 'lr': OneVsRestClassifier(estimator=LogisticRegression(max_iter=5000))}, metrics=['roc_auc', 'accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=100,\n    cardinality_threshold=20, cv=KFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      rf\n      0.845663\n      0.450\n      0.691683\n      0.615753\n      0.629569\n      0.356053\n      0.045371\n    \n    \n      lr\n      0.840438\n      0.368\n      0.625530\n      0.588408\n      0.604364\n      0.018141\n      0.004137\n    \n    \n      DummyClassifier\n      0.500000\n      0.077\n      0.124400\n      0.240000\n      0.163723\n      0.001717\n      0.003786"
  },
  {
    "objectID": "01_main_parameters.html#metrics",
    "href": "01_main_parameters.html#metrics",
    "title": "Main parameters",
    "section": "metrics",
    "text": "metrics\nMetrics can be passed as a list of strings such as \"accuracy\" or \"neg_mean_squared_error\", following the familiar scikit-learn nomenclature, or as a dict of str: Callable. For convenience, it can also be a single string.\nHowever, in a departure from scikit-learn, metrics will fail if a Callable is passed directly. This restriction is in place to facilitate naming columns in the get_results() method.\n\n\n\n\n\n\nscoring vs. metrics parameters\n\n\n\nIn scikit-learn parlance, a metric is a measure of the prediction error of a model. Scores, which is used in sklearn model evaluation objects (like GridSearchCV or cross_validate), reflects the same, but with the restriction that higher values are better than lower values. Poniard uses the parameter name metrics for now, but will eventually migrate to scoring as that reflects more accurately its meaning.\n\n\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom poniard import PoniardRegressor\n\n\nX, y = make_regression(n_samples=500, n_features=10, n_informative=5)\npnd = PoniardRegressor(metrics=[\"explained_variance\", \"neg_median_absolute_error\"],\n                       estimators=[LinearRegression()])\npnd.setup(X, y)\npnd.fit()\npnd.get_results(return_train_scores=True)\n\nTarget info\n-----------\nType: continuous\nShape: (500,)\nUnique values: 500\n\nMain metric\n-----------\nexplained_variance\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      \n      \n      \n    \n    \n      1\n      1.0\n      \n      \n      \n    \n    \n      2\n      2.0\n      \n      \n      \n    \n    \n      3\n      3.0\n      \n      \n      \n    \n    \n      4\n      4.0\n      \n      \n      \n    \n    \n      5\n      5.0\n      \n      \n      \n    \n    \n      6\n      6.0\n      \n      \n      \n    \n    \n      7\n      7.0\n      \n      \n      \n    \n    \n      8\n      8.0\n      \n      \n      \n    \n    \n      9\n      9.0\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nCompleted: 100%|█████████████████████████████████| 2/2 [00:00<00:00, 100.18it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_explained_variance\n      train_explained_variance\n      test_neg_median_absolute_error\n      train_neg_median_absolute_error\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      1.0\n      1.0\n      -7.105427e-14\n      -7.620571e-14\n      0.001239\n      0.000349\n    \n    \n      DummyRegressor\n      0.0\n      0.0\n      -7.941050e+01\n      -7.808466e+01\n      0.000688\n      0.000266\n    \n  \n\n\n\n\n\nfrom sklearn.metrics import r2_score, make_scorer\nfrom sklearn.linear_model import Ridge\n\n\ndef scaled_r2(y_true, y_pred):\n    return round(r2_score(y_true, y_pred) * 100, 1)\n\npnd = PoniardRegressor(metrics={\"scaled_r2\": make_scorer(scaled_r2, greater_is_better=True),\n                                \"usual_r2\": make_scorer(r2_score, greater_is_better=True)},\n                       estimators=[LinearRegression(), Ridge()])\npnd.setup(X, y).fit().get_results()\n\nTarget info\n-----------\nType: continuous\nShape: (500,)\nUnique values: 500\n\nMain metric\n-----------\nscaled_r2\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      \n      \n      \n    \n    \n      1\n      1.0\n      \n      \n      \n    \n    \n      2\n      2.0\n      \n      \n      \n    \n    \n      3\n      3.0\n      \n      \n      \n    \n    \n      4\n      4.0\n      \n      \n      \n    \n    \n      5\n      5.0\n      \n      \n      \n    \n    \n      6\n      6.0\n      \n      \n      \n    \n    \n      7\n      7.0\n      \n      \n      \n    \n    \n      8\n      8.0\n      \n      \n      \n    \n    \n      9\n      9.0\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nCompleted: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 104.79it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_scaled_r2\n      test_usual_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      100.0\n      1.000000\n      0.001284\n      0.000308\n    \n    \n      Ridge\n      100.0\n      0.999993\n      0.000779\n      0.000266\n    \n    \n      DummyRegressor\n      -1.3\n      -0.012921\n      0.000727\n      0.000278\n    \n  \n\n\n\n\nThe order in which scorers are passed matters; to be precise, the first scorer will be used in some methods if no other metric is defined.\n\nprint(pnd.metrics)\nfig = pnd.plot.permutation_importance(\"Ridge\")\nfig.show(\"notebook\")\n\n{'scaled_r2': make_scorer(scaled_r2), 'usual_r2': make_scorer(r2_score)}"
  },
  {
    "objectID": "01_main_parameters.html#cv",
    "href": "01_main_parameters.html#cv",
    "title": "Main parameters",
    "section": "cv",
    "text": "cv\nCross validation can be anything that scikit-learn accepts. By default, classification tasks will be paired with a StratifiedKFold if the target is binary, and KFold otherwise. Regression tasks use KFold by default.\ncv=int or cv=None are internally converted to one of the above classes so that Poniard’s random_state parameter can be passed on.\n\nfrom IPython.utils import io\nfrom sklearn.model_selection import RepeatedKFold\n\n\nwith io.capture_output() as c:\n    pnd_5 = PoniardRegressor(cv=4).setup(X, y)\n    pnd_none = PoniardRegressor(cv=None).setup(X, y)\n    pnd_k = PoniardRegressor(cv=RepeatedKFold(n_splits=3)).setup(X, y)\n    \nprint(pnd_5.cv, pnd_none.cv, pnd_k.cv, sep=\"\\n\")\n\nKFold(n_splits=4, random_state=0, shuffle=True)\nKFold(n_splits=5, random_state=0, shuffle=True)\nRepeatedKFold(n_repeats=10, n_splits=3, random_state=0)\n\n\nNote that even though we didn’t specify random_state for the third estimator, it gets injected during setup."
  },
  {
    "objectID": "02_preprocessing.html#use-a-custom-preprocessor",
    "href": "02_preprocessing.html#use-a-custom-preprocessor",
    "title": "Preprocessing",
    "section": "Use a custom preprocessor",
    "text": "Use a custom preprocessor\nDuring init of either PoniardRegressor or PoniardClassifier (see docs for PoniardBaseEstimator which sets up most of the functionality), preprocess=False disables preprocessing altogether, while custom_preprocessor accepts a scikit-learn transformer (or pipeline/column transformer) that replaces the default Poniard transformation pipeline.\nLogically, there is no type inference involved when these options are used and full control is given to the user.\nIn the following example, we use TfidfVectorizer() to process the 20 News Groups dataset.\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\n\nfrom poniard import PoniardClassifier\n\n\nX, y = fetch_20newsgroups(return_X_y=True, remove=(\"headers\", \"footers\", \"quotes\"),\n                          categories=(\"sci.crypt\", \"sci.electronics\", \"sci.med\"))\npreprocessor = make_pipeline(TfidfVectorizer(), Normalizer())\npnd = PoniardClassifier(estimators=[LogisticRegression()], custom_preprocessor=preprocessor)\npnd.setup(X, y)\npnd.preprocessor\n\nTarget info\n-----------\nType: multiclass\nShape: (1780,)\nUnique values: 3\n\nMain metric\n-----------\nroc_auc_ovr\n\n\n\nPipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('normalizer', Normalizer())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('normalizer', Normalizer())])TfidfVectorizerTfidfVectorizer()NormalizerNormalizer()\n\n\n\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 2/2 [00:12<00:00,  6.12s/it]\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc_ovr\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      0.976337\n      0.888202\n      0.896170\n      0.888335\n      0.888822\n      0.718083\n      0.140723\n    \n    \n      DummyClassifier\n      0.500000\n      0.334270\n      0.111423\n      0.333333\n      0.167018\n      0.339606\n      0.139167"
  },
  {
    "objectID": "02_preprocessing.html#modifying-the-default-preprocessor",
    "href": "02_preprocessing.html#modifying-the-default-preprocessor",
    "title": "Preprocessing",
    "section": "Modifying the default preprocessor",
    "text": "Modifying the default preprocessor"
  },
  {
    "objectID": "02_preprocessing.html#default-preprocessing-pipeline",
    "href": "02_preprocessing.html#default-preprocessing-pipeline",
    "title": "Preprocessing",
    "section": "Default preprocessing pipeline",
    "text": "Default preprocessing pipeline\nThe list of default transformations is:\n\nMissing data imputation.\nZ-score scaling for numeric variables.\nOne-hot encoding for low cardinality categorical variables.\nTarget encoding for the remaining categorical variables. This is a custom transformer based on Micci-Barreca, 2001, with implementation heavily based on Dirty Cat. If the task is multilabel or multioutput, ordinal encoding will be used instead.\nDatetime encoding for datetime variables. This also uses a custom transformer that extracts multiple datetime levels.\nZero-variance feature elimination.\n\nThis includes some type inference logic that decides whether a given feature is either numeric, categorical high cardinality, categorical low cardinality or datetime (see inferred_types).\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n                     \"age\": rng.uniform(1, 200, 500).astype(int),\n                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n                     \"rating\": random.choices(range(50), k=500),\n                     \"target\": random.choices([0, 1], k=500)})\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier().setup(X, y)\npnd.preprocessor\n\nTarget info\n-----------\nType: binary\nShape: (500,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()"
  },
  {
    "objectID": "00_nutshell.html#plots-when-get_results-is-not-enough",
    "href": "00_nutshell.html#plots-when-get_results-is-not-enough",
    "title": "Poniard in a nutshell: getting started",
    "section": "Plots: when get_results is not enough",
    "text": "Plots: when get_results is not enough\nWhile a nicely formatted table is useful, graphical aid can make things go a lot smoother. Poniard estimators include a plot accessor that gives access to multiple prebuilt plots.\nRefer to the plotting guide or the plotting reference for a deeper dive.\n\nfig = pnd.plot.metrics(kind=\"bar\", metrics=[\"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\"])\nfig.show(\"notebook\")\n\n\n                                                \n\n\n\nfig = pnd.plot.residuals_histogram(estimator_names=[\"LinearRegression\", \"SGDRegressor\"])\nfig.show(\"notebook\")\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 34.04it/s]\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00,  1.78it/s]\n\n\n\n                                                \n\n\nBy using Plotly, users can have an easier time exploring charts, zooming in, selecting specific models, etc."
  },
  {
    "objectID": "00_nutshell.html#plugins-extending-poniard",
    "href": "00_nutshell.html#plugins-extending-poniard",
    "title": "Poniard in a nutshell: getting started",
    "section": "Plugins: extending Poniard",
    "text": "Plugins: extending Poniard"
  },
  {
    "objectID": "guide/02_preprocessing.html",
    "href": "guide/02_preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Poniard tries to apply minimal preprocessing to data. In general, it just tries to make sure that models fit correctly without introducing signifcant transformation overhead. In particular, there is no anomaly detection, dimensionality reduction, clustering, resampling, feature creation from polynomial interactions, feature selection, etc.\nThis is so the user always knows what’s going on.\nHowever, the default options may not be suitable for your data or objectives, so these can be set during initialization or modified afterwards."
  },
  {
    "objectID": "guide/02_preprocessing.html#default-preprocessing-pipeline",
    "href": "guide/02_preprocessing.html#default-preprocessing-pipeline",
    "title": "Preprocessing",
    "section": "Default preprocessing pipeline",
    "text": "Default preprocessing pipeline\nThe list of default transformations is:\n\nMissing data imputation.\nZ-score scaling for numeric variables.\nOne-hot encoding for low cardinality categorical variables.\nTarget encoding for the remaining categorical variables. This is a custom transformer based on Micci-Barreca, 2001, with implementation heavily based on Dirty Cat. If the task is multilabel or multioutput, ordinal encoding will be used instead.\nDatetime encoding for datetime variables. This also uses a custom transformer that extracts multiple datetime levels.\nZero-variance feature elimination.\n\nThis includes some type inference logic that decides whether a given feature is either numeric, categorical high cardinality, categorical low cardinality or datetime (see inferred_types).\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n                     \"age\": rng.uniform(1, 200, 500).astype(int),\n                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n                     \"rating\": random.choices(range(50), k=500),\n                     \"target\": random.choices([0, 1], k=500)})\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier().setup(X, y)\npnd.preprocessor\n\nTarget info\n-----------\nType: binary\nShape: (500,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\n\n\n\n\n\n\nEmpty subpreprocessors\n\n\n\nIf no features are assigned to a subpreprocessor (like datetime_preprocessor or categorical_low_preprocessor), then it will be dropped. This does not affect results as scikit-learn effectively ignores transformers with no assigned features, but it makes the HTML representation cleaner."
  },
  {
    "objectID": "guide/02_preprocessing.html#use-a-custom-preprocessor",
    "href": "guide/02_preprocessing.html#use-a-custom-preprocessor",
    "title": "Preprocessing",
    "section": "Use a custom preprocessor",
    "text": "Use a custom preprocessor\nDuring init of either PoniardRegressor or PoniardClassifier (see docs for PoniardBaseEstimator which sets up most of the functionality), preprocess=False disables preprocessing altogether, while custom_preprocessor accepts a scikit-learn transformer (or pipeline/column transformer) that replaces the default Poniard transformation pipeline.\nLogically, there is no type inference involved when these options are used and full control is given to the user.\nIn the following example, we use TfidfVectorizer and Normalizer to process the 20 News Groups dataset.\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\n\nfrom poniard import PoniardClassifier\n\n\nX, y = fetch_20newsgroups(return_X_y=True, remove=(\"headers\", \"footers\", \"quotes\"),\n                          categories=(\"sci.crypt\", \"sci.electronics\", \"sci.med\"))\npreprocessor = make_pipeline(TfidfVectorizer(), Normalizer())\npnd = PoniardClassifier(estimators=[LogisticRegression()], custom_preprocessor=preprocessor)\npnd.setup(X, y)\npnd.preprocessor\n\nTarget info\n-----------\nType: multiclass\nShape: (1780,)\nUnique values: 3\n\nMain metric\n-----------\nroc_auc_ovr\n\n\n\nPipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('normalizer', Normalizer())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('normalizer', Normalizer())])TfidfVectorizerTfidfVectorizer()NormalizerNormalizer()\n\n\n\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 2/2 [00:12<00:00,  6.26s/it]\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc_ovr\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      0.976337\n      0.888202\n      0.896170\n      0.888335\n      0.888822\n      0.732272\n      0.148423\n    \n    \n      DummyClassifier\n      0.500000\n      0.334270\n      0.111423\n      0.333333\n      0.167018\n      0.343316\n      0.142217"
  },
  {
    "objectID": "guide/02_preprocessing.html#modifying-the-default-preprocessor",
    "href": "guide/02_preprocessing.html#modifying-the-default-preprocessor",
    "title": "Preprocessing",
    "section": "Modifying the default preprocessor",
    "text": "Modifying the default preprocessor\nCombining properly setup feature types with the scaler, numeric_imputer and high_cardinality_encoder parameters allows almost complete customization of the default preprocessing pipeline.\nThese three parameters take strings representing transformers (as in scaler=\"minmax\" will use scikit-learn’s MinMaxScaler, see the reference), and also accept scikit-learn transformers and pipelines.\nFor now, we are deliberately not providing options for the categorical imputer (a SimpleImputer(strategy=\"most_frequent\") is used) or the low cardinality categorical encoder (always OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\", sparse=False)). While this is not set in stone, we feel that these are less debatable.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.impute import KNNImputer\nfrom poniard import PoniardRegressor\n\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nreg = PoniardRegressor(numeric_imputer=KNNImputer(), scaler=\"robust\")\nreg.setup(X, y)\nreg.reassign_types(numeric=[\"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"],\n                   categorical_high=[\"HouseAge\"])\nreg.preprocessor\n\nTarget info\n-----------\nType: continuous\nShape: (20640,)\nUnique values: 3842\n\nMain metric\n-----------\nneg_mean_squared_error\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 2064\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      MedInc\n      HouseAge\n      \n      \n    \n    \n      1\n      AveRooms\n      Latitude\n      \n      \n    \n    \n      2\n      AveBedrms\n      Longitude\n      \n      \n    \n    \n      3\n      Population\n      \n      \n      \n    \n    \n      4\n      AveOccup\n      \n      \n      \n    \n  \n\n\n\n\n\nAssigned feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      AveRooms\n      HouseAge\n      \n      \n    \n    \n      1\n      AveBedrms\n      \n      \n      \n    \n    \n      2\n      Population\n      \n      \n      \n    \n    \n      3\n      AveOccup\n      \n      \n      \n    \n    \n      4\n      Latitude\n      \n      \n      \n    \n    \n      5\n      Longitude\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   KNNImputer()),\n                                                                  ('scaler',\n                                                                   RobustScaler())]),\n                                                  ['AveRooms', 'AveBedrms',\n                                                   'Population', 'AveOccup',\n                                                   'Latitude', 'Longitude']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   KNNImputer()),\n                                                                  ('scaler',\n                                                                   RobustScaler())]),\n                                                  ['AveRooms', 'AveBedrms',\n                                                   'Population', 'AveOccup',\n                                                   'Latitude', 'Longitude']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  KNNImputer()),\n                                                 ('scaler', RobustScaler())]),\n                                 ['AveRooms', 'AveBedrms', 'Population',\n                                  'AveOccup', 'Latitude', 'Longitude']),\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='regression'))]),\n                                 ['HouseAge'])])numeric_preprocessor['AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']KNNImputerKNNImputer()RobustScalerRobustScaler()categorical_high_preprocessor['HouseAge']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='regression')VarianceThresholdVarianceThreshold()"
  },
  {
    "objectID": "guide/01_main_parameters.html",
    "href": "guide/01_main_parameters.html",
    "title": "Main parameters",
    "section": "",
    "text": "At the core of Poniard lie the choice of estimators, metrics and CV strategy. While defaults might work for most cases, we try to keep it flexible."
  },
  {
    "objectID": "guide/01_main_parameters.html#estimators",
    "href": "guide/01_main_parameters.html#estimators",
    "title": "Main parameters",
    "section": "estimators",
    "text": "estimators\nEstimators can be passed as a dict of estimator_name: estimator_instance, as a sequence of estimator_instance or as a single estimator. When not specifying names, they will be obtained directly from the class.\nUsing a dictionary allows passing multiple instances of the same estimator with different parameters.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom poniard import PoniardClassifier\n\n\nX, y = make_classification(n_classes=3, n_features=5, n_informative=3)\npnd = PoniardClassifier(estimators={\"lr\": LogisticRegression(max_iter=5000),\n                                    \"lr_no_penalty\": LogisticRegression(max_iter=5000, penalty=\"none\"),\n                                    \"lda\": LinearDiscriminantAnalysis()})\npnd.setup(X, y)\npnd.fit()\n\nTarget info\n-----------\nType: multiclass\nShape: (100,)\nUnique values: 3\n\nMain metric\n-----------\nroc_auc_ovr\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 10\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      \n      \n      \n    \n    \n      1\n      1.0\n      \n      \n      \n    \n    \n      2\n      2.0\n      \n      \n      \n    \n    \n      3\n      3.0\n      \n      \n      \n    \n    \n      4\n      4.0\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nCompleted: 100%|██████████████████████████████████| 4/4 [00:00<00:00, 39.74it/s]\n\n\nPoniardClassifier(estimators={'lr': LogisticRegression(max_iter=5000, random_state=0), 'lr_no_penalty': LogisticRegression(max_iter=5000, penalty='none', random_state=0), 'lda': LinearDiscriminantAnalysis()}, metrics=['roc_auc_ovr', 'accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=10,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nSince we are in scikit-learn-land, most of the stuff you expect to work still works. For example, multilabel classification.\nHere we had to use a dictionary because estimator.__class__.__name__, which is used for assigning a name to each estimator when a list is passed, would be the same for both OneVsRestClassifier and they would be overwritten.\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nX, y = make_multilabel_classification(n_samples=1000, n_features=6)\npnd = PoniardClassifier(estimators={\"rf\": OneVsRestClassifier(RandomForestClassifier()),\n                                    \"lr\": OneVsRestClassifier(LogisticRegression(max_iter=5000))})\npnd.setup(X, y)\npnd.fit()\n\nTarget info\n-----------\nType: multilabel-indicator\nShape: (1000, 5)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 100\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      \n      0.0\n      \n      \n    \n    \n      1\n      \n      1.0\n      \n      \n    \n    \n      2\n      \n      2.0\n      \n      \n    \n    \n      3\n      \n      3.0\n      \n      \n    \n    \n      4\n      \n      4.0\n      \n      \n    \n    \n      5\n      \n      5.0\n      \n      \n    \n  \n\n\n\n/Users/rafxavier/Documents/Repos/personal/poniard/poniard/estimators/core.py:265: UserWarning: TargetEncoder is not supported for multilabel or multioutput targets. Switching to OrdinalEncoder.\n  self.preprocessor = self._build_preprocessor()\n\n\n\n\n\n\nCompleted: 100%|██████████████████████████████████| 3/3 [00:02<00:00,  1.14it/s]\n\n\nPoniardClassifier(estimators={'rf': OneVsRestClassifier(estimator=RandomForestClassifier()), 'lr': OneVsRestClassifier(estimator=LogisticRegression(max_iter=5000))}, metrics=['roc_auc', 'accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=100,\n    cardinality_threshold=20, cv=KFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      rf\n      0.845663\n      0.450\n      0.691683\n      0.615753\n      0.629569\n      0.356053\n      0.045371\n    \n    \n      lr\n      0.840438\n      0.368\n      0.625530\n      0.588408\n      0.604364\n      0.018141\n      0.004137\n    \n    \n      DummyClassifier\n      0.500000\n      0.077\n      0.124400\n      0.240000\n      0.163723\n      0.001717\n      0.003786"
  },
  {
    "objectID": "guide/01_main_parameters.html#metrics",
    "href": "guide/01_main_parameters.html#metrics",
    "title": "Main parameters",
    "section": "metrics",
    "text": "metrics\nMetrics can be passed as a list of strings such as \"accuracy\" or \"neg_mean_squared_error\", following the familiar scikit-learn nomenclature, or as a dict of str: Callable. For convenience, it can also be a single string.\nHowever, in a departure from scikit-learn, metrics will fail if a Callable is passed directly. This restriction is in place to facilitate naming columns in the poniard.estimators.core.PoniardBaseEstimator.get_results method.\n\n\n\n\n\n\nscoring vs. metrics parameters\n\n\n\nIn scikit-learn parlance, a metric is a measure of the prediction error of a model. Scores, which is used in sklearn model evaluation objects (like GridSearchCV or cross_validate), reflects the same, but with the restriction that higher values are better than lower values. Poniard uses the parameter name metrics for now, but will eventually migrate to scoring as that reflects more accurately its meaning.\n\n\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom poniard import PoniardRegressor\n\n\nX, y = make_regression(n_samples=500, n_features=10, n_informative=5)\npnd = PoniardRegressor(metrics=[\"explained_variance\", \"neg_median_absolute_error\"],\n                       estimators=[LinearRegression()])\npnd.setup(X, y)\npnd.fit()\npnd.get_results(return_train_scores=True)\n\nTarget info\n-----------\nType: continuous\nShape: (500,)\nUnique values: 500\n\nMain metric\n-----------\nexplained_variance\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      \n      \n      \n    \n    \n      1\n      1.0\n      \n      \n      \n    \n    \n      2\n      2.0\n      \n      \n      \n    \n    \n      3\n      3.0\n      \n      \n      \n    \n    \n      4\n      4.0\n      \n      \n      \n    \n    \n      5\n      5.0\n      \n      \n      \n    \n    \n      6\n      6.0\n      \n      \n      \n    \n    \n      7\n      7.0\n      \n      \n      \n    \n    \n      8\n      8.0\n      \n      \n      \n    \n    \n      9\n      9.0\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nCompleted: 100%|█████████████████████████████████| 2/2 [00:00<00:00, 100.18it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_explained_variance\n      train_explained_variance\n      test_neg_median_absolute_error\n      train_neg_median_absolute_error\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      1.0\n      1.0\n      -7.105427e-14\n      -7.620571e-14\n      0.001239\n      0.000349\n    \n    \n      DummyRegressor\n      0.0\n      0.0\n      -7.941050e+01\n      -7.808466e+01\n      0.000688\n      0.000266\n    \n  \n\n\n\n\n\nfrom sklearn.metrics import r2_score, make_scorer\nfrom sklearn.linear_model import Ridge\n\n\ndef scaled_r2(y_true, y_pred):\n    return round(r2_score(y_true, y_pred) * 100, 1)\n\npnd = PoniardRegressor(metrics={\"scaled_r2\": make_scorer(scaled_r2, greater_is_better=True),\n                                \"usual_r2\": make_scorer(r2_score, greater_is_better=True)},\n                       estimators=[LinearRegression(), Ridge()])\npnd.setup(X, y).fit().get_results()\n\nTarget info\n-----------\nType: continuous\nShape: (500,)\nUnique values: 500\n\nMain metric\n-----------\nscaled_r2\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      0.0\n      \n      \n      \n    \n    \n      1\n      1.0\n      \n      \n      \n    \n    \n      2\n      2.0\n      \n      \n      \n    \n    \n      3\n      3.0\n      \n      \n      \n    \n    \n      4\n      4.0\n      \n      \n      \n    \n    \n      5\n      5.0\n      \n      \n      \n    \n    \n      6\n      6.0\n      \n      \n      \n    \n    \n      7\n      7.0\n      \n      \n      \n    \n    \n      8\n      8.0\n      \n      \n      \n    \n    \n      9\n      9.0\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nCompleted: 100%|█████████████████████████████████| 3/3 [00:00<00:00, 104.79it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_scaled_r2\n      test_usual_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      100.0\n      1.000000\n      0.001284\n      0.000308\n    \n    \n      Ridge\n      100.0\n      0.999993\n      0.000779\n      0.000266\n    \n    \n      DummyRegressor\n      -1.3\n      -0.012921\n      0.000727\n      0.000278\n    \n  \n\n\n\n\nThe order in which scorers are passed matters; to be precise, the first scorer will be used in some methods if no other metric is defined.\n\nprint(pnd.metrics)\nfig = pnd.plot.permutation_importance(\"Ridge\")\nfig.show(\"notebook\")\n\n{'scaled_r2': make_scorer(scaled_r2), 'usual_r2': make_scorer(r2_score)}"
  },
  {
    "objectID": "guide/01_main_parameters.html#cv",
    "href": "guide/01_main_parameters.html#cv",
    "title": "Main parameters",
    "section": "cv",
    "text": "cv\nCross validation can be anything that scikit-learn accepts. By default, classification tasks will be paired with a StratifiedKFold if the target is binary, and KFold otherwise. Regression tasks use KFold by default.\ncv=int or cv=None are internally converted to one of the above classes so that Poniard’s random_state parameter can be passed on.\n\nfrom IPython.utils import io\nfrom sklearn.model_selection import RepeatedKFold\n\n\nwith io.capture_output() as c:\n    pnd_5 = PoniardRegressor(cv=4).setup(X, y)\n    pnd_none = PoniardRegressor(cv=None).setup(X, y)\n    pnd_k = PoniardRegressor(cv=RepeatedKFold(n_splits=3)).setup(X, y)\n    \nprint(pnd_5.cv, pnd_none.cv, pnd_k.cv, sep=\"\\n\")\n\nKFold(n_splits=4, random_state=0, shuffle=True)\nKFold(n_splits=5, random_state=0, shuffle=True)\nRepeatedKFold(n_repeats=10, n_splits=3, random_state=0)\n\n\nNote that even though we didn’t specify random_state for the third estimator, it gets injected during setup."
  },
  {
    "objectID": "guide/00_nutshell.html",
    "href": "guide/00_nutshell.html",
    "title": "Poniard in a nutshell: getting started",
    "section": "",
    "text": "Essentially, a Poniard estimator is a set of scikit-learn estimators, a preprocessing strategy, a cross validation strategy and one or more metrics with which to score models.\nThe idea behind Poniard is to abstract away some of the boilerplate involved in fitting multiple models and comparing their cross validated results. However, a significant effort is made to keep everything flexible and as close to scikit-learn as possible.\nPoniard includes a PoniardClassifier and a PoniardRegressor, aligned with scikit-learn classifiers and regressors."
  },
  {
    "objectID": "guide/00_nutshell.html#basic-usage",
    "href": "guide/00_nutshell.html#basic-usage",
    "title": "Poniard in a nutshell: getting started",
    "section": "Basic usage",
    "text": "Basic usage\nIn the following example we will load a toy dataset (sklearn’s diabetes dataset, a simple regression task) and have at it with default parameters.\n\nfrom poniard import PoniardRegressor\nfrom sklearn.datasets import load_diabetes\n\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\npnd = PoniardRegressor()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: continuous\nShape: (442,)\nUnique values: 214\n\nMain metric\n-----------\nneg_mean_squared_error\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 44\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      \n      sex\n      \n    \n    \n      1\n      bmi\n      \n      \n      \n    \n    \n      2\n      bp\n      \n      \n      \n    \n    \n      3\n      s1\n      \n      \n      \n    \n    \n      4\n      s2\n      \n      \n      \n    \n    \n      5\n      s3\n      \n      \n      \n    \n    \n      6\n      s4\n      \n      \n      \n    \n    \n      7\n      s5\n      \n      \n      \n    \n    \n      8\n      s6\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPoniardRegressor(estimators=None, metrics=['neg_mean_squared_error', 'neg_mean_absolute_percentage_error', 'neg_median_absolute_error', 'r2'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=44,\n    cardinality_threshold=20, cv=KFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nOut of the box, you will get some useful information regarding the target variable and the features, as well information regarding current Poniard settings (first metric and thresholds). These are covered in detail later.\nOnce Poniard has parsed the data and built the preprocessing pipeline, we are free to run PoniardBaseEstimator.fit and PoniardBaseEstimator.get_results.\n\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:02<00:00,  3.70it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.004133\n      0.001948\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003554\n      0.001801\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.101755\n      0.004725\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.248171\n      0.005931\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003473\n      0.002178\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.056660\n      0.003020\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.003440\n      0.001719\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003119\n      0.001703\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004548\n      0.001736\n    \n  \n\n\n\n\nIn those two lines, 9 different regression models were trained with cross validation and the average score for multiple metrics was printed.\n\n\n\n\n\n\nDummy estimators\n\n\n\nPoniard always include a DummyClassifier with strategy=\"prior\" or DummyRegressor with strategy=\"mean\" in order to have the absolute minimum baseline scores. Models should easily beat these, but you could be surprised.\n\n\nPoniard tries to provide good defaults everywhere.\n\nestimators: sklearn provides more than 40 classifiers and 50 regressors, but for most problems you can make do with a limited list of the most battle tested models. Poniard reflects that.\nmetrics: different metrics capture different aspects of the relationship between predictions and ground truth, so Poniard includes multiple suitable ones.\ncv: cross validation is a key aspect of the Poniard flow, and by default 5-fold validation is used.\n\n\n\n\n\n\n\nrandom_seed behavior\n\n\n\nPoniard estimators’ random_seed parameter is always set (if random_seed=None at initialization, it will be forced to 0) and injected into models and cross validators. The idea is to get a reproducible environment, including using the same cross validation folds for each model.\n\n\nDefault preprocessing deserves its own mention. By default, type inference will be run on the datasets’ features and transformations will be applied accordingly. The end goal of the default preprocessor is to make models run without raising any errors.\nAs with most things in Poniard, the preprocessing pipeline can be modified or replaced entirely with the scikit-learn transformers you are used to.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'bmi', 'bp', 's1',\n                                                   's2', 's3', 's4', 's5',\n                                                   's6']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['sex'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4',\n                                  's5', 's6']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['sex'])])numeric_preprocessor['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['sex']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)VarianceThresholdVarianceThreshold()\n\n\nPoniard keeps track of which models it has cross validated, which means that\n\nIf a new one is added, it will not fit the existing ones.\nIf the preprocessor is changed after training, it will fit everything again.\n\n\nfrom sklearn.linear_model import SGDRegressor\n\n\npnd.add_estimators(SGDRegressor(max_iter=10000))\npnd.fit()\npnd.get_results()\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00,  1.74it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_neg_mean_squared_error\n      test_neg_mean_absolute_percentage_error\n      test_neg_median_absolute_error\n      test_r2\n      fit_time\n      score_time\n    \n  \n  \n    \n      LinearRegression\n      -2977.598515\n      -0.396566\n      -39.009146\n      0.489155\n      0.004133\n      0.001948\n    \n    \n      SGDRegressor\n      -3007.445505\n      -0.402132\n      -42.121968\n      0.484970\n      0.113252\n      0.000743\n    \n    \n      ElasticNet\n      -3159.017211\n      -0.422912\n      -42.619546\n      0.460740\n      0.003554\n      0.001801\n    \n    \n      RandomForestRegressor\n      -3431.823331\n      -0.419956\n      -42.203000\n      0.414595\n      0.101755\n      0.004725\n    \n    \n      HistGradientBoostingRegressor\n      -3544.069433\n      -0.407417\n      -40.396390\n      0.391633\n      0.248171\n      0.005931\n    \n    \n      KNeighborsRegressor\n      -3615.195398\n      -0.418674\n      -38.980000\n      0.379625\n      0.003473\n      0.002178\n    \n    \n      XGBRegressor\n      -3923.488860\n      -0.426471\n      -39.031309\n      0.329961\n      0.056660\n      0.003020\n    \n    \n      LinearSVR\n      -4268.314411\n      -0.374296\n      -43.388592\n      0.271443\n      0.003440\n      0.001719\n    \n    \n      DummyRegressor\n      -5934.577616\n      -0.621540\n      -61.775921\n      -0.000797\n      0.003119\n      0.001703\n    \n    \n      DecisionTreeRegressor\n      -6728.423034\n      -0.591906\n      -59.700000\n      -0.145460\n      0.004548\n      0.001736\n    \n  \n\n\n\n\n\n\n\n\n\n\nSingle or array-like inputs\n\n\n\nAnywhere Poniard takes estimators or metrics, or strings representing them, a single element or a sequence of elements can be safely passed and will be handed gracefully."
  },
  {
    "objectID": "guide/00_nutshell.html#plots-when-get_results-is-not-enough",
    "href": "guide/00_nutshell.html#plots-when-get_results-is-not-enough",
    "title": "Poniard in a nutshell: getting started",
    "section": "Plots: when get_results is not enough",
    "text": "Plots: when get_results is not enough\nWhile a nicely formatted table is useful, graphical aid can make things go a lot smoother. Poniard estimators include a plot accessor that gives access to multiple prebuilt plots.\nRefer to the plotting guide or the plotting reference for a deeper dive.\n\nfig = pnd.plot.metrics(kind=\"bar\", metrics=[\"neg_mean_absolute_percentage_error\", \"neg_mean_squared_error\"])\nfig.show(\"notebook\")\n\n\n                                                \n\n\n\nfig = pnd.plot.residuals_histogram(estimator_names=[\"LinearRegression\", \"SGDRegressor\"])\nfig.show(\"notebook\")\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 34.04it/s]\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00,  1.78it/s]\n\n\n\n                                                \n\n\nBy using Plotly, users can have an easier time exploring charts, zooming in, selecting specific models, etc."
  },
  {
    "objectID": "guide/00_nutshell.html#a-reasonably-unified-api",
    "href": "guide/00_nutshell.html#a-reasonably-unified-api",
    "title": "Poniard in a nutshell: getting started",
    "section": "A reasonably unified API",
    "text": "A reasonably unified API\nSo far we have analyzed a regression task. Luckily, PoniardClassifier and PoniardRegressor differ only in default models, default cross validation strategy and default metrics.\n\nfrom poniard import PoniardClassifier\nfrom sklearn.datasets import load_wine\n\n\nX, y = load_wine(return_X_y=True, as_frame=True)\nclf = PoniardClassifier().setup(X, y)\n\nTarget info\n-----------\nType: multiclass\nShape: (178,)\nUnique values: 3\n\nMain metric\n-----------\nroc_auc_ovr\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 17\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      alcohol\n      \n      \n      \n    \n    \n      1\n      malic_acid\n      \n      \n      \n    \n    \n      2\n      ash\n      \n      \n      \n    \n    \n      3\n      alcalinity_of_ash\n      \n      \n      \n    \n    \n      4\n      magnesium\n      \n      \n      \n    \n    \n      5\n      total_phenols\n      \n      \n      \n    \n    \n      6\n      flavanoids\n      \n      \n      \n    \n    \n      7\n      nonflavanoid_phenols\n      \n      \n      \n    \n    \n      8\n      proanthocyanins\n      \n      \n      \n    \n    \n      9\n      color_intensity\n      \n      \n      \n    \n    \n      10\n      hue\n      \n      \n      \n    \n    \n      11\n      od280/od315_of_diluted_wines\n      \n      \n      \n    \n    \n      12\n      proline\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\n\nclf.fit()\nclf.get_results()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:02<00:00,  4.17it/s]\n\n\n\n\n\n\n  \n    \n      \n      test_roc_auc_ovr\n      test_accuracy\n      test_precision_macro\n      test_recall_macro\n      test_f1_macro\n      fit_time\n      score_time\n    \n  \n  \n    \n      LogisticRegression\n      1.000000\n      0.983175\n      0.982828\n      0.983810\n      0.982571\n      0.003325\n      0.002601\n    \n    \n      RandomForestClassifier\n      0.999336\n      0.971905\n      0.973216\n      0.974098\n      0.972726\n      0.045176\n      0.007993\n    \n    \n      HistGradientBoostingClassifier\n      0.999311\n      0.971746\n      0.970350\n      0.976508\n      0.972109\n      0.243912\n      0.021794\n    \n    \n      SVC\n      0.999128\n      0.960635\n      0.960133\n      0.965079\n      0.960681\n      0.002728\n      0.002654\n    \n    \n      GaussianNB\n      0.998855\n      0.971905\n      0.973533\n      0.974098\n      0.972720\n      0.001754\n      0.002812\n    \n    \n      XGBClassifier\n      0.998213\n      0.949206\n      0.956410\n      0.950548\n      0.950512\n      0.021561\n      0.003983\n    \n    \n      KNeighborsClassifier\n      0.995903\n      0.960794\n      0.959845\n      0.965079\n      0.960468\n      0.001666\n      0.003280\n    \n    \n      DecisionTreeClassifier\n      0.945058\n      0.927302\n      0.933483\n      0.927961\n      0.928931\n      0.001852\n      0.002390\n    \n    \n      DummyClassifier\n      0.500000\n      0.399048\n      0.133016\n      0.333333\n      0.190095\n      0.001453\n      0.002623\n    \n  \n\n\n\n\n\nfig = clf.plot.confusion_matrix(estimator_name=\"LogisticRegression\")\nfig.show(\"notebook\")\n\nCompleted: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 39.65it/s]"
  },
  {
    "objectID": "guide/00_nutshell.html#intended-use",
    "href": "guide/00_nutshell.html#intended-use",
    "title": "Poniard in a nutshell: getting started",
    "section": "Intended use",
    "text": "Intended use\nIn the real world where real data lives, building machine learning models is not as simple as running an abstraction in 2 lines and calling it a day.\nOur preferred way of working is splitting the data in train-test sets before touching Poniard. That way you can pass the training data to PoniardBaseEstimator.setup and let the inbuilt cross validation handle model evaluation.\nWhen you are done, PoniardBaseEstimator.get_estimator simply returns a pipeline by name and optionally trains it with full data (which should be just training data), making it easy to continue working on models while preserving an unseen test set which can now be used to assess generalization power.\n\nclf.get_estimator(\"RandomForestClassifier\", retrain=False)\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  Pipeline(steps=[('numeric_imputer',\n                                                   SimpleImputer()),\n                                                  ('scaler',\n                                                   StandardScaler())])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(random_state=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: PipelinePipeline(steps=[('numeric_imputer', SimpleImputer()),\n                ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()VarianceThresholdVarianceThreshold()RandomForestClassifierRandomForestClassifier(random_state=0)"
  },
  {
    "objectID": "reference/estimators.core.html",
    "href": "reference/estimators.core.html",
    "title": "Base estimator",
    "section": "",
    "text": "PoniardBaseEstimator (estimators:Optional[Union[Sequence[ClassifierMixin]\n                       ,Dict[str,ClassifierMixin],Sequence[RegressorMixin]\n                       ,Dict[str,RegressorMixin]]]=None, metrics:Optional[\n                       Union[str,Dict[str,Callable],Sequence[str]]]=None,\n                       preprocess:bool=True,\n                       scaler:Optional[Union[str,TransformerMixin]]=None, \n                       high_cardinality_encoder:Optional[Union[str,Transfo\n                       rmerMixin]]=None, numeric_imputer:Optional[Union[st\n                       r,TransformerMixin]]=None, custom_preprocessor:Unio\n                       n[None,Pipeline,TransformerMixin]=None,\n                       numeric_threshold:Union[int,float]=0.1,\n                       cardinality_threshold:Union[int,float]=20, cv:Union\n                       [int,BaseCrossValidator,BaseShuffleSplit,Sequence]=\n                       None, verbose:int=0,\n                       random_state:Optional[int]=None,\n                       n_jobs:Optional[int]=None,\n                       plugins:Optional[Sequence[Any]]=None,\n                       plot_options:Optional[PoniardPlotFactory]=None,\n                       cache_transformations:bool=False)\n\nBase estimator that sets up all the functionality for the classifier and regressor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimators\nOptional[Union[Sequence[ClassifierMixin], Dict[str, ClassifierMixin], Sequence[RegressorMixin], Dict[str, RegressorMixin]]]\nNone\nEstimators to evaluate.\n\n\nmetrics\nOptional[Union[str, Dict[str, Callable], Sequence[str]]]\nNone\nMetrics to compute for each estimator. This is more restrictive than sklearn’s scoringparameter, as it does not allow callable scorers. Single strings are cast to listsautomatically.\n\n\npreprocess\nbool\nTrue\nIf True, impute missing values, standard scale numeric data and one-hot or ordinalencode categorical data.\n\n\nscaler\nOptional[Union[str, TransformerMixin]]\nNone\nNumeric scaler method. Either “standard”, “minmax”, “robust” or scikit-learn Transformer.\n\n\nhigh_cardinality_encoder\nOptional[Union[str, TransformerMixin]]\nNone\nEncoder for categorical features with high cardinality. Either “target” or “ordinal”,or scikit-learn Transformer.\n\n\nnumeric_imputer\nOptional[Union[str, TransformerMixin]]\nNone\nImputation method. Either “simple”, “iterative” or scikit-learn Transformer.\n\n\ncustom_preprocessor\nUnion[None, Pipeline, TransformerMixin]\nNone\nPreprocessor used instead of the default preprocessing pipeline. It must be able to beincluded directly in a scikit-learn Pipeline.\n\n\nnumeric_threshold\nUnion[int, float]\n0.1\nNumber features with unique values above a certain threshold will be treated as numeric. Iffloat, the threshold is numeric_threshold * samples.\n\n\ncardinality_threshold\nUnion[int, float]\n20\nNon-number features with cardinality above a certain threshold will be treated asordinal encoded instead of one-hot encoded. If float, the threshold iscardinality_threshold * samples.\n\n\ncv\nUnion[int, BaseCrossValidator, BaseShuffleSplit, Sequence]\nNone\nCross validation strategy. Either an integer, a scikit-learn cross validation object,or an iterable.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\nOptional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\nOptional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction.\n\n\nplugins\nOptional[Sequence[Any]]\nNone\nPlugin instances that run in set moments of setup, fit and plotting.\n\n\nplot_options\nOptional[PoniardPlotFactory]\nNone\n:class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly formatoptions or None, which sets the default factory.\n\n\ncache_transformations\nbool\nFalse\nWhether to cache transformations and set the memory parameter for Pipelines. This can speed up slow transformations as they are not recalculated for each estimator."
  },
  {
    "objectID": "reference/estimators.core.html#estimators-metrics-and-cv",
    "href": "reference/estimators.core.html#estimators-metrics-and-cv",
    "title": "Base estimator",
    "section": "estimators, metrics and cv",
    "text": "estimators, metrics and cv\nPoniard estimators’ main parameters can be grouped in the following way:\n\nEstimators.\nPreprocessing parameters.\n\nImputers\nNumeric scaler\nCategorical encoder\nCustom preprocessor\n\nMetrics.\nCross validation strategy.\nRest.\n\nThese give a good amount of flexibility while providing sane defaults, so that after initialization only setup and fit have to be called in order to train multiple models.\nestimators takes a scikit-learn-compatible estimator, array of estimators or dict of name: estimators.\n\nfrom poniard import PoniardRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n\nestimators = [LinearRegression(), Ridge()]\nPoniardRegressor(estimators)\n\nPoniardRegressor(estimators=[LinearRegression(), Ridge()], metrics=None,\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=0.1,\n    cardinality_threshold=20, cv=None, verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())"
  },
  {
    "objectID": "reference/estimators.core.html#setup",
    "href": "reference/estimators.core.html#setup",
    "title": "Base estimator",
    "section": "setup",
    "text": "setup\nsetup takes features and target as parameters, while fit does not accept any. This runs contrary to the established convention defined by scikit-learn where there is no setting up to do and fit takes the data as params.\nThis is because Poniard does not only fit the models, but also infer features types and create the preprocesor based on these types. While this could all be stuffed inside fit (that was the case initially), having it separated allows the user to check whether Poniard’s assumptions are correct and adjust if needed before running fit, which can take long depending on how many models were passed to estimators, the cross validation strategy and the size of the dataset.\n\n\nPoniardBaseEstimator.setup\n\n PoniardBaseEstimator.setup\n                             (X:Union[pandas.core.frame.DataFrame,numpy.nd\n                             array,List], y:Union[pandas.core.frame.DataFr\n                             ame,numpy.ndarray,List])\n\nOrchestrator.\nConverts inputs to arrays if necessary, sets metrics, preprocessor, cv and pipelines.\nAfter running setup, both X and y will be held as attributes.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nFeatures.\n\n\ny\nUnion[pd.DataFrame, np.ndarray, List]\nTarget\n\n\nReturns\nPoniardBaseEstimator\n\n\n\n\n\n\nAn example\nLet’s load some random data and setup a PoniardClassifier, which inherits from PoniardBaseEstimator.\n\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n                     \"age\": rng.uniform(1, 200, 500).astype(int),\n                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n                     \"rating\": random.choices(range(50), k=500),\n                     \"target\": random.choices([0, 1], k=500)})\ndata.head()\n\n\n\n\n\n  \n    \n      \n      type\n      age\n      date\n      rating\n      target\n    \n  \n  \n    \n      0\n      apartment\n      127\n      2022-01-31\n      1\n      1\n    \n    \n      1\n      apartment\n      54\n      2022-02-28\n      17\n      1\n    \n    \n      2\n      house\n      9\n      2022-03-31\n      0\n      1\n    \n    \n      3\n      house\n      4\n      2022-04-30\n      48\n      1\n    \n    \n      4\n      apartment\n      162\n      2022-05-31\n      40\n      0\n    \n  \n\n\n\n\nsetup will conveniently output information about the data so it can be reviewed.\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: binary\nShape: (500,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\n\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\n\n\nAttributes available after setup\nAfter passing data to Poniard estimators through setup, multiple attributes become available.\ninferred_types is a DataFrame that sorts features in 4 categories (numeric, categorical_high, categorical_low and datetime) using some basic heuristics.\nThese depend on the feature dtypes, and numeric_threshold and cardinality_threshold which are set during PoniardBaseEstimator’s construction.\n\npnd.inferred_types\n\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\nThe preprocessor in turn depends on inferred_types, and the scaler, numeric_imputer and high_cardinality_encoder parameters passed to the Poniard estimator init.\nAs will be seen further on, this preprocessor can be modified significantly to fit multiple use cases and datasets.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nEach estimator has a set of default metrics, but others can be passed during construction.\n\npnd.metrics\n\n['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n\n\nLikewise, cv has sane defaults but can be modified accordingly.\n\npnd.cv\n\nStratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\n\ntarget_info lists information about y.\n\npnd.target_info\n\n{'type_': 'binary', 'ndim': 1, 'shape': (500,), 'nunique': 2}\n\n\npipelines is a dict containing each pipeline which will be trained during fit. Each Poniard estimator has a limited set of default estimators.\n\npnd.pipelines[\"SVC\"]\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()SVCSVC(kernel='linear', probability=True, random_state=0, verbose=0)"
  },
  {
    "objectID": "reference/estimators.core.html#fit-and-get_results",
    "href": "reference/estimators.core.html#fit-and-get_results",
    "title": "Base estimator",
    "section": "fit and get_results",
    "text": "fit and get_results\nBecause features and target are passed to the Poniard estimator, fit does not take any parameters. Its main purpose is to run sklearn’s cross_validate function on each pipeline, scoring each metrics with the cv strategy, and store the results.\n\npnd.fit()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:07<00:00,  1.25it/s]\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nAfter fitting pipelines, cross validated results can be accessed by running get_results\n\n\nPoniardBaseEstimator.get_results\n\n PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n                                   std:bool=False, wrt_dummy:bool=False)\n\nReturn dataframe containing scoring results. By default returns the mean score and fit and score times. Optionally returns standard deviations as well.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_train_scores\nbool\nFalse\nIf False, only return test scores.\n\n\nstd\nbool\nFalse\nWhether to return standard deviation of the scores. Default False.\n\n\nwrt_dummy\nbool\nFalse\nWhether to compute each score/time with respect to the dummy estimator results. DefaultFalse.\n\n\nReturns\nUnion[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]\n\nResults\n\n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.510256\n      0.510\n      0.531145\n      0.503846\n      0.516707\n      0.010677\n      0.007267\n    \n    \n      DummyClassifier\n      0.500000\n      0.520\n      0.520000\n      1.000000\n      0.684211\n      0.009707\n      0.007484\n    \n    \n      KNeighborsClassifier\n      0.496675\n      0.492\n      0.509150\n      0.534615\n      0.519465\n      0.009787\n      0.008506\n    \n    \n      SVC\n      0.472356\n      0.476\n      0.499007\n      0.688462\n      0.575907\n      0.712589\n      0.008838\n    \n    \n      LogisticRegression\n      0.468990\n      0.488\n      0.509234\n      0.573077\n      0.536862\n      0.019665\n      0.007746\n    \n    \n      XGBClassifier\n      0.460417\n      0.486\n      0.502401\n      0.500000\n      0.499330\n      0.047521\n      0.010556\n    \n    \n      HistGradientBoostingClassifier\n      0.456571\n      0.488\n      0.505975\n      0.484615\n      0.494283\n      0.333436\n      0.019047\n    \n    \n      RandomForestClassifier\n      0.435056\n      0.462\n      0.479861\n      0.476923\n      0.477449\n      0.072617\n      0.015246\n    \n    \n      GaussianNB\n      0.423317\n      0.468\n      0.492473\n      0.565385\n      0.525371\n      0.010087\n      0.007634\n    \n  \n\n\n\n\n\nmeans, stds = pnd.get_results(std=True, return_train_scores=True)\nstds\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      train_roc_auc\n      test_accuracy\n      train_accuracy\n      test_precision\n      train_precision\n      test_recall\n      train_recall\n      test_f1\n      train_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.060706\n      0.000000e+00\n      0.060332\n      0.000000\n      0.059942\n      0.000000\n      0.058835\n      0.000000\n      0.057785\n      0.000000\n      0.000260\n      0.000063\n    \n    \n      DummyClassifier\n      0.000000\n      0.000000e+00\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000349\n      0.000283\n    \n    \n      KNeighborsClassifier\n      0.021105\n      8.429609e-03\n      0.019391\n      0.010840\n      0.019140\n      0.008157\n      0.081043\n      0.022053\n      0.049760\n      0.012869\n      0.000352\n      0.000087\n    \n    \n      SVC\n      0.038609\n      3.600720e-02\n      0.042708\n      0.032496\n      0.031965\n      0.028405\n      0.085485\n      0.073140\n      0.036968\n      0.026864\n      0.110735\n      0.000180\n    \n    \n      LogisticRegression\n      0.068079\n      2.545484e-02\n      0.041183\n      0.027946\n      0.037992\n      0.024759\n      0.065948\n      0.021371\n      0.036585\n      0.022583\n      0.003979\n      0.000413\n    \n    \n      XGBClassifier\n      0.065278\n      0.000000e+00\n      0.035553\n      0.000000\n      0.033315\n      0.000000\n      0.091826\n      0.000000\n      0.061108\n      0.000000\n      0.001559\n      0.000640\n    \n    \n      HistGradientBoostingClassifier\n      0.059681\n      7.749323e-04\n      0.041183\n      0.007483\n      0.039938\n      0.011912\n      0.070291\n      0.005607\n      0.054859\n      0.007046\n      0.080132\n      0.005613\n    \n    \n      RandomForestClassifier\n      0.060809\n      7.021667e-17\n      0.039192\n      0.000000\n      0.038392\n      0.000000\n      0.077307\n      0.000000\n      0.056132\n      0.000000\n      0.000510\n      0.000183\n    \n    \n      GaussianNB\n      0.045845\n      2.494438e-02\n      0.042143\n      0.018303\n      0.037330\n      0.015830\n      0.031246\n      0.038051\n      0.025456\n      0.018727\n      0.000488\n      0.000487\n    \n  \n\n\n\n\n\n\n\nPoniardBaseEstimator.reassign_types\n\n PoniardBaseEstimator.reassign_types\n                                      (numeric:Union[List[Union[str,int]],\n                                      NoneType]=None, categorical_high:Uni\n                                      on[List[Union[str,int]],NoneType]=No\n                                      ne, categorical_low:Union[List[Union\n                                      [str,int]],NoneType]=None, datetime:\n                                      Union[List[Union[str,int]],NoneType]\n                                      =None)\n\nReassign feature types.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnumeric\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_high\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_low\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ndatetime\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\nReturns\nPoniardBaseEstimator\n\nself."
  },
  {
    "objectID": "guide/02_preprocessing.html#type-inference",
    "href": "guide/02_preprocessing.html#type-inference",
    "title": "Preprocessing",
    "section": "Type inference",
    "text": "Type inference\nType inference is governed by the input data types and two thresholds included in the estimator constructor.\nNumber features (as defined by numpy) with unique values greater than numeric_threshold will be treated as numeric, with the remainder being treated as non-numeric. If this parameter is a float, the actual threshold is numeric_threshold * samples.\nNon-numeric features (either because they are number features below numeric_threshold or they are non-number features like strings) with unique values greater than cardinality_threshold will be considered high cardinality. Likewise, in the case of a float value, the threshold is cardinality_threshold * samples.\nDefaults are set at reasonable limits, but do pay attention to the output of PoniardBaseEstimator.setup as it might expose misclassified features. In that scenario there’s three options: initialize the estimator with different thresholds that better acommodate the dataset, use a custom_preprocessor that applies appropiate transformations to different sets of features, or use the PoniardBaseEstimator.reassign_types method to explicitly assign features to the three categories.\nIn the following example, PoniardBaseEstimator.reassign_types is used to make every feature numeric as far as preprocessing goes.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom poniard import PoniardRegressor\n\n\nX, y = fetch_california_housing(return_X_y=True, as_frame=True)\nreg = PoniardRegressor()\nreg.setup(X, y)\nreg.preprocessor\n\nTarget info\n-----------\nType: continuous\nShape: (20640,)\nUnique values: 3842\n\nMain metric\n-----------\nneg_mean_squared_error\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 2064\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      MedInc\n      HouseAge\n      \n      \n    \n    \n      1\n      AveRooms\n      Latitude\n      \n      \n    \n    \n      2\n      AveBedrms\n      Longitude\n      \n      \n    \n    \n      3\n      Population\n      \n      \n      \n    \n    \n      4\n      AveOccup\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge', 'Latitude',\n                                                   'Longitude'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['MedInc', 'AveRooms',\n                                                   'AveBedrms', 'Population',\n                                                   'AveOccup']),\n                                                 ('categorical_high_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='regression'))]),\n                                                  ['HouseAge', 'Latitude',\n                                                   'Longitude'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['MedInc', 'AveRooms', 'AveBedrms',\n                                  'Population', 'AveOccup']),\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='regression'))]),\n                                 ['HouseAge', 'Latitude', 'Longitude'])])numeric_preprocessor['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_high_preprocessor['HouseAge', 'Latitude', 'Longitude']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='regression')VarianceThresholdVarianceThreshold()\n\n\n\nreg.reassign_types(numeric=[\"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\", \"HouseAge\", \"Latitude\", \"Longitude\"])\nreg.preprocessor\n\nAssigned feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      AveRooms\n      \n      \n      \n    \n    \n      1\n      AveBedrms\n      \n      \n      \n    \n    \n      2\n      Population\n      \n      \n      \n    \n    \n      3\n      AveOccup\n      \n      \n      \n    \n    \n      4\n      HouseAge\n      \n      \n      \n    \n    \n      5\n      Latitude\n      \n      \n      \n    \n    \n      6\n      Longitude\n      \n      \n      \n    \n  \n\n\n\n\n\n\n\nPipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 Pipeline(steps=[('numeric_imputer', SimpleImputer()),\n                                 ('scaler', StandardScaler())])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: PipelinePipeline(steps=[('numeric_imputer', SimpleImputer()),\n                ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()VarianceThresholdVarianceThreshold()\n\n\n\n\n\n\n\n\nUndefined features in reassign_types\n\n\n\nAny feature that is not included in any of the PoniardBaseEstimator.reassign_types parameters will be effectively dropped, which is why already-numeric features had to be included in the numeric parameter. This behavior will be changed in the future."
  },
  {
    "objectID": "reference/core.html",
    "href": "reference/core.html",
    "title": "Base estimator",
    "section": "",
    "text": "PoniardBaseEstimator (estimators:Optional[Union[Sequence[ClassifierMixin]\n                       ,Dict[str,ClassifierMixin],Sequence[RegressorMixin]\n                       ,Dict[str,RegressorMixin]]]=None, metrics:Optional[\n                       Union[str,Dict[str,Callable],Sequence[str]]]=None,\n                       preprocess:bool=True,\n                       scaler:Optional[Union[str,TransformerMixin]]=None, \n                       high_cardinality_encoder:Optional[Union[str,Transfo\n                       rmerMixin]]=None, numeric_imputer:Optional[Union[st\n                       r,TransformerMixin]]=None, custom_preprocessor:Unio\n                       n[None,Pipeline,TransformerMixin]=None,\n                       numeric_threshold:Union[int,float]=0.1,\n                       cardinality_threshold:Union[int,float]=20, cv:Union\n                       [int,BaseCrossValidator,BaseShuffleSplit,Sequence]=\n                       None, verbose:int=0,\n                       random_state:Optional[int]=None,\n                       n_jobs:Optional[int]=None,\n                       plugins:Optional[Sequence[Any]]=None,\n                       plot_options:Optional[PoniardPlotFactory]=None,\n                       cache_transformations:bool=False)\n\nBase estimator that sets up all the functionality for the classifier and regressor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimators\nOptional[Union[Sequence[ClassifierMixin], Dict[str, ClassifierMixin], Sequence[RegressorMixin], Dict[str, RegressorMixin]]]\nNone\nEstimators to evaluate.\n\n\nmetrics\nOptional[Union[str, Dict[str, Callable], Sequence[str]]]\nNone\nMetrics to compute for each estimator. This is more restrictive than sklearn’s scoringparameter, as it does not allow callable scorers. Single strings are cast to listsautomatically.\n\n\npreprocess\nbool\nTrue\nIf True, impute missing values, standard scale numeric data and one-hot or ordinalencode categorical data.\n\n\nscaler\nOptional[Union[str, TransformerMixin]]\nNone\nNumeric scaler method. Either “standard”, “minmax”, “robust” or scikit-learn Transformer.\n\n\nhigh_cardinality_encoder\nOptional[Union[str, TransformerMixin]]\nNone\nEncoder for categorical features with high cardinality. Either “target” or “ordinal”,or scikit-learn Transformer.\n\n\nnumeric_imputer\nOptional[Union[str, TransformerMixin]]\nNone\nImputation method. Either “simple”, “iterative” or scikit-learn Transformer.\n\n\ncustom_preprocessor\nUnion[None, Pipeline, TransformerMixin]\nNone\nPreprocessor used instead of the default preprocessing pipeline. It must be able to beincluded directly in a scikit-learn Pipeline.\n\n\nnumeric_threshold\nUnion[int, float]\n0.1\nNumber features with unique values above a certain threshold will be treated as numeric. Iffloat, the threshold is numeric_threshold * samples.\n\n\ncardinality_threshold\nUnion[int, float]\n20\nNon-number features with cardinality above a certain threshold will be treated asordinal encoded instead of one-hot encoded. If float, the threshold iscardinality_threshold * samples.\n\n\ncv\nUnion[int, BaseCrossValidator, BaseShuffleSplit, Sequence]\nNone\nCross validation strategy. Either an integer, a scikit-learn cross validation object,or an iterable.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\nOptional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\nOptional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction.\n\n\nplugins\nOptional[Sequence[Any]]\nNone\nPlugin instances that run in set moments of setup, fit and plotting.\n\n\nplot_options\nOptional[PoniardPlotFactory]\nNone\n:class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly formatoptions or None, which sets the default factory.\n\n\ncache_transformations\nbool\nFalse\nWhether to cache transformations and set the memory parameter for Pipelines. This can speed up slow transformations as they are not recalculated for each estimator."
  },
  {
    "objectID": "reference/core.html#estimators-metrics-and-cv",
    "href": "reference/core.html#estimators-metrics-and-cv",
    "title": "Base estimator",
    "section": "estimators, metrics and cv",
    "text": "estimators, metrics and cv\nPoniard estimators’ main parameters can be grouped in the following way:\n\nEstimators.\nPreprocessing parameters.\n\nImputers\nNumeric scaler\nCategorical encoder\nCustom preprocessor\n\nMetrics.\nCross validation strategy.\nRest.\n\nThese give a good amount of flexibility while providing sane defaults, so that after initialization only setup and fit have to be called in order to train multiple models.\nestimators takes a scikit-learn-compatible estimator, array of estimators or dict of name: estimators.\n\nfrom poniard import PoniardRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n\nestimators = [LinearRegression(), Ridge()]\nPoniardRegressor(estimators)\n\nPoniardRegressor(estimators=[LinearRegression(), Ridge()], metrics=None,\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=0.1,\n    cardinality_threshold=20, cv=None, verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())"
  },
  {
    "objectID": "reference/core.html#setup",
    "href": "reference/core.html#setup",
    "title": "Base estimator",
    "section": "setup",
    "text": "setup\nsetup takes features and target as parameters, while fit does not accept any. This runs contrary to the established convention defined by scikit-learn where there is no setting up to do and fit takes the data as params.\nThis is because Poniard does not only fit the models, but also infer features types and create the preprocesor based on these types. While this could all be stuffed inside fit (that was the case initially), having it separated allows the user to check whether Poniard’s assumptions are correct and adjust if needed before running fit, which can take long depending on how many models were passed to estimators, the cross validation strategy and the size of the dataset.\n\n\nPoniardBaseEstimator.setup\n\n PoniardBaseEstimator.setup\n                             (X:Union[pandas.core.frame.DataFrame,numpy.nd\n                             array,List], y:Union[pandas.core.frame.DataFr\n                             ame,numpy.ndarray,List])\n\nOrchestrator.\nConverts inputs to arrays if necessary, sets metrics, preprocessor, cv and pipelines.\nAfter running setup, both X and y will be held as attributes.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nFeatures.\n\n\ny\nUnion[pd.DataFrame, np.ndarray, List]\nTarget\n\n\nReturns\nPoniardBaseEstimator\n\n\n\n\n\n\nAn example\nLet’s load some random data and setup a PoniardClassifier, which inherits from PoniardBaseEstimator.\n\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n                     \"age\": rng.uniform(1, 200, 500).astype(int),\n                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n                     \"rating\": random.choices(range(50), k=500),\n                     \"target\": random.choices([0, 1], k=500)})\ndata.head()\n\n\n\n\n\n  \n    \n      \n      type\n      age\n      date\n      rating\n      target\n    \n  \n  \n    \n      0\n      apartment\n      127\n      2022-01-31\n      1\n      1\n    \n    \n      1\n      apartment\n      54\n      2022-02-28\n      17\n      1\n    \n    \n      2\n      house\n      9\n      2022-03-31\n      0\n      1\n    \n    \n      3\n      house\n      4\n      2022-04-30\n      48\n      1\n    \n    \n      4\n      apartment\n      162\n      2022-05-31\n      40\n      0\n    \n  \n\n\n\n\nsetup will conveniently output information about the data so it can be reviewed.\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: binary\nShape: (500,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\n\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\n\n\nAttributes available after setup\nAfter passing data to Poniard estimators through setup, multiple attributes become available.\ninferred_types is a DataFrame that sorts features in 4 categories (numeric, categorical_high, categorical_low and datetime) using some basic heuristics.\nThese depend on the feature dtypes, and numeric_threshold and cardinality_threshold which are set during PoniardBaseEstimator’s construction.\n\npnd.inferred_types\n\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\nThe preprocessor in turn depends on inferred_types, and the scaler, numeric_imputer and high_cardinality_encoder parameters passed to the Poniard estimator init.\nAs will be seen further on, this preprocessor can be modified significantly to fit multiple use cases and datasets.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nEach estimator has a set of default metrics, but others can be passed during construction.\n\npnd.metrics\n\n['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n\n\nLikewise, cv has sane defaults but can be modified accordingly.\n\npnd.cv\n\nStratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\n\ntarget_info lists information about y.\n\npnd.target_info\n\n{'type_': 'binary', 'ndim': 1, 'shape': (500,), 'nunique': 2}\n\n\npipelines is a dict containing each pipeline which will be trained during fit. Each Poniard estimator has a limited set of default estimators.\n\npnd.pipelines[\"SVC\"]\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()SVCSVC(kernel='linear', probability=True, random_state=0, verbose=0)"
  },
  {
    "objectID": "reference/core.html#fit-and-get_results",
    "href": "reference/core.html#fit-and-get_results",
    "title": "Base estimator",
    "section": "fit and get_results",
    "text": "fit and get_results\nBecause features and target are passed to the Poniard estimator, fit does not take any parameters. Its main purpose is to run sklearn’s cross_validate function on each pipeline, scoring each metrics with the cv strategy, and store the results.\n\npnd.fit()\n\nCompleted: 100%|██████████████████████████████████| 9/9 [00:07<00:00,  1.25it/s]\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nAfter fitting pipelines, cross validated results can be accessed by running get_results\n\n\nPoniardBaseEstimator.get_results\n\n PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n                                   std:bool=False, wrt_dummy:bool=False)\n\nReturn dataframe containing scoring results. By default returns the mean score and fit and score times. Optionally returns standard deviations as well.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_train_scores\nbool\nFalse\nIf False, only return test scores.\n\n\nstd\nbool\nFalse\nWhether to return standard deviation of the scores. Default False.\n\n\nwrt_dummy\nbool\nFalse\nWhether to compute each score/time with respect to the dummy estimator results. DefaultFalse.\n\n\nReturns\nUnion[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]\n\nResults\n\n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.510256\n      0.510\n      0.531145\n      0.503846\n      0.516707\n      0.010677\n      0.007267\n    \n    \n      DummyClassifier\n      0.500000\n      0.520\n      0.520000\n      1.000000\n      0.684211\n      0.009707\n      0.007484\n    \n    \n      KNeighborsClassifier\n      0.496675\n      0.492\n      0.509150\n      0.534615\n      0.519465\n      0.009787\n      0.008506\n    \n    \n      SVC\n      0.472356\n      0.476\n      0.499007\n      0.688462\n      0.575907\n      0.712589\n      0.008838\n    \n    \n      LogisticRegression\n      0.468990\n      0.488\n      0.509234\n      0.573077\n      0.536862\n      0.019665\n      0.007746\n    \n    \n      XGBClassifier\n      0.460417\n      0.486\n      0.502401\n      0.500000\n      0.499330\n      0.047521\n      0.010556\n    \n    \n      HistGradientBoostingClassifier\n      0.456571\n      0.488\n      0.505975\n      0.484615\n      0.494283\n      0.333436\n      0.019047\n    \n    \n      RandomForestClassifier\n      0.435056\n      0.462\n      0.479861\n      0.476923\n      0.477449\n      0.072617\n      0.015246\n    \n    \n      GaussianNB\n      0.423317\n      0.468\n      0.492473\n      0.565385\n      0.525371\n      0.010087\n      0.007634\n    \n  \n\n\n\n\n\nmeans, stds = pnd.get_results(std=True, return_train_scores=True)\nstds\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      train_roc_auc\n      test_accuracy\n      train_accuracy\n      test_precision\n      train_precision\n      test_recall\n      train_recall\n      test_f1\n      train_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.060706\n      0.000000e+00\n      0.060332\n      0.000000\n      0.059942\n      0.000000\n      0.058835\n      0.000000\n      0.057785\n      0.000000\n      0.000260\n      0.000063\n    \n    \n      DummyClassifier\n      0.000000\n      0.000000e+00\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000349\n      0.000283\n    \n    \n      KNeighborsClassifier\n      0.021105\n      8.429609e-03\n      0.019391\n      0.010840\n      0.019140\n      0.008157\n      0.081043\n      0.022053\n      0.049760\n      0.012869\n      0.000352\n      0.000087\n    \n    \n      SVC\n      0.038609\n      3.600720e-02\n      0.042708\n      0.032496\n      0.031965\n      0.028405\n      0.085485\n      0.073140\n      0.036968\n      0.026864\n      0.110735\n      0.000180\n    \n    \n      LogisticRegression\n      0.068079\n      2.545484e-02\n      0.041183\n      0.027946\n      0.037992\n      0.024759\n      0.065948\n      0.021371\n      0.036585\n      0.022583\n      0.003979\n      0.000413\n    \n    \n      XGBClassifier\n      0.065278\n      0.000000e+00\n      0.035553\n      0.000000\n      0.033315\n      0.000000\n      0.091826\n      0.000000\n      0.061108\n      0.000000\n      0.001559\n      0.000640\n    \n    \n      HistGradientBoostingClassifier\n      0.059681\n      7.749323e-04\n      0.041183\n      0.007483\n      0.039938\n      0.011912\n      0.070291\n      0.005607\n      0.054859\n      0.007046\n      0.080132\n      0.005613\n    \n    \n      RandomForestClassifier\n      0.060809\n      7.021667e-17\n      0.039192\n      0.000000\n      0.038392\n      0.000000\n      0.077307\n      0.000000\n      0.056132\n      0.000000\n      0.000510\n      0.000183\n    \n    \n      GaussianNB\n      0.045845\n      2.494438e-02\n      0.042143\n      0.018303\n      0.037330\n      0.015830\n      0.031246\n      0.038051\n      0.025456\n      0.018727\n      0.000488\n      0.000487\n    \n  \n\n\n\n\n\n\n\nPoniardBaseEstimator.reassign_types\n\n PoniardBaseEstimator.reassign_types\n                                      (numeric:Union[List[Union[str,int]],\n                                      NoneType]=None, categorical_high:Uni\n                                      on[List[Union[str,int]],NoneType]=No\n                                      ne, categorical_low:Union[List[Union\n                                      [str,int]],NoneType]=None, datetime:\n                                      Union[List[Union[str,int]],NoneType]\n                                      =None)\n\nReassign feature types.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnumeric\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_high\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_low\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ndatetime\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\nReturns\nPoniardBaseEstimator\n\nself."
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Base estimator",
    "section": "",
    "text": "PoniardBaseEstimator (estimators:Optional[Union[Sequence[ClassifierMixin]\n                       ,Dict[str,ClassifierMixin],Sequence[RegressorMixin]\n                       ,Dict[str,RegressorMixin]]]=None, metrics:Optional[\n                       Union[str,Dict[str,Callable],Sequence[str]]]=None,\n                       preprocess:bool=True,\n                       scaler:Optional[Union[str,TransformerMixin]]=None, \n                       high_cardinality_encoder:Optional[Union[str,Transfo\n                       rmerMixin]]=None, numeric_imputer:Optional[Union[st\n                       r,TransformerMixin]]=None, custom_preprocessor:Unio\n                       n[None,Pipeline,TransformerMixin]=None,\n                       numeric_threshold:Union[int,float]=0.1,\n                       cardinality_threshold:Union[int,float]=20, cv:Union\n                       [int,BaseCrossValidator,BaseShuffleSplit,Sequence]=\n                       None, verbose:int=0,\n                       random_state:Optional[int]=None,\n                       n_jobs:Optional[int]=None,\n                       plugins:Optional[Sequence[Any]]=None,\n                       plot_options:Optional[PoniardPlotFactory]=None,\n                       cache_transformations:bool=False)\n\nBase estimator that sets up all the functionality for the classifier and regressor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimators\nOptional[Union[Sequence[ClassifierMixin], Dict[str, ClassifierMixin], Sequence[RegressorMixin], Dict[str, RegressorMixin]]]\nNone\nEstimators to evaluate.\n\n\nmetrics\nOptional[Union[str, Dict[str, Callable], Sequence[str]]]\nNone\nMetrics to compute for each estimator. This is more restrictive than sklearn’s scoringparameter, as it does not allow callable scorers. Single strings are cast to listsautomatically.\n\n\npreprocess\nbool\nTrue\nIf True, impute missing values, standard scale numeric data and one-hot or ordinalencode categorical data.\n\n\nscaler\nOptional[Union[str, TransformerMixin]]\nNone\nNumeric scaler method. Either “standard”, “minmax”, “robust” or scikit-learn Transformer.\n\n\nhigh_cardinality_encoder\nOptional[Union[str, TransformerMixin]]\nNone\nEncoder for categorical features with high cardinality. Either “target” or “ordinal”,or scikit-learn Transformer.\n\n\nnumeric_imputer\nOptional[Union[str, TransformerMixin]]\nNone\nImputation method. Either “simple”, “iterative” or scikit-learn Transformer.\n\n\ncustom_preprocessor\nUnion[None, Pipeline, TransformerMixin]\nNone\nPreprocessor used instead of the default preprocessing pipeline. It must be able to beincluded directly in a scikit-learn Pipeline.\n\n\nnumeric_threshold\nUnion[int, float]\n0.1\nNumber features with unique values above a certain threshold will be treated as numeric. Iffloat, the threshold is numeric_threshold * samples.\n\n\ncardinality_threshold\nUnion[int, float]\n20\nNon-number features with cardinality above a certain threshold will be treated asordinal encoded instead of one-hot encoded. If float, the threshold iscardinality_threshold * samples.\n\n\ncv\nUnion[int, BaseCrossValidator, BaseShuffleSplit, Sequence]\nNone\nCross validation strategy. Either an integer, a scikit-learn cross validation object,or an iterable.\n\n\nverbose\nint\n0\nVerbosity level. Propagated to every scikit-learn function and estimator.\n\n\nrandom_state\nOptional[int]\nNone\nRNG. Propagated to every scikit-learn function and estimator. The default None setsrandom_state to 0 so that cross_validate results are comparable.\n\n\nn_jobs\nOptional[int]\nNone\nControls parallel processing. -1 uses all cores. Propagated to every scikit-learnfunction.\n\n\nplugins\nOptional[Sequence[Any]]\nNone\nPlugin instances that run in set moments of setup, fit and plotting.\n\n\nplot_options\nOptional[PoniardPlotFactory]\nNone\n:class:poniard.plot.plot_factory.PoniardPlotFactory instance specifying Plotly formatoptions or None, which sets the default factory.\n\n\ncache_transformations\nbool\nFalse\nWhether to cache transformations and set the memory parameter for Pipelines. This can speed up slow transformations as they are not recalculated for each estimator."
  },
  {
    "objectID": "core.html#estimators-metrics-and-cv",
    "href": "core.html#estimators-metrics-and-cv",
    "title": "Base estimator",
    "section": "estimators, metrics and cv",
    "text": "estimators, metrics and cv\nPoniard estimators’ main parameters can be grouped in the following way:\n\nEstimators.\nPreprocessing parameters.\n\nImputers\nNumeric scaler\nCategorical encoder\nCustom preprocessor\n\nMetrics.\nCross validation strategy.\nRest.\n\nThese give a good amount of flexibility while providing sane defaults, so that after initialization only setup and fit have to be called in order to train multiple models.\nestimators takes a scikit-learn-compatible estimator, array of estimators or dict of name: estimators.\n\nfrom poniard import PoniardRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n\nestimators = [LinearRegression(), Ridge()]\nPoniardRegressor(estimators)\n\nPoniardRegressor(estimators=[LinearRegression(), Ridge()], metrics=None,\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=0.1,\n    cardinality_threshold=20, cv=None, verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())"
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "Base estimator",
    "section": "setup",
    "text": "setup\nsetup takes features and target as parameters, while fit does not accept any. This runs contrary to the established convention defined by scikit-learn where there is no setting up to do and fit takes the data as params.\nThis is because Poniard does not only fit the models, but also infer features types and create the preprocesor based on these types. While this could all be stuffed inside fit (that was the case initially), having it separated allows the user to check whether Poniard’s assumptions are correct and adjust if needed before running fit, which can take long depending on how many models were passed to estimators, the cross validation strategy and the size of the dataset.\n\n\nPoniardBaseEstimator.setup\n\n PoniardBaseEstimator.setup\n                             (X:Union[pandas.core.frame.DataFrame,numpy.nd\n                             array,List], y:Union[pandas.core.frame.DataFr\n                             ame,numpy.ndarray,List])\n\nOrchestrator.\nConverts inputs to arrays if necessary, sets metrics, preprocessor, cv and pipelines.\nAfter running setup, both X and y will be held as attributes.\n\n\n\n\nType\nDetails\n\n\n\n\nX\nUnion[pd.DataFrame, np.ndarray, List]\nFeatures.\n\n\ny\nUnion[pd.DataFrame, np.ndarray, List]\nTarget\n\n\nReturns\nPoniardBaseEstimator\n\n\n\n\n\n\nAn example\nLet’s load some random data and setup a PoniardClassifier, which inherits from PoniardBaseEstimator.\n\nfrom poniard import PoniardClassifier\n\n\nrandom.seed(0)\nrng = np.random.default_rng(0)\n\ndata = pd.DataFrame({\"type\": random.choices([\"house\", \"apartment\"], k=500),\n                     \"age\": rng.uniform(1, 200, 500).astype(int),\n                     \"date\": pd.date_range(\"2022-01-01\", freq=\"M\", periods=500),\n                     \"rating\": random.choices(range(50), k=500),\n                     \"target\": random.choices([0, 1], k=500)})\ndata.head()\n\n\n\n\n\n  \n    \n      \n      type\n      age\n      date\n      rating\n      target\n    \n  \n  \n    \n      0\n      apartment\n      127\n      2022-01-31\n      1\n      1\n    \n    \n      1\n      apartment\n      54\n      2022-02-28\n      17\n      1\n    \n    \n      2\n      house\n      9\n      2022-03-31\n      0\n      1\n    \n    \n      3\n      house\n      4\n      2022-04-30\n      48\n      1\n    \n    \n      4\n      apartment\n      162\n      2022-05-31\n      40\n      0\n    \n  \n\n\n\n\nsetup will conveniently output information about the data so it can be reviewed.\n\nX, y = data.drop(\"target\", axis=1), data[\"target\"]\npnd = PoniardClassifier()\npnd.setup(X, y)\n\nTarget info\n-----------\nType: binary\nShape: (500,)\nUnique values: 2\n\nMain metric\n-----------\nroc_auc\n\nThresholds\n----------\nMinimum unique values to consider a feature numeric: 50\nMinimum unique values to consider a categorical high cardinality: 20\n\nInferred feature types\n----------------------\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\n\n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\n\n\nAttributes available after setup\nAfter passing data to Poniard estimators through setup, multiple attributes become available.\ninferred_types is a DataFrame that sorts features in 4 categories (numeric, categorical_high, categorical_low and datetime) using some basic heuristics.\nThese depend on the feature dtypes, and numeric_threshold and cardinality_threshold which are set during PoniardBaseEstimator’s construction.\n\npnd.inferred_types\n\n\n\n\n\n  \n    \n      \n      numeric\n      categorical_high\n      categorical_low\n      datetime\n    \n  \n  \n    \n      0\n      age\n      rating\n      type\n      date\n    \n  \n\n\n\n\nThe preprocessor in turn depends on inferred_types, and the scaler, numeric_imputer and high_cardinality_encoder parameters passed to the Poniard estimator init.\nAs will be seen further on, this preprocessor can be modified significantly to fit multiple use cases and datasets.\n\npnd.preprocessor\n\nPipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()\n\n\nEach estimator has a set of default metrics, but others can be passed during construction.\n\npnd.metrics\n\n['roc_auc', 'accuracy', 'precision', 'recall', 'f1']\n\n\nLikewise, cv has sane defaults but can be modified accordingly.\n\npnd.cv\n\nStratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\n\ntarget_info lists information about y.\n\npnd.target_info\n\n{'type_': 'binary', 'ndim': 1, 'shape': (500,), 'nunique': 2}\n\n\npipelines is a dict containing each pipeline which will be trained during fit. Each Poniard estimator has a limited set of default estimators.\n\npnd.pipelines[\"SVC\"]\n\nPipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocessor',\n                 Pipeline(steps=[('type_preprocessor',\n                                  ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                                   Pipeline(steps=[('numeric_imputer',\n                                                                                    SimpleImputer()),\n                                                                                   ('scaler',\n                                                                                    StandardScaler())]),\n                                                                   ['age']),\n                                                                  ('categorical_low_preprocessor',\n                                                                   Pipeline(steps=[('categorical_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent')),\n                                                                                   ('one-hot_encoder',\n                                                                                    One...\n                                                                                   ('high_cardinality_encoder',\n                                                                                    TargetEncoder(handle_unknown='ignore',\n                                                                                                  task='classification'))]),\n                                                                   ['rating']),\n                                                                  ('datetime_preprocessor',\n                                                                   Pipeline(steps=[('datetime_encoder',\n                                                                                    DatetimeEncoder()),\n                                                                                   ('datetime_imputer',\n                                                                                    SimpleImputer(strategy='most_frequent'))]),\n                                                                   ['date'])])),\n                                 ('remove_invariant', VarianceThreshold())])),\n                ('SVC',\n                 SVC(kernel='linear', probability=True, random_state=0,\n                     verbose=0))])preprocessor: PipelinePipeline(steps=[('type_preprocessor',\n                 ColumnTransformer(transformers=[('numeric_preprocessor',\n                                                  Pipeline(steps=[('numeric_imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age']),\n                                                 ('categorical_low_preprocessor',\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one-hot_encoder',\n                                                                   OneHotEncoder(drop='if_binary',\n                                                                                 hand...\n                                                  Pipeline(steps=[('categorical_imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('high_cardinality_encoder',\n                                                                   TargetEncoder(handle_unknown='ignore',\n                                                                                 task='classification'))]),\n                                                  ['rating']),\n                                                 ('datetime_preprocessor',\n                                                  Pipeline(steps=[('datetime_encoder',\n                                                                   DatetimeEncoder()),\n                                                                  ('datetime_imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['date'])])),\n                ('remove_invariant', VarianceThreshold())])type_preprocessor: ColumnTransformerColumnTransformer(transformers=[('numeric_preprocessor',\n                                 Pipeline(steps=[('numeric_imputer',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age']),\n                                ('categorical_low_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one-hot_encoder',\n                                                  OneHotEncoder(drop='if_binary',\n                                                                handle_unknown='ignore',\n                                                                sparse=False))]),...\n                                ('categorical_high_preprocessor',\n                                 Pipeline(steps=[('categorical_imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('high_cardinality_encoder',\n                                                  TargetEncoder(handle_unknown='ignore',\n                                                                task='classification'))]),\n                                 ['rating']),\n                                ('datetime_preprocessor',\n                                 Pipeline(steps=[('datetime_encoder',\n                                                  DatetimeEncoder()),\n                                                 ('datetime_imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['date'])])numeric_preprocessor['age']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical_low_preprocessor['type']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False)categorical_high_preprocessor['rating']SimpleImputerSimpleImputer(strategy='most_frequent')TargetEncoderTargetEncoder(handle_unknown='ignore', task='classification')datetime_preprocessor['date']DatetimeEncoderDatetimeEncoder()SimpleImputerSimpleImputer(strategy='most_frequent')VarianceThresholdVarianceThreshold()SVCSVC(kernel='linear', probability=True, random_state=0, verbose=0)"
  },
  {
    "objectID": "core.html#fit-and-get_results",
    "href": "core.html#fit-and-get_results",
    "title": "Base estimator",
    "section": "fit and get_results",
    "text": "fit and get_results\nBecause features and target are passed to the Poniard estimator, fit does not take any parameters. Its main purpose is to run sklearn’s cross_validate function on each pipeline, scoring each metrics with the cv strategy, and store the results.\n\npnd.fit()\n\nCompleted: 100%|██████████| 9/9 [00:12<00:00,  1.42s/it]                     \n\n\nPoniardClassifier(estimators=None, metrics=['roc_auc', 'accuracy', 'precision', 'recall', 'f1'],\n    preprocess=True, scaler=standard, numeric_imputer=simple,\n    custom_preprocessor=None, numeric_threshold=50,\n    cardinality_threshold=20, cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True), verbose=0,\n    random_state=0, n_jobs=None, plugins=None,\n    plot_options=PoniardPlotFactory())\n            \n\n\nAfter fitting pipelines, cross validated results can be accessed by running get_results\n\n\nPoniardBaseEstimator.get_results\n\n PoniardBaseEstimator.get_results (return_train_scores:bool=False,\n                                   std:bool=False, wrt_dummy:bool=False)\n\nReturn dataframe containing scoring results. By default returns the mean score and fit and score times. Optionally returns standard deviations as well.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_train_scores\nbool\nFalse\nIf False, only return test scores.\n\n\nstd\nbool\nFalse\nWhether to return standard deviation of the scores. Default False.\n\n\nwrt_dummy\nbool\nFalse\nWhether to compute each score/time with respect to the dummy estimator results. DefaultFalse.\n\n\nReturns\nUnion[Tuple[pd.DataFrame, pd.DataFrame], pd.DataFrame]\n\nResults\n\n\n\n\npnd.get_results()\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      test_accuracy\n      test_precision\n      test_recall\n      test_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.510256\n      0.510\n      0.531145\n      0.503846\n      0.516707\n      0.013237\n      0.009078\n    \n    \n      DummyClassifier\n      0.500000\n      0.520\n      0.520000\n      1.000000\n      0.684211\n      0.013290\n      0.010209\n    \n    \n      KNeighborsClassifier\n      0.496675\n      0.492\n      0.509150\n      0.534615\n      0.519465\n      0.014249\n      0.012771\n    \n    \n      SVC\n      0.472356\n      0.476\n      0.499007\n      0.688462\n      0.575907\n      0.902448\n      0.013387\n    \n    \n      LogisticRegression\n      0.468990\n      0.488\n      0.509234\n      0.573077\n      0.536862\n      0.024270\n      0.009585\n    \n    \n      XGBClassifier\n      0.460417\n      0.486\n      0.502401\n      0.500000\n      0.499330\n      0.082598\n      0.023691\n    \n    \n      HistGradientBoostingClassifier\n      0.456571\n      0.488\n      0.505975\n      0.484615\n      0.494283\n      1.072933\n      0.033191\n    \n    \n      RandomForestClassifier\n      0.435056\n      0.462\n      0.479861\n      0.476923\n      0.477449\n      0.091021\n      0.023910\n    \n    \n      GaussianNB\n      0.423317\n      0.468\n      0.492473\n      0.565385\n      0.525371\n      0.012375\n      0.008995\n    \n  \n\n\n\n\n\nmeans, stds = pnd.get_results(std=True, return_train_scores=True)\nstds\n\n\n\n\n\n  \n    \n      \n      test_roc_auc\n      train_roc_auc\n      test_accuracy\n      train_accuracy\n      test_precision\n      train_precision\n      test_recall\n      train_recall\n      test_f1\n      train_f1\n      fit_time\n      score_time\n    \n  \n  \n    \n      DecisionTreeClassifier\n      0.060706\n      0.000000e+00\n      0.060332\n      0.000000\n      0.059942\n      0.000000\n      0.058835\n      0.000000\n      0.057785\n      0.000000\n      0.000319\n      0.000572\n    \n    \n      DummyClassifier\n      0.000000\n      0.000000e+00\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.001170\n      0.001189\n    \n    \n      KNeighborsClassifier\n      0.021105\n      8.429609e-03\n      0.019391\n      0.010840\n      0.019140\n      0.008157\n      0.081043\n      0.022053\n      0.049760\n      0.012869\n      0.002155\n      0.002610\n    \n    \n      SVC\n      0.038609\n      3.600720e-02\n      0.042708\n      0.032496\n      0.031965\n      0.028405\n      0.085485\n      0.073140\n      0.036968\n      0.026864\n      0.105589\n      0.001795\n    \n    \n      LogisticRegression\n      0.068079\n      2.545484e-02\n      0.041183\n      0.027946\n      0.037992\n      0.024759\n      0.065948\n      0.021371\n      0.036585\n      0.022583\n      0.005073\n      0.000693\n    \n    \n      XGBClassifier\n      0.065278\n      0.000000e+00\n      0.035553\n      0.000000\n      0.033315\n      0.000000\n      0.091826\n      0.000000\n      0.061108\n      0.000000\n      0.011152\n      0.010473\n    \n    \n      HistGradientBoostingClassifier\n      0.059681\n      7.749323e-04\n      0.041183\n      0.007483\n      0.039938\n      0.011912\n      0.070291\n      0.005607\n      0.054859\n      0.007046\n      0.447239\n      0.004421\n    \n    \n      RandomForestClassifier\n      0.060809\n      7.021667e-17\n      0.039192\n      0.000000\n      0.038392\n      0.000000\n      0.077307\n      0.000000\n      0.056132\n      0.000000\n      0.015291\n      0.013291\n    \n    \n      GaussianNB\n      0.045845\n      2.494438e-02\n      0.042143\n      0.018303\n      0.037330\n      0.015830\n      0.031246\n      0.038051\n      0.025456\n      0.018727\n      0.000830\n      0.000163\n    \n  \n\n\n\n\n\n\n\nPoniardBaseEstimator.reassign_types\n\n PoniardBaseEstimator.reassign_types\n                                      (numeric:Union[List[Union[str,int]],\n                                      NoneType]=None, categorical_high:Uni\n                                      on[List[Union[str,int]],NoneType]=No\n                                      ne, categorical_low:Union[List[Union\n                                      [str,int]],NoneType]=None, datetime:\n                                      Union[List[Union[str,int]],NoneType]\n                                      =None)\n\nReassign feature types.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnumeric\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_high\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ncategorical_low\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\ndatetime\nOptional[List[Union[str, int]]]\nNone\nList of column names or indices. Default None.\n\n\nReturns\nPoniardBaseEstimator\n\nself."
  }
]
